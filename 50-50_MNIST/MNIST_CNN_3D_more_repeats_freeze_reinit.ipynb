{"cells":[{"cell_type":"markdown","metadata":{"id":"Tnr5Tf1Qa1H-"},"source":["# Setup and utils"]},{"cell_type":"code","execution_count":138,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T15:26:41.121611Z","iopub.status.busy":"2023-11-12T15:26:41.121288Z","iopub.status.idle":"2023-11-12T15:26:47.333420Z","shell.execute_reply":"2023-11-12T15:26:47.332577Z","shell.execute_reply.started":"2023-11-12T15:26:41.121585Z"},"id":"L0eJ0XYWQHS6","trusted":true},"outputs":[],"source":["import numpy as np\n","import random\n","import copy\n","import matplotlib.pyplot as plt\n","import json, copy\n","import torch\n","import torch.nn as nn\n","import torch.utils.data as data\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","from torchvision import datasets, transforms\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from types import SimpleNamespace\n","import plotly.express as px\n","import pandas as pd\n","import plotly.graph_objects as go\n","from scipy.interpolate import griddata\n","import math\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# ------------------------------------ MODEL UTILS ----------------------------------------------\n","class CustomCNN(nn.Module):\n","    def __init__(self, input_dim, output_dim, depth, num_channels, hidden_dim_lin, activation_function, kernel_size, use_pooling=True):\n","        super(CustomCNN, self).__init__()\n","\n","        # Initial number of input channels, assuming grayscale images\n","        in_channels = 1\n","\n","        # Dynamically add convolutional and activation layers based on the specified depth\n","        for i in range(depth):\n","            # Create a convolutional layer and add it to the model\n","            setattr(self, f\"conv{i}\", nn.Conv2d(in_channels, num_channels, kernel_size=kernel_size, padding=math.floor(kernel_size/2)))\n","\n","            # Create an activation layer (e.g., ReLU) and add it to the model\n","            setattr(self, f\"act{i}\", activation_function())\n","\n","            # Update the input dimensions after convolution\n","            input_dim = (input_dim - kernel_size + 2 * math.floor(kernel_size/2)) + 1\n","\n","            # Optionally add pooling layers to reduce spatial dimensions\n","            if use_pooling and (i+1) % depth == 0:\n","                setattr(self, f\"pool{i}\", nn.MaxPool2d(2))\n","                input_dim = input_dim // 2\n","\n","            # Update the input channels for the next convolutional layer\n","            in_channels = num_channels\n","\n","        # Compute the size of the flattened features for the fully connected layer\n","        flattened_size = in_channels * input_dim * input_dim\n","        # Add two fully connected layers for classification\n","        self.fc_1 = nn.Linear(flattened_size, hidden_dim_lin)\n","        self.relu = activation_function()\n","        self.fc_2 = nn.Linear(hidden_dim_lin, output_dim)\n","\n","        # Add log softmax layer for multi-class classification output\n","        self.logsoftmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, x):\n","        # Iterate over each module in the CustomCNN class\n","        for layer_name, layer in self.named_children():\n","            # Process the input tensor through convolutional and activation layers\n","            if \"conv\" in layer_name or \"act\" in layer_name:\n","                x = layer(x)\n","            # Process the input tensor through pooling layers if they exist\n","            elif \"pool\" in layer_name:\n","                x = layer(x)\n","            # If reached fully connected layers, break the loop\n","            elif isinstance(layer, nn.Linear):\n","                break\n","\n","        # Flatten the tensor to fit the input shape of the fully connected layers\n","        x = x.view(x.size(0), -1)\n","        # Pass the tensor through the fully connected layers\n","        x = self.fc_1(x)\n","        x = self.relu(x)\n","        x = self.fc_2(x)\n","\n","        # Return log softmax activated output\n","        return self.logsoftmax(x)\n","\n","def generate_cnn(input_dim, output_dim, depth, num_channels, hidden_dim_lin, kernel_size, activation_function=nn.ReLU, use_pooling=True):\n","    model = CustomCNN(input_dim, output_dim, depth, num_channels, hidden_dim_lin, activation_function, kernel_size, use_pooling)\n","    return model\n","\n","class Trainer:\n","    \"\"\"\n","    A class for training and evaluating a model with early stopping and best model saving functionalities.\n","\n","    Attributes:\n","    - model: PyTorch model to be trained and evaluated.\n","    - dataloader: Contains data loaders (train, validation, test) for training and evaluation.\n","    - params: Dictionary containing various hyperparameters and settings.\n","    - device: the device to which tensors should be moved before computation.\n","    - optimizer: The optimizer for training.\n","    - best_model_state: State dictionary of the best model.\n","    - max_val_acc: The highest validation accuracy encountered during training.\n","    - no_improve_epochs: Number of epochs without improvement in validation accuracy.\n","    - is_cnn: Flag indicating if the model is a CNN.\n","    - is_debug: Flag indicating if debug information should be printed.\n","    - classification_report_flag: Flag indicating if a classification report should be generated.\n","\n","    Methods:\n","    - train_epoch(): Runs a single epoch of training.\n","    - evaluate(loader): Evaluates the model on a given data loader.\n","    - save_best_model(): Saves the current state of the model as the best model.\n","    - save_checkpoint(epoch, train_acc, val_acc): Saves the current state of the model and other information as a checkpoint.\n","    - early_stopping_check(val_acc): Checks the stopping criterion and performs actions based on it.\n","    - train(): Runs the training process for a number of epochs, with early stopping functionality.\n","\n","    Usage:\n","    params = {\n","      'device': 'cuda',\n","      'lr': 0.001,\n","      'num_train': 10,\n","      'early_stop_patience': 3,\n","      'save_best': True,\n","      'save_checkpoints': False,\n","      'is_cnn': True,\n","      'is_debug': True,\n","      'classification_report_flag': True\n","    }\n","\n","    trainer = Trainer(model, dataloader, params)\n","    train_acc, test_acc, effective_epochs, checkpoints = trainer.train()\n","    \"\"\"\n","    def __init__(self, model, dataloader, lr, params):\n","        self.model = model\n","        self.dataloader = dataloader\n","        self.params = params\n","        self.device = torch.device(params['device'])\n","        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n","        # optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n","        # Initialize best_model_state with the current model state\n","        self.best_model_state = copy.deepcopy(self.model.state_dict())\n","        self.max_val_acc = 0.\n","        self.no_improve_epochs = 0\n","        self.is_cnn = params.get('is_cnn', False)\n","        self.is_debug = params.get('is_debug', False)\n","        self.classification_report_flag = params.get('classification_report_flag', False)\n","        self.logger = params.get('logger', print)\n","\n","    def train_epoch(self):\n","      self.model.train()\n","      for batch_idx, (data, target) in enumerate(self.dataloader.train_loader):\n","          # Print the size of the current batch\n","          if self.is_cnn:\n","            data = data.view(data.size(0), 1, 28, 28)\n","          else:\n","            data = data.reshape([data.shape[0], -1])\n","          data, target = data.to(self.device), target.to(self.device)\n","          self.optimizer.zero_grad()\n","          output = self.model(data)\n","          loss = F.nll_loss(output, target)\n","          loss.backward()\n","          self.optimizer.step()\n","\n","          if self.is_debug and batch_idx % 20 == 0:\n","              self.logger(f\"Batch: {batch_idx}, Loss: {loss.item()}\")\n","\n","    def evaluate(self, loader):\n","        return eval(self.model, self.device, loader, self.is_debug, self.classification_report_flag, self.is_cnn)\n","\n","    def save_best_model(self):\n","        torch.save(self.model.state_dict(), 'best_model.pth')\n","\n","    def save_checkpoint(self, epoch, train_acc, val_acc):\n","        checkpoint = {\n","            'epoch': epoch,\n","            'model_state_dict': self.model.state_dict(),\n","            'optimizer_state_dict': self.optimizer.state_dict(),\n","            'train_acc': train_acc,\n","            'val_acc': val_acc\n","        }\n","        torch.save(checkpoint, f'checkpoint_epoch_{epoch}.pth')\n","        return checkpoint\n","\n","    def early_stopping_check(self, val_acc):\n","        if val_acc > self.max_val_acc:\n","            self.max_val_acc = val_acc\n","            self.no_improve_epochs = 0\n","            # Deep copy the model's state\n","            self.best_model_state = copy.deepcopy(self.model.state_dict())\n","            if self.params.get('save_best', False):\n","                self.save_best_model()\n","        else:\n","            self.no_improve_epochs += 1\n","            if self.no_improve_epochs >= self.params['early_stop_patience']:\n","                self.logger(\"Early stopping invoked.\")\n","                # Only load if best_model_state has been set\n","                if self.best_model_state is not None:\n","                    self.model.load_state_dict(self.best_model_state)\n","                return True\n","        return False\n","\n","    def train(self, verbose=1):\n","        effective_epochs = 0\n","        checkpoints = []\n","\n","        for epoch in range(self.params['num_train']):\n","            effective_epochs += 1\n","            self.train_epoch()\n","\n","            train_acc = self.evaluate(self.dataloader.train_loader)\n","            val_acc = self.evaluate(self.dataloader.val_loader)\n","            if verbose >= 1:\n","                self.logger(f'Epoch: {epoch} \\tTraining Accuracy: {train_acc*100:.2f}%')\n","                self.logger(f'Validation Accuracy: {val_acc*100:.2f}%')\n","\n","            if self.params.get('early_stop_patience', None):\n","                if self.early_stopping_check(val_acc):\n","                    self.model.load_state_dict(self.best_model_state)\n","                    break\n","\n","            if self.params.get('save_checkpoints', False):\n","                checkpoint = self.save_checkpoint(epoch, train_acc, val_acc)\n","                checkpoints.append(checkpoint)\n","\n","        # Final evaluations\n","        train_acc = self.evaluate(self.dataloader.train_loader)\n","        test_acc = self.evaluate(self.dataloader.test_loader)\n","\n","        return train_acc, test_acc, effective_epochs, checkpoints\n","\n","def eval(model, device, dataset_loader, debug=False, classification_report_flag=False, is_cnn=True, logger=print):\n","    \"\"\"\n","    Evaluates the model on the given dataset loader.\n","\n","    Parameters:\n","    - model: the PyTorch model to evaluate.\n","    - device: the device to which tensors should be moved before computation.\n","    - dataset_loader: DataLoader for evaluation.\n","    - debug: whether to print debug info like loss and accuracy.\n","    - classification_report_flag: whether to print a classification report.\n","    - is_cnn: a flag indicating if the model is a CNN. If it's not, the input data will be reshaped.\n","    - logger: logging function for printing messages.\n","\n","    Returns:\n","    - Accuracy of the model on the provided dataset loader.\n","\n","    Usage:\n","    - accuracy = eval(model, device, dataset_loader, debug=False, is_cnn=False, classification_report_flag=False)\n","    \"\"\"\n","\n","    model.eval()\n","    test_loss, correct = 0., 0.\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for data, target in dataset_loader:\n","            if is_cnn:\n","              data = data.view(data.size(0), 1, 28, 28)\n","            else:\n","              data = data.reshape([data.shape[0], -1])\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()\n","            pred = output.argmax(dim=1, keepdim=True)\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","            all_preds.extend(pred.cpu().numpy())\n","            all_labels.extend(target.cpu().numpy())\n","\n","    num_data = len(dataset_loader.dataset)\n","    test_loss /= num_data\n","    acc = correct / num_data\n","\n","    if debug:\n","        logger('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, num_data, 100. * acc))\n","\n","    if classification_report_flag:\n","        unique_labels = np.unique(all_labels).tolist()\n","        logger(classification_report(all_labels, all_preds, labels=unique_labels, target_names=[f'Class {i}' for i in unique_labels]))\n","\n","    return acc\n","\n","def cut_custom_cnn_model(model, cut_point, params):\n","    \"\"\"\n","    Cut the CustomCNN model at a specific layer and reinitialize the weights for layers after cut_point.\n","\n","    Parameters:\n","    - model (CustomCNN): Original CustomCNN model.\n","    - cut_point (int): Layer index (in terms of conv layers) at which to modify the model.\n","    - freeze (bool): If True, layers before cut_point will have their weights frozen.\n","    - reinitialize (bool): If True, layers after cut_point will have their weights reinitialized.\n","\n","    Returns:\n","    - new_model (CustomCNN): Modified model.\n","    \"\"\"\n","\n","    new_model = copy.deepcopy(model)\n","\n","    # Get names of layers in the model\n","    layer_names = list(new_model._modules.keys())\n","\n","    # Find indices of Conv layers\n","    conv_indices = [i for i, name in enumerate(layer_names) if 'conv' in name]\n","    #print(conv_indices)\n","\n","    # If freeze is True, set requires_grad to False for layers before cut_point\n","    if params[\"freeze\"]:\n","        for idx in conv_indices[:cut_point]:\n","            for param in getattr(new_model, layer_names[idx]).parameters():\n","                param.requires_grad = False\n","\n","    # Reinitialize layers after cut_point\n","    if params[\"reinit\"]:\n","        for idx in conv_indices[cut_point:]:\n","            layer = getattr(new_model, layer_names[idx])\n","            layer.reset_parameters()\n","            \"\"\"\n","            nn.init.kaiming_uniform_(layer.weight, a=0, mode='fan_in', nonlinearity='relu')\n","            if layer.bias is not None:\n","                nn.init.constant_(layer.bias, 0)\n","            \"\"\"\n","\n","    # if reinit_both_dense: reinit the one before the last one too\n","    if params[\"reinit_both_dense\"]:\n","        new_model.fc_1.reset_parameters()\n","\n","    # reinit the final dense layer anyway\n","    new_model.fc_2.reset_parameters()\n","    \n","    return new_model\n","\n","# --------------------------------- DATA UTILS -----------------------------------\n","def reduce_dataset(dataloader, percentage, balanced=True, seed=42):\n","\n","    \"\"\"\n","    Reduces the dataset to the given percentage. Can ensure class balance if needed.\n","\n","    Parameters:\n","    - dataloader: PyTorch DataLoader object.\n","    - percentage: Desired percentage of the original dataset.\n","    - balanced: If True, ensures class balance. If False, reduces randomly.\n","    - seed: Seed for reproducibility.\n","\n","    Returns:\n","    - reduced_dataloader: DataLoader with the reduced dataset.\n","    \"\"\"\n","    # Extract the dataset from the dataloader\n","    dataset = dataloader.dataset\n","\n","    # Extract all data and labels from the dataset\n","    X = [dataset[i][0] for i in range(len(dataset))]\n","    y = [dataset[i][1] for i in range(len(dataset))]\n","\n","    # Set the seed for reproducibility\n","    torch.manual_seed(seed)\n","\n","    if not balanced:\n","        # Determine the number of samples to keep\n","        num_samples = int(len(dataset) * percentage)\n","\n","        # Randomly select indices without replacement\n","        indices = torch.randperm(len(dataset))[:num_samples].tolist()\n","\n","    else:\n","        # Get unique classes and their counts\n","        classes, class_counts = torch.unique(torch.tensor(y), return_counts=True)\n","\n","        # Determine the number of samples per class to keep\n","        num_samples_per_class = int(len(dataset) * percentage / len(classes))\n","        indices = []\n","\n","        for class_label in classes:\n","            class_indices = [i for i, label in enumerate(y) if label == class_label]\n","\n","            # Randomly select indices without replacement for each class\n","            class_selected_indices = torch.randperm(len(class_indices))[:num_samples_per_class].tolist()\n","            indices.extend([class_indices[i] for i in class_selected_indices])\n","\n","    # Use a Subset of the original dataset to create a reduced dataset\n","    reduced_dataset = data.Subset(dataset, indices)\n","\n","    # Create a DataLoader with the reduced dataset.\n","    reduced_dataloader = data.DataLoader(reduced_dataset, batch_size=dataloader.batch_size, shuffle=True)\n","\n","    return reduced_dataloader\n","\n","class RelabeledSubset(torch.utils.data.Dataset):\n","    def __init__(self, dataset, offset):\n","        self.dataset = dataset\n","        self.offset = offset\n","\n","    def __getitem__(self, idx):\n","        data, label = self.dataset[idx]\n","        # Offset the label to start from 0\n","        label = label - self.offset\n","        return data, label\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","class TransferLearningMNIST(object):\n","    def __init__(self, batch_size, input_dim=28*28, val_split=0.1, num_workers=0, seed=42):\n","        self.input_dim = input_dim\n","        self.output_dim = 10\n","        self.val_split = val_split\n","\n","        def filter_dataset(dataset, classes):\n","            indices = [i for i, t in enumerate(dataset.targets) if t in classes]\n","            return torch.utils.data.Subset(dataset, indices)\n","\n","        mnist_train_data = datasets.MNIST(\n","            '../data',\n","            train=True,\n","            download=True,\n","            transform=transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Lambda(lambda x: (x * 2 - 1) * 0.5),\n","            ]))\n","\n","        pretrain_train_data = filter_dataset(mnist_train_data, list(range(5)))\n","        finetune_train_data = filter_dataset(mnist_train_data, list(range(5, 10)))\n","\n","        pretrain_len = len(pretrain_train_data)\n","        finetune_len = len(finetune_train_data)\n","        pretrain_val_len = int(val_split * pretrain_len)\n","        finetune_val_len = int(val_split * finetune_len)\n","        pretrain_train_set, pretrain_val_set = torch.utils.data.random_split(\n","            pretrain_train_data, [pretrain_len - pretrain_val_len, pretrain_val_len], generator=torch.Generator().manual_seed(seed))\n","        finetune_train_set, finetune_val_set = torch.utils.data.random_split(\n","            finetune_train_data, [finetune_len - finetune_val_len, finetune_val_len], generator=torch.Generator().manual_seed(seed))\n","\n","        self.pretrain_train_loader = torch.utils.data.DataLoader(pretrain_train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n","        self.pretrain_val_loader = torch.utils.data.DataLoader(pretrain_val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","\n","        # Use the RelabeledSubset for fine-tuning datasets\n","        finetune_train_set = RelabeledSubset(finetune_train_set, 5)\n","        finetune_val_set = RelabeledSubset(finetune_val_set, 5)\n","\n","        self.finetune_train_loader = torch.utils.data.DataLoader(finetune_train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n","        self.finetune_val_loader = torch.utils.data.DataLoader(finetune_val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","\n","        mnist_test_data = datasets.MNIST(\n","            '../data',\n","            train=False,\n","            download=True,\n","            transform=transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Lambda(lambda x: (x * 2 - 1) * 0.5),\n","            ]))\n","\n","        pretrain_test_data = filter_dataset(mnist_test_data, list(range(5)))\n","        finetune_test_data = filter_dataset(mnist_test_data, list(range(5, 10)))\n","\n","        self.pretrain_test_loader = torch.utils.data.DataLoader(pretrain_test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","\n","        # Use the RelabeledSubset for fine-tuning test datasets\n","        finetune_test_data = RelabeledSubset(finetune_test_data, 5)\n","        self.finetune_test_loader = torch.utils.data.DataLoader(finetune_test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","\n","        # Complete test loader contains all test examples.\n","        self.complete_test_loader = torch.utils.data.DataLoader(mnist_test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","\n","class TransferLearningMNISTWrapper:\n","    \"\"\"\n","    This wrapper class provides a convenient way to switch between pretraining and fine-tuning phases.\n","\n","    It allows for changing the phase and accordingly updating the data loaders (train, val, test)\n","    to either pretraining or fine-tuning sets.\n","    \"\"\"\n","    def __init__(self, transferLearningMNISTObj, phase):\n","        \"\"\"\n","        Initializes the TransferLearningMNISTWrapper object.\n","\n","        Parameters:\n","        - transferLearningMNISTObj: An instance of the TransferLearningMNIST class.\n","        - phase: String indicating the current phase (\"pretrain\" or \"finetune\").\n","        \"\"\"\n","        self.transferLearningMNISTObj = transferLearningMNISTObj\n","        self.phase = phase\n","        self.input_dim = self.transferLearningMNISTObj.input_dim\n","        self.output_dim = self.transferLearningMNISTObj.output_dim\n","        self.update_phase(phase)\n","\n","    def update_phase(self, phase):\n","        \"\"\"\n","        Updates the phase and the corresponding data loaders.\n","\n","        Parameters:\n","        - phase: String indicating the desired phase (\"pretrain\" or \"finetune\").\n","\n","        Throws:\n","        - ValueError: If the phase is neither \"pretrain\" nor \"finetune\".\n","        \"\"\"\n","        self.phase = phase\n","        if phase == 'pretrain':\n","            self.train_loader = self.transferLearningMNISTObj.pretrain_train_loader\n","            self.val_loader = self.transferLearningMNISTObj.pretrain_val_loader\n","            self.test_loader = self.transferLearningMNISTObj.pretrain_test_loader\n","        elif phase == 'finetune':\n","            self.train_loader = self.transferLearningMNISTObj.finetune_train_loader\n","            self.val_loader = self.transferLearningMNISTObj.finetune_val_loader\n","            self.test_loader = self.transferLearningMNISTObj.finetune_test_loader\n","        else:\n","            raise ValueError('Phase must be either \"pretrain\" or \"finetune\".')\n","\n","    def get_current_phase(self):\n","      return self.phase\n","\n","# ------------------------------------------ PLOTTING UTILS -------------------------------------------\n","'''def effective_rank(singular_values):\n","    sigma_max = np.max(singular_values)\n","    sigma_min = singular_values[-1] if singular_values[-1] > 0 else np.min(singular_values[singular_values > 0])\n","    # print(sigma_max, sigma_min)\n","    print(np.sqrt(sigma_max / sigma_min))\n","    print('----')\n","    return np.sqrt(sigma_max / sigma_min)'''\n","\n","def effective_rank(singular_values):\n","    normalized_singular_values = singular_values / np.sum(singular_values)\n","    entropy = -np.sum(normalized_singular_values * np.log(normalized_singular_values))\n","    eff_rank = np.exp(entropy)\n","    return eff_rank\n","\n","def plot_layer_effective_ranks(model, print_ranks=True):\n","    effective_ranks = []\n","    layer_names = []\n","\n","    for name, param in model.named_parameters():\n","        if 'weight' in name:  # We are only interested in weight matrices\n","            weight_matrix = param.detach().cpu().numpy()\n","            singular_values = np.linalg.svd(weight_matrix, compute_uv=False)\n","            eff_rank = effective_rank(singular_values)\n","            effective_ranks.append(eff_rank)\n","            layer_names.append(name)\n","\n","    if print_ranks:\n","        for layer_name, eff_rank in zip(layer_names, effective_ranks):\n","            print(f'{layer_name}: {eff_rank:.4f}')\n","\n","    # Plotting\n","    plt.figure(figsize=(15, 5))\n","    plt.bar(layer_names, effective_ranks, color='green')\n","    plt.xlabel('Layer')\n","    plt.ylabel('Effective Rank')\n","    plt.title('Effective Rank of Weight Matrices for Each Layer')\n","    plt.grid(True)\n","\n","    y_max = np.max(effective_ranks) + 1  # Get maximum rank and add 1 for better visualization\n","    y_min = np.min(effective_ranks) - 1  # Get minimum rank and subtract 1 for better visualization\n","    plt.yticks(np.arange(0, int(y_max)+2, step=2))  # Set yticks\n","\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"NJ5JiRZYQW-J"},"source":["# EXPERIMENT SETUP 1: _FREEZE, REINIT, POOLING, DENSE:REINIT BOTH_\n","- percentages_set_1 = [0.001, 0.002, 0.005, 0.01, 0.05, 0.1, 0.3, 0.5, 0.8, 1]\n","- dataset: same as before MNIST 5 to 5\n","\n","- architecture:\n","  - Conv 1 (5,5), channels=10\n","  - Relu\n","  - Conv 2 (5,5), channels=10\n","  - Relu\n","  - Conv 3 (5,5), channels=10\n","  - Relu\n","  - _POOLING_\n","  - Dense 1 (x, a) x=output shape of prev layer, a:random hidden layer width (we use 128)\n","  - Relu\n","  - Dense 2 (a, 5)\n","  - softmax\n","\n","- lr pretraining = 0.001\n","- lr finetuning = 0.0001\n","- lr end-to-end = 0.001\n","\n","- Freezing the layers before the cut: _YES_\n","- Reinitializing the Convolutional layers after the cut: _YES_\n","- Reinitializing Dense 1: _YES_\n","- Reinitializing Dense 2: _YES_\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T15:26:47.336625Z","iopub.status.busy":"2023-11-12T15:26:47.335716Z","iopub.status.idle":"2023-11-12T15:26:47.341905Z","shell.execute_reply":"2023-11-12T15:26:47.340983Z","shell.execute_reply.started":"2023-11-12T15:26:47.336587Z"},"id":"FeWfmLUgswad","trusted":true},"outputs":[],"source":["percentages = [0.001, 0.002, 0.005, 0.01, 0.05, 0.1, 0.3, 0.5, 0.8, 1]\n","\n","# cuts=0 means: end-to-end model if we are reinitializing\n","cuts = [0,1,2,3]\n","seed_set = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]  # currently not being used\n","batch_size = 4096"]},{"cell_type":"markdown","metadata":{"id":"UbrS0kwrcHfE"},"source":["## Pretraining\n","\n"]},{"cell_type":"code","execution_count":150,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-11-12T15:26:47.343864Z","iopub.status.busy":"2023-11-12T15:26:47.343558Z","iopub.status.idle":"2023-11-12T15:26:54.538341Z","shell.execute_reply":"2023-11-12T15:26:54.537339Z","shell.execute_reply.started":"2023-11-12T15:26:47.343839Z"},"id":"6sK-eF2ucUDa","outputId":"f7f2e890-7c35-48ee-edab-dc0e97d5ce52","trusted":true},"outputs":[{"data":{"text/plain":["CustomCNN(\n","  (conv0): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","  (act0): ReLU()\n","  (conv1): Conv2d(10, 10, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","  (act1): ReLU()\n","  (conv2): Conv2d(10, 10, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","  (act2): ReLU()\n","  (fc_1): Linear(in_features=7840, out_features=128, bias=True)\n","  (relu): ReLU()\n","  (fc_2): Linear(in_features=128, out_features=10, bias=True)\n","  (logsoftmax): LogSoftmax(dim=1)\n",")"]},"execution_count":150,"metadata":{},"output_type":"execute_result"}],"source":["dataloader = TransferLearningMNIST(batch_size)\n","dataloader_wrapped = TransferLearningMNISTWrapper(dataloader, phase = 'pretrain')\n","\n","# Changes Here for the experiments\n","params = {\n","      'depth': 3,\n","      'width': 10, # num channels for CNN\n","      'hidden_dim_lin': 128,\n","      'activation_function': nn.ReLU,\n","      'kernel_size': 5,\n","      'device': device,\n","      'lr_pretrain': 0.001,   \n","      'lr_fine_tune': 0.001,  # CHANGE: if no layer-wise lr\n","      'lr_fine_tune_reinit': 0.001,         # CHANGE: if no layer-wise lr\n","      'lr_fine_tune_no_reinit': 0.0001,     # CHANGE: if layer-wise lr\n","      'num_train': 40,\n","      'early_stop_patience': 6,\n","      'save_best': False,\n","      'save_checkpoints': False,\n","      'is_cnn': True,\n","      'is_debug': False,\n","      'classification_report_flag': False,\n","      'percentages':percentages,\n","      'batch_size':batch_size,\n","      'seed_set':seed_set,\n","      'use_pooling': False,   # CHANGE\n","      'freeze': True,         # CHANGE: freeze the conv layers before the cut\n","      'reinit': True,         # CHANGE: reinit the conv lyers only after the cut\n","      'reinit_both_dense': True   # CHANGE: True for reinitialize both dense layers, False for reinit only the last dense layer\n","    }\n","\n","# Create DNN model\n","pretrained_model = generate_cnn(input_dim = 28, output_dim = 10, depth = params['depth'], num_channels = params['width'],\n","                    hidden_dim_lin = params['hidden_dim_lin'], kernel_size = params['kernel_size'], activation_function = params[\"activation_function\"], use_pooling=params['use_pooling'])\n","pretrained_model.to(device)"]},{"cell_type":"code","execution_count":74,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-11-12T15:26:54.539725Z","iopub.status.busy":"2023-11-12T15:26:54.539463Z","iopub.status.idle":"2023-11-12T15:29:07.301509Z","shell.execute_reply":"2023-11-12T15:29:07.300272Z","shell.execute_reply.started":"2023-11-12T15:26:54.539702Z"},"id":"gU9NwDAjhT3o","outputId":"7335d6d9-a841-4397-b879-6ceef21ab105","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0 \tTraining Accuracy: 22.06%\n","Validation Accuracy: 21.90%\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/mnt/c/Users/user/Desktop/MP/mp-tl-study/50-50_MNIST/MNIST_CNN_3D_more_repeats_freeze_reinit.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/user/Desktop/MP/mp-tl-study/50-50_MNIST/MNIST_CNN_3D_more_repeats_freeze_reinit.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train and evaluate\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/user/Desktop/MP/mp-tl-study/50-50_MNIST/MNIST_CNN_3D_more_repeats_freeze_reinit.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(pretrained_model, dataloader_wrapped, params[\u001b[39m\"\u001b[39m\u001b[39mlr_pretrain\u001b[39m\u001b[39m\"\u001b[39m], params)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/user/Desktop/MP/mp-tl-study/50-50_MNIST/MNIST_CNN_3D_more_repeats_freeze_reinit.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m train_acc, test_acc, effective_epochs, checkpoints \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/user/Desktop/MP/mp-tl-study/50-50_MNIST/MNIST_CNN_3D_more_repeats_freeze_reinit.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFinal Training Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/user/Desktop/MP/mp-tl-study/50-50_MNIST/MNIST_CNN_3D_more_repeats_freeze_reinit.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFinal Test Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtest_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n","\u001b[1;32m/mnt/c/Users/user/Desktop/MP/mp-tl-study/50-50_MNIST/MNIST_CNN_3D_more_repeats_freeze_reinit.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/user/Desktop/MP/mp-tl-study/50-50_MNIST/MNIST_CNN_3D_more_repeats_freeze_reinit.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=201'>202</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mnum_train\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/user/Desktop/MP/mp-tl-study/50-50_MNIST/MNIST_CNN_3D_more_repeats_freeze_reinit.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=202'>203</a>\u001b[0m     effective_epochs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/user/Desktop/MP/mp-tl-study/50-50_MNIST/MNIST_CNN_3D_more_repeats_freeze_reinit.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_epoch()\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/user/Desktop/MP/mp-tl-study/50-50_MNIST/MNIST_CNN_3D_more_repeats_freeze_reinit.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=205'>206</a>\u001b[0m     train_acc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader\u001b[39m.\u001b[39mtrain_loader)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/user/Desktop/MP/mp-tl-study/50-50_MNIST/MNIST_CNN_3D_more_repeats_freeze_reinit.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=206'>207</a>\u001b[0m     val_acc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader\u001b[39m.\u001b[39mval_loader)\n","\u001b[1;32m/mnt/c/Users/user/Desktop/MP/mp-tl-study/50-50_MNIST/MNIST_CNN_3D_more_repeats_freeze_reinit.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/user/Desktop/MP/mp-tl-study/50-50_MNIST/MNIST_CNN_3D_more_repeats_freeze_reinit.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=144'>145</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_epoch\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/user/Desktop/MP/mp-tl-study/50-50_MNIST/MNIST_CNN_3D_more_repeats_freeze_reinit.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=145'>146</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/user/Desktop/MP/mp-tl-study/50-50_MNIST/MNIST_CNN_3D_more_repeats_freeze_reinit.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=146'>147</a>\u001b[0m   \u001b[39mfor\u001b[39;00m batch_idx, (data, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader\u001b[39m.\u001b[39mtrain_loader):\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/user/Desktop/MP/mp-tl-study/50-50_MNIST/MNIST_CNN_3D_more_repeats_freeze_reinit.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=147'>148</a>\u001b[0m       \u001b[39m# Print the size of the current batch\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/user/Desktop/MP/mp-tl-study/50-50_MNIST/MNIST_CNN_3D_more_repeats_freeze_reinit.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=148'>149</a>\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_cnn:\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/user/Desktop/MP/mp-tl-study/50-50_MNIST/MNIST_CNN_3D_more_repeats_freeze_reinit.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=149'>150</a>\u001b[0m         data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mview(data\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m1\u001b[39m, \u001b[39m28\u001b[39m, \u001b[39m28\u001b[39m)\n","File \u001b[0;32m~/anaconda3/envs/mp-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m~/anaconda3/envs/mp-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m~/anaconda3/envs/mp-env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m~/anaconda3/envs/mp-env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m~/anaconda3/envs/mp-env/lib/python3.11/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n","File \u001b[0;32m~/anaconda3/envs/mp-env/lib/python3.11/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n","File \u001b[0;32m~/anaconda3/envs/mp-env/lib/python3.11/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n","File \u001b[0;32m~/anaconda3/envs/mp-env/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Train and evaluate\n","trainer = Trainer(pretrained_model, dataloader_wrapped, params[\"lr_pretrain\"], params)\n","train_acc, test_acc, effective_epochs, checkpoints = trainer.train()\n","\n","print(f\"Final Training Accuracy: {train_acc:.4f}\")\n","print(f\"Final Test Accuracy: {test_acc:.4f}\")"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Average loss: 0.0317, Accuracy: 3023.0/3059 (99%)\n","\n","              precision    recall  f1-score   support\n","\n","     Class 0       0.99      0.99      0.99       544\n","     Class 1       1.00      0.99      0.99       670\n","     Class 2       0.97      0.99      0.98       631\n","     Class 3       0.99      0.98      0.98       613\n","     Class 4       1.00      1.00      1.00       601\n","\n","    accuracy                           0.99      3059\n","   macro avg       0.99      0.99      0.99      3059\n","weighted avg       0.99      0.99      0.99      3059\n","\n"]},{"data":{"text/plain":["0.9882314481856816"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["eval(pretrained_model, device, dataloader_wrapped.val_loader, debug=True, classification_report_flag=True, is_cnn=True)"]},{"cell_type":"markdown","metadata":{"id":"ge8Q1YiEqC7e"},"source":["## Fine-tuning Experiments"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# load results: to continue from a checkpoint (actually don't run)\n","with open('results.json', 'r') as f:\n","    results = json.load(f)"]},{"cell_type":"markdown","metadata":{},"source":["### Baselines (skip if it was already run)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Sampled Percentage: 0.001, Lr: 0.0001, Repeat: 0\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.001, Lr: 0.0001, Repeat: 1\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.001, Lr: 0.0001, Repeat: 2\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.1971\n","\n","Sampled Percentage: 0.001, Lr: 0.0001, Repeat: 3\n","Early stopping invoked.\n","Training Accuracy: 0.2400, Test Accuracy: 0.2707\n","\n","Sampled Percentage: 0.001, Lr: 0.0001, Repeat: 4\n","Early stopping invoked.\n","Training Accuracy: 0.6000, Test Accuracy: 0.3329\n","\n","Sampled Percentage: 0.001, Lr: 0.0001, Repeat: 5\n","Early stopping invoked.\n","Training Accuracy: 0.4000, Test Accuracy: 0.3273\n","\n","Sampled Percentage: 0.001, Lr: 0.0001, Repeat: 6\n","Early stopping invoked.\n","Training Accuracy: 0.4000, Test Accuracy: 0.2263\n","\n","Sampled Percentage: 0.001, Lr: 0.0001, Repeat: 7\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2115\n","\n","Sampled Percentage: 0.001, Lr: 0.0001, Repeat: 8\n","Early stopping invoked.\n","Training Accuracy: 0.3600, Test Accuracy: 0.2331\n","\n","Sampled Percentage: 0.001, Lr: 0.0001, Repeat: 9\n","Early stopping invoked.\n","Training Accuracy: 0.4000, Test Accuracy: 0.3308\n","\n","Sampled Percentage: 0.002, Lr: 0.0001, Repeat: 0\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.002, Lr: 0.0001, Repeat: 1\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.002, Lr: 0.0001, Repeat: 2\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.1971\n","\n","Sampled Percentage: 0.002, Lr: 0.0001, Repeat: 3\n","Early stopping invoked.\n","Training Accuracy: 0.2800, Test Accuracy: 0.2611\n","\n","Sampled Percentage: 0.002, Lr: 0.0001, Repeat: 4\n","Early stopping invoked.\n","Training Accuracy: 0.5600, Test Accuracy: 0.3499\n","\n","Sampled Percentage: 0.002, Lr: 0.0001, Repeat: 5\n","Early stopping invoked.\n","Training Accuracy: 0.4000, Test Accuracy: 0.3407\n","\n","Sampled Percentage: 0.002, Lr: 0.0001, Repeat: 6\n","Early stopping invoked.\n","Training Accuracy: 0.4000, Test Accuracy: 0.3347\n","\n","Sampled Percentage: 0.002, Lr: 0.0001, Repeat: 7\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2115\n","\n","Sampled Percentage: 0.002, Lr: 0.0001, Repeat: 8\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2115\n","\n","Sampled Percentage: 0.002, Lr: 0.0001, Repeat: 9\n","Early stopping invoked.\n","Training Accuracy: 0.2400, Test Accuracy: 0.2100\n","\n","Sampled Percentage: 0.005, Lr: 0.0001, Repeat: 0\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.005, Lr: 0.0001, Repeat: 1\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.005, Lr: 0.0001, Repeat: 2\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.1971\n","\n","Sampled Percentage: 0.005, Lr: 0.0001, Repeat: 3\n","Early stopping invoked.\n","Training Accuracy: 0.2462, Test Accuracy: 0.2244\n","\n","Sampled Percentage: 0.005, Lr: 0.0001, Repeat: 4\n","Early stopping invoked.\n","Training Accuracy: 0.5231, Test Accuracy: 0.4631\n","\n","Sampled Percentage: 0.005, Lr: 0.0001, Repeat: 5\n","Early stopping invoked.\n","Training Accuracy: 0.3462, Test Accuracy: 0.3329\n","\n","Sampled Percentage: 0.005, Lr: 0.0001, Repeat: 6\n","Early stopping invoked.\n","Training Accuracy: 0.3923, Test Accuracy: 0.3512\n","\n","Sampled Percentage: 0.005, Lr: 0.0001, Repeat: 7\n","Early stopping invoked.\n","Training Accuracy: 0.4000, Test Accuracy: 0.3839\n","\n","Sampled Percentage: 0.005, Lr: 0.0001, Repeat: 8\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2115\n","\n","Sampled Percentage: 0.005, Lr: 0.0001, Repeat: 9\n","Early stopping invoked.\n","Training Accuracy: 0.4615, Test Accuracy: 0.3837\n","\n","Sampled Percentage: 0.01, Lr: 0.0001, Repeat: 0\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.01, Lr: 0.0001, Repeat: 1\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.01, Lr: 0.0001, Repeat: 2\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.1971\n","\n","Sampled Percentage: 0.01, Lr: 0.0001, Repeat: 3\n","Early stopping invoked.\n","Training Accuracy: 0.3077, Test Accuracy: 0.2833\n","\n","Sampled Percentage: 0.01, Lr: 0.0001, Repeat: 4\n","Early stopping invoked.\n","Training Accuracy: 0.5038, Test Accuracy: 0.4801\n","\n","Sampled Percentage: 0.01, Lr: 0.0001, Repeat: 5\n","Early stopping invoked.\n","Training Accuracy: 0.3269, Test Accuracy: 0.3150\n","\n","Sampled Percentage: 0.01, Lr: 0.0001, Repeat: 6\n","Early stopping invoked.\n","Training Accuracy: 0.3731, Test Accuracy: 0.3553\n","\n","Sampled Percentage: 0.01, Lr: 0.0001, Repeat: 7\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2115\n","\n","Sampled Percentage: 0.01, Lr: 0.0001, Repeat: 8\n","Early stopping invoked.\n","Training Accuracy: 0.2038, Test Accuracy: 0.2144\n","\n","Sampled Percentage: 0.01, Lr: 0.0001, Repeat: 9\n","Early stopping invoked.\n","Training Accuracy: 0.2038, Test Accuracy: 0.1878\n","\n","Sampled Percentage: 0.05, Lr: 0.0001, Repeat: 0\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.05, Lr: 0.0001, Repeat: 1\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.05, Lr: 0.0001, Repeat: 2\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.1971\n","\n","Sampled Percentage: 0.05, Lr: 0.0001, Repeat: 3\n","Early stopping invoked.\n","Training Accuracy: 0.2553, Test Accuracy: 0.2695\n","\n","Sampled Percentage: 0.05, Lr: 0.0001, Repeat: 4\n","Early stopping invoked.\n","Training Accuracy: 0.3758, Test Accuracy: 0.3767\n","\n","Sampled Percentage: 0.1, Lr: 0.0001, Repeat: 0\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.1, Lr: 0.0001, Repeat: 1\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.1, Lr: 0.0001, Repeat: 2\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.1971\n","\n","Sampled Percentage: 0.1, Lr: 0.0001, Repeat: 3\n","Early stopping invoked.\n","Training Accuracy: 0.2851, Test Accuracy: 0.3001\n","\n","Sampled Percentage: 0.1, Lr: 0.0001, Repeat: 4\n","Early stopping invoked.\n","Training Accuracy: 0.3716, Test Accuracy: 0.3765\n","\n","Sampled Percentage: 0.3, Lr: 0.0001, Repeat: 0\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.3, Lr: 0.0001, Repeat: 1\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.3, Lr: 0.0001, Repeat: 2\n","Early stopping invoked.\n","Training Accuracy: 0.3628, Test Accuracy: 0.3436\n","\n","Sampled Percentage: 0.3, Lr: 0.0001, Repeat: 3\n","Early stopping invoked.\n","Training Accuracy: 0.2001, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.3, Lr: 0.0001, Repeat: 4\n","Early stopping invoked.\n","Training Accuracy: 0.3719, Test Accuracy: 0.3779\n","\n","Sampled Percentage: 0.5, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9137, Test Accuracy: 0.9144\n","\n","Sampled Percentage: 0.5, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9307, Test Accuracy: 0.9282\n","\n","Sampled Percentage: 0.5, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9177, Test Accuracy: 0.9194\n","\n","Sampled Percentage: 0.8, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9335, Test Accuracy: 0.9311\n","\n","Sampled Percentage: 0.8, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9481, Test Accuracy: 0.9430\n","\n","Sampled Percentage: 0.8, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9286, Test Accuracy: 0.9278\n","\n","Sampled Percentage: 1, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9397, Test Accuracy: 0.9389\n","\n","Sampled Percentage: 1, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9520, Test Accuracy: 0.9484\n","\n","Sampled Percentage: 1, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9356, Test Accuracy: 0.9350\n","\n","Sampled Percentage: 0.001, Lr: 0.0001, Repeat: 0\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.001, Lr: 0.0001, Repeat: 1\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.001, Lr: 0.0001, Repeat: 2\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.1971\n","\n","Sampled Percentage: 0.001, Lr: 0.0001, Repeat: 3\n","Early stopping invoked.\n","Training Accuracy: 0.2400, Test Accuracy: 0.2707\n","\n","Sampled Percentage: 0.001, Lr: 0.0001, Repeat: 4\n","Early stopping invoked.\n","Training Accuracy: 0.6000, Test Accuracy: 0.3329\n","\n","Sampled Percentage: 0.001, Lr: 0.0001, Repeat: 5\n","Early stopping invoked.\n","Training Accuracy: 0.4000, Test Accuracy: 0.3273\n","\n","Sampled Percentage: 0.001, Lr: 0.0001, Repeat: 6\n","Early stopping invoked.\n","Training Accuracy: 0.4000, Test Accuracy: 0.2263\n","\n","Sampled Percentage: 0.001, Lr: 0.0001, Repeat: 7\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2115\n","\n","Sampled Percentage: 0.001, Lr: 0.0001, Repeat: 8\n","Early stopping invoked.\n","Training Accuracy: 0.3600, Test Accuracy: 0.2331\n","\n","Sampled Percentage: 0.001, Lr: 0.0001, Repeat: 9\n","Early stopping invoked.\n","Training Accuracy: 0.4000, Test Accuracy: 0.3308\n","\n","Sampled Percentage: 0.002, Lr: 0.0001, Repeat: 0\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.002, Lr: 0.0001, Repeat: 1\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.002, Lr: 0.0001, Repeat: 2\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.1971\n","\n","Sampled Percentage: 0.002, Lr: 0.0001, Repeat: 3\n","Early stopping invoked.\n","Training Accuracy: 0.2800, Test Accuracy: 0.2611\n","\n","Sampled Percentage: 0.002, Lr: 0.0001, Repeat: 4\n","Early stopping invoked.\n","Training Accuracy: 0.5600, Test Accuracy: 0.3499\n","\n","Sampled Percentage: 0.002, Lr: 0.0001, Repeat: 5\n","Early stopping invoked.\n","Training Accuracy: 0.4000, Test Accuracy: 0.3407\n","\n","Sampled Percentage: 0.002, Lr: 0.0001, Repeat: 6\n","Early stopping invoked.\n","Training Accuracy: 0.4000, Test Accuracy: 0.3347\n","\n","Sampled Percentage: 0.002, Lr: 0.0001, Repeat: 7\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2115\n","\n","Sampled Percentage: 0.002, Lr: 0.0001, Repeat: 8\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2115\n","\n","Sampled Percentage: 0.002, Lr: 0.0001, Repeat: 9\n","Early stopping invoked.\n","Training Accuracy: 0.2400, Test Accuracy: 0.2100\n","\n","Sampled Percentage: 0.005, Lr: 0.0001, Repeat: 0\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.005, Lr: 0.0001, Repeat: 1\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.005, Lr: 0.0001, Repeat: 2\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.1971\n","\n","Sampled Percentage: 0.005, Lr: 0.0001, Repeat: 3\n","Early stopping invoked.\n","Training Accuracy: 0.2462, Test Accuracy: 0.2244\n","\n","Sampled Percentage: 0.005, Lr: 0.0001, Repeat: 4\n","Early stopping invoked.\n","Training Accuracy: 0.5231, Test Accuracy: 0.4631\n","\n","Sampled Percentage: 0.005, Lr: 0.0001, Repeat: 5\n","Early stopping invoked.\n","Training Accuracy: 0.3462, Test Accuracy: 0.3329\n","\n","Sampled Percentage: 0.005, Lr: 0.0001, Repeat: 6\n","Early stopping invoked.\n","Training Accuracy: 0.3923, Test Accuracy: 0.3512\n","\n","Sampled Percentage: 0.005, Lr: 0.0001, Repeat: 7\n","Early stopping invoked.\n","Training Accuracy: 0.4000, Test Accuracy: 0.3839\n","\n","Sampled Percentage: 0.005, Lr: 0.0001, Repeat: 8\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2115\n","\n","Sampled Percentage: 0.005, Lr: 0.0001, Repeat: 9\n","Early stopping invoked.\n","Training Accuracy: 0.4615, Test Accuracy: 0.3837\n","\n","Sampled Percentage: 0.01, Lr: 0.0001, Repeat: 0\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.01, Lr: 0.0001, Repeat: 1\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.01, Lr: 0.0001, Repeat: 2\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.1971\n","\n","Sampled Percentage: 0.01, Lr: 0.0001, Repeat: 3\n","Early stopping invoked.\n","Training Accuracy: 0.3077, Test Accuracy: 0.2833\n","\n","Sampled Percentage: 0.01, Lr: 0.0001, Repeat: 4\n","Early stopping invoked.\n","Training Accuracy: 0.5038, Test Accuracy: 0.4801\n","\n","Sampled Percentage: 0.01, Lr: 0.0001, Repeat: 5\n","Early stopping invoked.\n","Training Accuracy: 0.3269, Test Accuracy: 0.3150\n","\n","Sampled Percentage: 0.01, Lr: 0.0001, Repeat: 6\n","Early stopping invoked.\n","Training Accuracy: 0.3731, Test Accuracy: 0.3553\n","\n","Sampled Percentage: 0.01, Lr: 0.0001, Repeat: 7\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2115\n","\n","Sampled Percentage: 0.01, Lr: 0.0001, Repeat: 8\n","Early stopping invoked.\n","Training Accuracy: 0.2038, Test Accuracy: 0.2144\n","\n","Sampled Percentage: 0.01, Lr: 0.0001, Repeat: 9\n","Early stopping invoked.\n","Training Accuracy: 0.2038, Test Accuracy: 0.1878\n","\n","Sampled Percentage: 0.05, Lr: 0.0001, Repeat: 0\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.05, Lr: 0.0001, Repeat: 1\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.2076\n","\n","Sampled Percentage: 0.05, Lr: 0.0001, Repeat: 2\n","Early stopping invoked.\n","Training Accuracy: 0.2000, Test Accuracy: 0.1971\n","\n","Sampled Percentage: 0.05, Lr: 0.0001, Repeat: 3\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mSampled Percentage: \u001b[39m\u001b[39m{\u001b[39;00msampled_percentage\u001b[39m}\u001b[39;00m\u001b[39m, Lr: \u001b[39m\u001b[39m{\u001b[39;00mlr\u001b[39m}\u001b[39;00m\u001b[39m, Repeat: \u001b[39m\u001b[39m{\u001b[39;00mrepeat\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[39m# Reduce the dataset\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m train_loader_reduced \u001b[39m=\u001b[39m reduce_dataset(dataloader_wrapped\u001b[39m.\u001b[39;49mtrain_loader, sampled_percentage, seed \u001b[39m=\u001b[39;49m \u001b[39m42\u001b[39;49m)\n\u001b[1;32m     25\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(repeat)\n\u001b[1;32m     26\u001b[0m \u001b[39m#train_loader_reduced = reduce_dataset(dataloader_wrapped.train_loader, sampled_percentage, seed = repeat)\u001b[39;00m\n","Cell \u001b[0;32mIn[1], line 346\u001b[0m, in \u001b[0;36mreduce_dataset\u001b[0;34m(dataloader, percentage, balanced, seed)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39m# Extract all data and labels from the dataset\u001b[39;00m\n\u001b[1;32m    345\u001b[0m X \u001b[39m=\u001b[39m [dataset[i][\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dataset))]\n\u001b[0;32m--> 346\u001b[0m y \u001b[39m=\u001b[39m [dataset[i][\u001b[39m1\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(dataset))]\n\u001b[1;32m    348\u001b[0m \u001b[39m# Set the seed for reproducibility\u001b[39;00m\n\u001b[1;32m    349\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(seed)\n","Cell \u001b[0;32mIn[1], line 346\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39m# Extract all data and labels from the dataset\u001b[39;00m\n\u001b[1;32m    345\u001b[0m X \u001b[39m=\u001b[39m [dataset[i][\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dataset))]\n\u001b[0;32m--> 346\u001b[0m y \u001b[39m=\u001b[39m [dataset[i][\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dataset))]\n\u001b[1;32m    348\u001b[0m \u001b[39m# Set the seed for reproducibility\u001b[39;00m\n\u001b[1;32m    349\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(seed)\n","Cell \u001b[0;32mIn[1], line 387\u001b[0m, in \u001b[0;36mRelabeledSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m--> 387\u001b[0m     data, label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx]\n\u001b[1;32m    388\u001b[0m     \u001b[39m# Offset the label to start from 0\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     label \u001b[39m=\u001b[39m label \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffset\n","File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n","File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n","File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.11/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n","File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#training of baseline, end to end, models (#trials x #percentages)\n","\n","dataloader_wrapped.update_phase('finetune')\n","\n","# template_model = generate_cnn(input_dim = 28, output_dim = 10, depth = params['depth'], num_channels = params['width'],\n","#                      hidden_dim_lin = params['hidden_dim_lin'], kernel_size = params['kernel_size'], activation_function = params[\"activation_function\"], use_pooling=params['use_pooling'])\n","\n","for sampled_percentage in percentages:      \n","    if sampled_percentage <= 0.01:\n","        repeats = 10\n","    elif sampled_percentage < 0.5:\n","        repeats = 5\n","    else:\n","        repeats = 3\n","    \n","    for repeat in range(repeats):\n","        # Print or log the sampled values for transparency\n","        print(f\"\\nSampled Percentage: {sampled_percentage}, Lr: {params['lr_fine_tune']}, Repeat: {repeat}\")\n","\n","        # Reduce the dataset\n","        train_loader_reduced = reduce_dataset(dataloader_wrapped.train_loader, sampled_percentage, seed = 42)\n","        torch.manual_seed(repeat)\n","        #train_loader_reduced = reduce_dataset(dataloader_wrapped.train_loader, sampled_percentage, seed = repeat)\n","        dataset_namespace_new = SimpleNamespace(train_loader=train_loader_reduced, test_loader=dataloader_wrapped.test_loader, val_loader=dataloader_wrapped.val_loader)\n","\n","        # Copy and then cut the model - we already deepcopy it in the function: pretrained_model\n","        #model_temp = copy.deepcopy(template_model)\n","        model_temp = generate_cnn(input_dim = 28, output_dim = 10, depth = params['depth'], num_channels = params['width'],\n","        hidden_dim_lin = params['hidden_dim_lin'], kernel_size = params['kernel_size'], activation_function = params[\"activation_function\"], use_pooling=params['use_pooling'])\n","        model_temp.to(device)\n","\n","        # Train and evaluate\n","        trainer = Trainer(model_temp, dataset_namespace_new, params['lr_fine_tune'], params)\n","        train_acc, test_acc, effective_epochs, checkpoints = trainer.train(verbose=0)\n","        print(f\"Training Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n","\n","        # Store the results\n","        results.append({\"lr\":params['lr_fine_tune'], \"sampled_percentage\":sampled_percentage, \"sampled_cut_point\":-1, \"repeat\":repeat, \"train_acc\":train_acc, \"test_acc\":test_acc}) # -1 for the cut point means it's baseline"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'results.json'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/mnt/c/Users/user/Desktop/MP/mp-tl-study/50-50_MNIST/MNIST_CNN_3D_more_repeats_freeze_reinit.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/user/Desktop/MP/mp-tl-study/50-50_MNIST/MNIST_CNN_3D_more_repeats_freeze_reinit.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(results, f)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/user/Desktop/MP/mp-tl-study/50-50_MNIST/MNIST_CNN_3D_more_repeats_freeze_reinit.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#load results\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/user/Desktop/MP/mp-tl-study/50-50_MNIST/MNIST_CNN_3D_more_repeats_freeze_reinit.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mresults.json\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/user/Desktop/MP/mp-tl-study/50-50_MNIST/MNIST_CNN_3D_more_repeats_freeze_reinit.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     results \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n","File \u001b[0;32m~/anaconda3/envs/mp-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results.json'"]}],"source":["# save baseline results\n","params_tmp = copy.deepcopy(params)\n","del params_tmp[\"device\"]\n","params_tmp[\"activation_function\"] = str(params_tmp[\"activation_function\"])\n","results = [params_tmp] + results\n","\n","with open(f'results_jsons/baselines_freeze_{params[\"freeze\"]}_pool_{params[\"use_pooling\"]}_lr_{params[\"lr_fine_tune\"]}.json', 'w') as f:\n","    json.dump(results, f)"]},{"cell_type":"markdown","metadata":{},"source":["### Fine-tuning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["results = []"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-11-12T15:29:09.268754Z","iopub.status.busy":"2023-11-12T15:29:09.268384Z","iopub.status.idle":"2023-11-12T17:54:32.877944Z","shell.execute_reply":"2023-11-12T17:54:32.876696Z","shell.execute_reply.started":"2023-11-12T15:29:09.268720Z"},"id":"hMZ4o1FGsipP","outputId":"08ac8a68-17f6-48a3-e06e-27f484b9507d","scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Sampled Percentage: 0.001, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 1.0000, Test Accuracy: 0.7322\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 1\n","Early stopping invoked.\n","Training Accuracy: 1.0000, Test Accuracy: 0.6676\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 1.0000, Test Accuracy: 0.7219\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 1.0000, Test Accuracy: 0.7062\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 1.0000, Test Accuracy: 0.7215\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 5\n","Training Accuracy: 1.0000, Test Accuracy: 0.7319\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 6\n","Training Accuracy: 1.0000, Test Accuracy: 0.7171\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 7\n","Training Accuracy: 1.0000, Test Accuracy: 0.7194\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 8\n","Training Accuracy: 1.0000, Test Accuracy: 0.7136\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 9\n","Training Accuracy: 1.0000, Test Accuracy: 0.7165\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 1.0000, Test Accuracy: 0.7307\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 1\n","Early stopping invoked.\n","Training Accuracy: 1.0000, Test Accuracy: 0.6667\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 1.0000, Test Accuracy: 0.7184\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 1.0000, Test Accuracy: 0.7075\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 1.0000, Test Accuracy: 0.7182\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 5\n","Training Accuracy: 1.0000, Test Accuracy: 0.7311\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 6\n","Training Accuracy: 1.0000, Test Accuracy: 0.7157\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 7\n","Training Accuracy: 1.0000, Test Accuracy: 0.7178\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 8\n","Training Accuracy: 1.0000, Test Accuracy: 0.7114\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 9\n","Training Accuracy: 1.0000, Test Accuracy: 0.7155\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 1.0000, Test Accuracy: 0.7328\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 1\n","Early stopping invoked.\n","Training Accuracy: 1.0000, Test Accuracy: 0.6686\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 1.0000, Test Accuracy: 0.7223\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 1.0000, Test Accuracy: 0.7079\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 1.0000, Test Accuracy: 0.7173\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 5\n","Training Accuracy: 1.0000, Test Accuracy: 0.7359\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 6\n","Training Accuracy: 1.0000, Test Accuracy: 0.7182\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 7\n","Training Accuracy: 1.0000, Test Accuracy: 0.7200\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 8\n","Training Accuracy: 1.0000, Test Accuracy: 0.7147\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 9\n","Training Accuracy: 1.0000, Test Accuracy: 0.7153\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 1.0000, Test Accuracy: 0.7346\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 1\n","Early stopping invoked.\n","Training Accuracy: 1.0000, Test Accuracy: 0.6680\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 1.0000, Test Accuracy: 0.7252\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 1.0000, Test Accuracy: 0.7101\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 1.0000, Test Accuracy: 0.7173\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 5\n","Training Accuracy: 1.0000, Test Accuracy: 0.7359\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 6\n","Training Accuracy: 1.0000, Test Accuracy: 0.7221\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 7\n","Training Accuracy: 1.0000, Test Accuracy: 0.7247\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 8\n","Training Accuracy: 1.0000, Test Accuracy: 0.7182\n","\n","Sampled Percentage: 0.001, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 9\n","Training Accuracy: 1.0000, Test Accuracy: 0.7155\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 1.0000, Test Accuracy: 0.8089\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 1.0000, Test Accuracy: 0.7918\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 1.0000, Test Accuracy: 0.8050\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 1.0000, Test Accuracy: 0.7955\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 1.0000, Test Accuracy: 0.8060\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 5\n","Training Accuracy: 1.0000, Test Accuracy: 0.8089\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 6\n","Training Accuracy: 1.0000, Test Accuracy: 0.8103\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 7\n","Training Accuracy: 1.0000, Test Accuracy: 0.8146\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 8\n","Training Accuracy: 1.0000, Test Accuracy: 0.8054\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 9\n","Training Accuracy: 1.0000, Test Accuracy: 0.8068\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 1.0000, Test Accuracy: 0.8093\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 1.0000, Test Accuracy: 0.7908\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 1.0000, Test Accuracy: 0.8035\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 1.0000, Test Accuracy: 0.7941\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 1.0000, Test Accuracy: 0.8033\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 5\n","Training Accuracy: 1.0000, Test Accuracy: 0.8085\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 6\n","Training Accuracy: 1.0000, Test Accuracy: 0.8091\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 7\n","Training Accuracy: 1.0000, Test Accuracy: 0.8136\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 8\n","Training Accuracy: 1.0000, Test Accuracy: 0.8048\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 9\n","Training Accuracy: 1.0000, Test Accuracy: 0.8058\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 1.0000, Test Accuracy: 0.8054\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 1.0000, Test Accuracy: 0.7910\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 1.0000, Test Accuracy: 0.8019\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 1.0000, Test Accuracy: 0.7941\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 1.0000, Test Accuracy: 0.8019\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 5\n","Training Accuracy: 1.0000, Test Accuracy: 0.8070\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 6\n","Training Accuracy: 1.0000, Test Accuracy: 0.8072\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 7\n","Training Accuracy: 1.0000, Test Accuracy: 0.8097\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 8\n","Training Accuracy: 1.0000, Test Accuracy: 0.8023\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 9\n","Training Accuracy: 1.0000, Test Accuracy: 0.8031\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 1.0000, Test Accuracy: 0.8013\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 1.0000, Test Accuracy: 0.7865\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 1.0000, Test Accuracy: 0.7978\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 1.0000, Test Accuracy: 0.7891\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 1.0000, Test Accuracy: 0.7978\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 5\n","Training Accuracy: 1.0000, Test Accuracy: 0.8011\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 6\n","Training Accuracy: 1.0000, Test Accuracy: 0.8017\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 7\n","Training Accuracy: 1.0000, Test Accuracy: 0.8054\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 8\n","Training Accuracy: 1.0000, Test Accuracy: 0.7988\n","\n","Sampled Percentage: 0.002, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 9\n","Training Accuracy: 1.0000, Test Accuracy: 0.8000\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9692, Test Accuracy: 0.8904\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9615, Test Accuracy: 0.8877\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9615, Test Accuracy: 0.8906\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 0.9615, Test Accuracy: 0.8936\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 0.9692, Test Accuracy: 0.8916\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 5\n","Training Accuracy: 0.9538, Test Accuracy: 0.8860\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 6\n","Training Accuracy: 0.9615, Test Accuracy: 0.8914\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 7\n","Training Accuracy: 0.9692, Test Accuracy: 0.8992\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 8\n","Training Accuracy: 0.9692, Test Accuracy: 0.8957\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 9\n","Training Accuracy: 0.9692, Test Accuracy: 0.8971\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9692, Test Accuracy: 0.8899\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9615, Test Accuracy: 0.8858\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9615, Test Accuracy: 0.8908\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 0.9615, Test Accuracy: 0.8916\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 0.9615, Test Accuracy: 0.8891\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 5\n","Training Accuracy: 0.9538, Test Accuracy: 0.8854\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 6\n","Training Accuracy: 0.9615, Test Accuracy: 0.8904\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 7\n","Training Accuracy: 0.9692, Test Accuracy: 0.8988\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 8\n","Training Accuracy: 0.9692, Test Accuracy: 0.8955\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 9\n","Training Accuracy: 0.9615, Test Accuracy: 0.8963\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9538, Test Accuracy: 0.8897\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9615, Test Accuracy: 0.8832\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9538, Test Accuracy: 0.8904\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 0.9615, Test Accuracy: 0.8918\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 0.9615, Test Accuracy: 0.8887\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 5\n","Training Accuracy: 0.9538, Test Accuracy: 0.8846\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 6\n","Training Accuracy: 0.9538, Test Accuracy: 0.8877\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 7\n","Training Accuracy: 0.9692, Test Accuracy: 0.8994\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 8\n","Training Accuracy: 0.9538, Test Accuracy: 0.8961\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 9\n","Training Accuracy: 0.9615, Test Accuracy: 0.8955\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9538, Test Accuracy: 0.8864\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9538, Test Accuracy: 0.8823\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9462, Test Accuracy: 0.8887\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 0.9538, Test Accuracy: 0.8899\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 0.9615, Test Accuracy: 0.8881\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 5\n","Training Accuracy: 0.9538, Test Accuracy: 0.8854\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 6\n","Training Accuracy: 0.9462, Test Accuracy: 0.8860\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 7\n","Training Accuracy: 0.9615, Test Accuracy: 0.8990\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 8\n","Training Accuracy: 0.9538, Test Accuracy: 0.8936\n","\n","Sampled Percentage: 0.005, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 9\n","Training Accuracy: 0.9538, Test Accuracy: 0.8949\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9615, Test Accuracy: 0.9060\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9615, Test Accuracy: 0.9060\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9615, Test Accuracy: 0.9072\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 0.9500, Test Accuracy: 0.9095\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 0.9654, Test Accuracy: 0.9099\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 5\n","Training Accuracy: 0.9615, Test Accuracy: 0.9045\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 6\n","Training Accuracy: 0.9538, Test Accuracy: 0.9078\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 7\n","Training Accuracy: 0.9692, Test Accuracy: 0.9171\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 8\n","Training Accuracy: 0.9692, Test Accuracy: 0.9103\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 9\n","Training Accuracy: 0.9692, Test Accuracy: 0.9128\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9615, Test Accuracy: 0.9062\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9538, Test Accuracy: 0.9052\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9577, Test Accuracy: 0.9060\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 0.9500, Test Accuracy: 0.9080\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 0.9654, Test Accuracy: 0.9087\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 5\n","Training Accuracy: 0.9615, Test Accuracy: 0.9045\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 6\n","Training Accuracy: 0.9500, Test Accuracy: 0.9074\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 7\n","Training Accuracy: 0.9692, Test Accuracy: 0.9167\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 8\n","Training Accuracy: 0.9692, Test Accuracy: 0.9101\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 9\n","Training Accuracy: 0.9654, Test Accuracy: 0.9126\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9615, Test Accuracy: 0.9043\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9538, Test Accuracy: 0.9039\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9577, Test Accuracy: 0.9060\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 0.9462, Test Accuracy: 0.9054\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 0.9615, Test Accuracy: 0.9089\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 5\n","Training Accuracy: 0.9577, Test Accuracy: 0.9041\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 6\n","Training Accuracy: 0.9462, Test Accuracy: 0.9054\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 7\n","Training Accuracy: 0.9615, Test Accuracy: 0.9140\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 8\n","Training Accuracy: 0.9577, Test Accuracy: 0.9093\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 9\n","Training Accuracy: 0.9615, Test Accuracy: 0.9117\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9500, Test Accuracy: 0.9039\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9500, Test Accuracy: 0.9054\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9462, Test Accuracy: 0.9050\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 0.9423, Test Accuracy: 0.9076\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 0.9577, Test Accuracy: 0.9076\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 5\n","Training Accuracy: 0.9500, Test Accuracy: 0.9035\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 6\n","Training Accuracy: 0.9462, Test Accuracy: 0.9060\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 7\n","Training Accuracy: 0.9615, Test Accuracy: 0.9140\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 8\n","Training Accuracy: 0.9500, Test Accuracy: 0.9103\n","\n","Sampled Percentage: 0.01, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 9\n","Training Accuracy: 0.9577, Test Accuracy: 0.9105\n","\n","Sampled Percentage: 0.05, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9303, Test Accuracy: 0.9150\n","\n","Sampled Percentage: 0.05, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9258, Test Accuracy: 0.9169\n","\n","Sampled Percentage: 0.05, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9273, Test Accuracy: 0.9148\n","\n","Sampled Percentage: 0.05, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 0.9303, Test Accuracy: 0.9185\n","\n","Sampled Percentage: 0.05, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 0.9318, Test Accuracy: 0.9187\n","\n","Sampled Percentage: 0.05, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9273, Test Accuracy: 0.9150\n","\n","Sampled Percentage: 0.05, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9265, Test Accuracy: 0.9161\n","\n","Sampled Percentage: 0.05, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9273, Test Accuracy: 0.9142\n","\n","Sampled Percentage: 0.05, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 0.9273, Test Accuracy: 0.9161\n","\n","Sampled Percentage: 0.05, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 0.9311, Test Accuracy: 0.9165\n","\n","Sampled Percentage: 0.05, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9258, Test Accuracy: 0.9126\n","\n","Sampled Percentage: 0.05, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9258, Test Accuracy: 0.9144\n","\n","Sampled Percentage: 0.05, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9235, Test Accuracy: 0.9113\n","\n","Sampled Percentage: 0.05, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 0.9227, Test Accuracy: 0.9144\n","\n","Sampled Percentage: 0.05, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 0.9288, Test Accuracy: 0.9157\n","\n","Sampled Percentage: 0.05, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9250, Test Accuracy: 0.9115\n","\n","Sampled Percentage: 0.05, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9220, Test Accuracy: 0.9134\n","\n","Sampled Percentage: 0.05, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9197, Test Accuracy: 0.9097\n","\n","Sampled Percentage: 0.05, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 0.9189, Test Accuracy: 0.9124\n","\n","Sampled Percentage: 0.05, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 0.9258, Test Accuracy: 0.9134\n","\n","Sampled Percentage: 0.1, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9263, Test Accuracy: 0.9200\n","\n","Sampled Percentage: 0.1, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9248, Test Accuracy: 0.9237\n","\n","Sampled Percentage: 0.1, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9233, Test Accuracy: 0.9202\n","\n","Sampled Percentage: 0.1, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 0.9244, Test Accuracy: 0.9224\n","\n","Sampled Percentage: 0.1, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 0.9267, Test Accuracy: 0.9224\n","\n","Sampled Percentage: 0.1, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9251, Test Accuracy: 0.9187\n","\n","Sampled Percentage: 0.1, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9240, Test Accuracy: 0.9218\n","\n","Sampled Percentage: 0.1, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9225, Test Accuracy: 0.9185\n","\n","Sampled Percentage: 0.1, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 0.9236, Test Accuracy: 0.9214\n","\n","Sampled Percentage: 0.1, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 0.9251, Test Accuracy: 0.9194\n","\n","Sampled Percentage: 0.1, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9202, Test Accuracy: 0.9179\n","\n","Sampled Percentage: 0.1, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9217, Test Accuracy: 0.9214\n","\n","Sampled Percentage: 0.1, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9202, Test Accuracy: 0.9189\n","\n","Sampled Percentage: 0.1, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 0.9206, Test Accuracy: 0.9208\n","\n","Sampled Percentage: 0.1, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 0.9225, Test Accuracy: 0.9187\n","\n","Sampled Percentage: 0.1, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9187, Test Accuracy: 0.9175\n","\n","Sampled Percentage: 0.1, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9202, Test Accuracy: 0.9206\n","\n","Sampled Percentage: 0.1, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9183, Test Accuracy: 0.9163\n","\n","Sampled Percentage: 0.1, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 0.9202, Test Accuracy: 0.9169\n","\n","Sampled Percentage: 0.1, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 0.9198, Test Accuracy: 0.9179\n","\n","Sampled Percentage: 0.3, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9564, Test Accuracy: 0.9504\n","\n","Sampled Percentage: 0.3, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9568, Test Accuracy: 0.9502\n","\n","Sampled Percentage: 0.3, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9568, Test Accuracy: 0.9490\n","\n","Sampled Percentage: 0.3, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 0.9572, Test Accuracy: 0.9517\n","\n","Sampled Percentage: 0.3, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 0.9573, Test Accuracy: 0.9502\n","\n","Sampled Percentage: 0.3, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9540, Test Accuracy: 0.9477\n","\n","Sampled Percentage: 0.3, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9544, Test Accuracy: 0.9492\n","\n","Sampled Percentage: 0.3, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9558, Test Accuracy: 0.9473\n","\n","Sampled Percentage: 0.3, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 0.9553, Test Accuracy: 0.9506\n","\n","Sampled Percentage: 0.3, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 0.9558, Test Accuracy: 0.9494\n","\n","Sampled Percentage: 0.3, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9507, Test Accuracy: 0.9436\n","\n","Sampled Percentage: 0.3, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9522, Test Accuracy: 0.9461\n","\n","Sampled Percentage: 0.3, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9501, Test Accuracy: 0.9436\n","\n","Sampled Percentage: 0.3, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 0.9534, Test Accuracy: 0.9463\n","\n","Sampled Percentage: 0.3, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 0.9510, Test Accuracy: 0.9447\n","\n","Sampled Percentage: 0.3, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9473, Test Accuracy: 0.9401\n","\n","Sampled Percentage: 0.3, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9477, Test Accuracy: 0.9422\n","\n","Sampled Percentage: 0.3, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9452, Test Accuracy: 0.9389\n","\n","Sampled Percentage: 0.3, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 3\n","Training Accuracy: 0.9482, Test Accuracy: 0.9418\n","\n","Sampled Percentage: 0.3, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 4\n","Training Accuracy: 0.9464, Test Accuracy: 0.9410\n","\n","Sampled Percentage: 0.5, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9710, Test Accuracy: 0.9724\n","\n","Sampled Percentage: 0.5, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9713, Test Accuracy: 0.9737\n","\n","Sampled Percentage: 0.5, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9707, Test Accuracy: 0.9726\n","\n","Sampled Percentage: 0.5, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9692, Test Accuracy: 0.9702\n","\n","Sampled Percentage: 0.5, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9704, Test Accuracy: 0.9706\n","\n","Sampled Percentage: 0.5, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9689, Test Accuracy: 0.9698\n","\n","Sampled Percentage: 0.5, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9655, Test Accuracy: 0.9663\n","\n","Sampled Percentage: 0.5, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9669, Test Accuracy: 0.9663\n","\n","Sampled Percentage: 0.5, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9646, Test Accuracy: 0.9661\n","\n","Sampled Percentage: 0.5, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9619, Test Accuracy: 0.9595\n","\n","Sampled Percentage: 0.5, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9630, Test Accuracy: 0.9611\n","\n","Sampled Percentage: 0.5, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9622, Test Accuracy: 0.9613\n","\n","Sampled Percentage: 0.8, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9766, Test Accuracy: 0.9782\n","\n","Sampled Percentage: 0.8, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9778, Test Accuracy: 0.9782\n","\n","Sampled Percentage: 0.8, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9770, Test Accuracy: 0.9792\n","\n","Sampled Percentage: 0.8, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9739, Test Accuracy: 0.9743\n","\n","Sampled Percentage: 0.8, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 1\n","Early stopping invoked.\n","Training Accuracy: 0.9706, Test Accuracy: 0.9673\n","\n","Sampled Percentage: 0.8, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 2\n","Early stopping invoked.\n","Training Accuracy: 0.9729, Test Accuracy: 0.9739\n","\n","Sampled Percentage: 0.8, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 0\n","Early stopping invoked.\n","Training Accuracy: 0.9686, Test Accuracy: 0.9698\n","\n","Sampled Percentage: 0.8, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 1\n","Early stopping invoked.\n","Training Accuracy: 0.9724, Test Accuracy: 0.9696\n","\n","Sampled Percentage: 0.8, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 2\n","Early stopping invoked.\n","Training Accuracy: 0.9703, Test Accuracy: 0.9700\n","\n","Sampled Percentage: 0.8, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9691, Test Accuracy: 0.9689\n","\n","Sampled Percentage: 0.8, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9722, Test Accuracy: 0.9700\n","\n","Sampled Percentage: 0.8, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9698, Test Accuracy: 0.9700\n","\n","Sampled Percentage: 1, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9798, Test Accuracy: 0.9794\n","\n","Sampled Percentage: 1, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9805, Test Accuracy: 0.9790\n","\n","Sampled Percentage: 1, Sampled Cut Point: 0, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9809, Test Accuracy: 0.9813\n","\n","Sampled Percentage: 1, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9788, Test Accuracy: 0.9774\n","\n","Sampled Percentage: 1, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9792, Test Accuracy: 0.9778\n","\n","Sampled Percentage: 1, Sampled Cut Point: 1, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9795, Test Accuracy: 0.9788\n","\n","Sampled Percentage: 1, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 0\n","Training Accuracy: 0.9765, Test Accuracy: 0.9765\n","\n","Sampled Percentage: 1, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9768, Test Accuracy: 0.9782\n","\n","Sampled Percentage: 1, Sampled Cut Point: 2, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9766, Test Accuracy: 0.9772\n","\n","Sampled Percentage: 1, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 0\n","Early stopping invoked.\n","Training Accuracy: 0.9703, Test Accuracy: 0.9689\n","\n","Sampled Percentage: 1, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 1\n","Training Accuracy: 0.9742, Test Accuracy: 0.9731\n","\n","Sampled Percentage: 1, Sampled Cut Point: 3, Lr: 0.0001, Repeat: 2\n","Training Accuracy: 0.9731, Test Accuracy: 0.9747\n"]}],"source":["dataloader_wrapped.update_phase('finetune')\n","\n","# Store results\n","#results = []\n","# Store unique combinations that have been tested: we need this if we want to test random combinations\n","# tested_combinations = set()\n","\n","# for lr in learning_rates:\n","#     params[\"lr\"] = lr\n","# repeating the whole thing with multiple lr and saving the results somewhere\n","for sampled_percentage in percentages:\n","\n","    if sampled_percentage <= 0.01:\n","        repeats = 10\n","    elif sampled_percentage < 0.5:\n","        repeats = 5\n","    else:\n","        repeats = 3\n","        \n","    for sampled_cut_point in cuts:\n","\n","        for repeat in range(repeats):\n","            # Add the combination to the tested set\n","            # tested_combinations.add((sampled_percentage, sampled_cut_point))\n","\n","            # Print or log the sampled values for transparency\n","            print(f\"\\nSampled Percentage: {sampled_percentage}, Sampled Cut Point: {sampled_cut_point}, Lr: {params['lr_fine_tune']}, Repeat: {repeat}\")\n","\n","            # Reduce the dataset\n","            train_loader_reduced = reduce_dataset(dataloader_wrapped.train_loader, sampled_percentage, seed=42)\n","            dataset_namespace_new = SimpleNamespace(train_loader=train_loader_reduced, test_loader=dataloader_wrapped.test_loader, val_loader=dataloader_wrapped.val_loader)\n","            torch.manual_seed(repeat) # because in the cut function we reinitialize some layers too (at least the dense layers)\n","            \n","            # Copy and then cut the model - we already deepcopy it in the function: pretrained_model\n","            model_new = cut_custom_cnn_model(pretrained_model, cut_point=sampled_cut_point, params=params)\n","\n","            # Train and evaluate\n","            trainer = Trainer(model_new, dataset_namespace_new, params['lr_fine_tune'], params)\n","            train_acc, test_acc, effective_epochs, checkpoints = trainer.train(verbose=0)\n","            print(f\"Training Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n","\n","            # Store the results\n","            results.append({\"lr\":params['lr_fine_tune'], \"sampled_percentage\":sampled_percentage, \"sampled_cut_point\":sampled_cut_point, \"repeat\":repeat, \"train_acc\":train_acc, \"test_acc\":test_acc})"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# save fine-tuning results\n","params_tmp = copy.deepcopy(params)\n","del params_tmp[\"device\"]\n","params_tmp[\"activation_function\"] = str(params_tmp[\"activation_function\"])\n","results = [params_tmp] + results\n","\n","with open(f'results_jsons/results_freeze_{params[\"freeze\"]}_reinit_{params[\"reinit\"]}_pool_{params[\"use_pooling\"]}_lr_{params[\"lr_fine_tune\"]}.json', 'w') as f:\n","    json.dump(results, f)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}

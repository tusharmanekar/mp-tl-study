{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example for:\n",
    "Transfer Learning Empirical Experiment from Vehicles (pretraining) to Animals (finetuning) classes of CIFAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which gpu\n",
    "import os\n",
    "gpu_id = 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/arnisaf/mp-tl-study')\n",
    "from functions.utils import *\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)  # if using multi-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuts=0 means: end-to-end model if we are reinitializing\n",
    "cuts = [0,1,2,3,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Changes Here for the experiments\n",
    "params = {\n",
    "      # MODEL ARCHITECTURE PARAMS\n",
    "      'depth': 6,\n",
    "      'num_channels': 64,\n",
    "      'width':64,\n",
    "      # 'hidden_dim_lin': 128,\n",
    "      'activation_function': nn.ReLU,\n",
    "      'kernel_size': 3,\n",
    "      # TRAINING PARAMS\n",
    "      'device': device,\n",
    "      'lr_pretrain': 0.001,   \n",
    "      'lr_fine_tune': 0.001,  # CHANGE: if no layer-wise lr\n",
    "      # 'lr_fine_tune_reinit': 0.001,         # CHANGE: if no layer-wise lr\n",
    "      # 'lr_fine_tune_no_reinit': 0.0001,     # CHANGE: if layer-wise lr\n",
    "      'num_train': 10,\n",
    "      'early_stop_patience': 5,\n",
    "      'save_best': False,\n",
    "      'save_checkpoints': False,\n",
    "      'is_cnn': True,\n",
    "      'is_debug': False,\n",
    "      'classification_report_flag': False,\n",
    "      'batch_size':64,\n",
    "      # DATASET PARAMS\n",
    "      'pre_train_classes': [2, 3, 4, 5, 6, 7],\n",
    "      'fine_tune_classes': [0, 1, 8, 9],\n",
    "      'val_split': 0.1,\n",
    "      'num_workers': 0,\n",
    "      'generate_dataset_seed': 42,\n",
    "      # EXPERIMENT SETTING PARAMS\n",
    "      'use_pooling': True,   # CHANGE\n",
    "      'pooling_every_n_layers': 2, # add pooling after every n layers specified here. For only one pooling after all the CNN layers, this equals params['depth']\n",
    "      'pooling_stride': 2,\n",
    "      'freeze': True,         # CHANGE: freeze the conv layers before the cut\n",
    "      'reinit': True,         # CHANGE: reinit the conv lyers only after the cut\n",
    "      'reinit_both_dense': True,   # CHANGE: True for reinitialize both dense layers, False for reinit only the last dense layer\n",
    "      'truncate': False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "root_dir = './data'  # Specify your data directory here\n",
    "transform=transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ])\n",
    "dataloader_wrapped = TransferLearningWrapper(params, datasets.CIFAR10, datasets.CIFAR10, root_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_wrapped.pretrain_train_loader.dataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomCNN(\n",
       "  (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act0): ReLU()\n",
       "  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act1): ReLU()\n",
       "  (pool1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act2): ReLU()\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act3): ReLU()\n",
       "  (pool3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act4): ReLU()\n",
       "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act5): ReLU()\n",
       "  (pool5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (fc): Linear(in_features=1024, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model = CustomCNN(params, dataloader_wrapped.output_dim, tuple(dataloader_wrapped.finetune_test_loader.dataset[0][0].shape))\n",
    "pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Accuracy: 46.32%\n",
      "Validation Accuracy: 44.77%\n",
      "Epoch: 1 \tTraining Accuracy: 55.19%\n",
      "Validation Accuracy: 53.07%\n",
      "Epoch: 2 \tTraining Accuracy: 62.42%\n",
      "Validation Accuracy: 59.77%\n",
      "Epoch: 3 \tTraining Accuracy: 67.20%\n",
      "Validation Accuracy: 63.07%\n",
      "Epoch: 4 \tTraining Accuracy: 70.06%\n",
      "Validation Accuracy: 65.37%\n",
      "Epoch: 5 \tTraining Accuracy: 73.70%\n",
      "Validation Accuracy: 68.43%\n",
      "Epoch: 6 \tTraining Accuracy: 75.85%\n",
      "Validation Accuracy: 69.23%\n",
      "Epoch: 7 \tTraining Accuracy: 77.35%\n",
      "Validation Accuracy: 70.67%\n",
      "Epoch: 8 \tTraining Accuracy: 78.44%\n",
      "Validation Accuracy: 70.43%\n",
      "Epoch: 9 \tTraining Accuracy: 81.73%\n",
      "Validation Accuracy: 72.67%\n",
      "Final Training Accuracy: 0.8173\n",
      "Final Test Accuracy: 0.8083\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate - Skip if loading saved model!\n",
    "trainer = Trainer(pretrained_model, dataloader_wrapped, params[\"lr_pretrain\"], params)\n",
    "train_acc, test_acc, effective_epochs, checkpoints = trainer.train()\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Final Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pretrained_model.state_dict(), 'pretrained_models/pretrained_model_animals/pretrained_model_animals_new.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomCNN(\n",
       "  (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act0): ReLU()\n",
       "  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act1): ReLU()\n",
       "  (pool1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act2): ReLU()\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act3): ReLU()\n",
       "  (pool3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act4): ReLU()\n",
       "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act5): ReLU()\n",
       "  (pool5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (fc): Linear(in_features=1024, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.load_state_dict(torch.load('pretrained_models/pretrained_model_animals/pretrained_model_animals_new.pth'))\n",
    "pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss: 0.5366, Accuracy: 24248.0/30000 (81%)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.84      0.80      0.82      5000\n",
      "     Class 1       0.65      0.80      0.72      5000\n",
      "     Class 2       0.85      0.80      0.82      5000\n",
      "     Class 3       0.82      0.70      0.76      5000\n",
      "     Class 4       0.82      0.93      0.87      5000\n",
      "     Class 5       0.94      0.83      0.88      5000\n",
      "\n",
      "    accuracy                           0.81     30000\n",
      "   macro avg       0.82      0.81      0.81     30000\n",
      "weighted avg       0.82      0.81      0.81     30000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8082666666666667"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(pretrained_model, device, dataloader_wrapped.test_loader, debug=True, classification_report_flag=True, is_cnn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines (End-to-end models trained on subsets of fine-tuning dataset)\n",
    "We also reuse the baselines a lot! so skip if we already have the jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines_results = []\n",
    "percentages = [0.001, 0.01, 0.1, 0.5, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampled Percentage: 0.001, Lr: 0.001, Repeat: 0\n",
      "Early stopping invoked.\n",
      "Training Accuracy: 0.3750, Test Accuracy: 0.2961\n",
      "\n",
      "Sampled Percentage: 0.001, Lr: 0.001, Repeat: 1\n"
     ]
    }
   ],
   "source": [
    "dataloader_wrapped.update_phase('finetune')\n",
    "\n",
    "for sampled_percentage in percentages:      \n",
    "    if sampled_percentage <= 0.01:\n",
    "        repeats = 25\n",
    "    elif sampled_percentage < 0.5:\n",
    "        repeats = 20\n",
    "    else:\n",
    "        repeats = 5\n",
    "    \n",
    "    for repeat in range(repeats):\n",
    "        # Print or log the sampled values for transparency\n",
    "        print(f\"\\nSampled Percentage: {sampled_percentage}, Lr: {params['lr_fine_tune']}, Repeat: {repeat}\")\n",
    "\n",
    "        # Reduce the dataset\n",
    "        train_loader_reduced = reduce_dataset(dataloader_wrapped.train_loader, sampled_percentage, seed = repeat)\n",
    "        torch.manual_seed(repeat)\n",
    "        #train_loader_reduced = reduce_dataset(dataloader_wrapped.train_loader, sampled_percentage, seed = repeat)\n",
    "        dataset_namespace_new = SimpleNamespace(train_loader=train_loader_reduced, test_loader=dataloader_wrapped.test_loader, val_loader=dataloader_wrapped.val_loader)\n",
    "\n",
    "        # Copy and then cut the model - we already deepcopy it in the function: pretrained_model\n",
    "        params_tmp = copy.deepcopy(params)\n",
    "        params_tmp[\"reinit\"] = True\n",
    "        model_new = cut_custom_cnn_model(pretrained_model, cut_point=0, params=params_tmp, output_dim=dataloader_wrapped.output_dim)\n",
    "        model_new.to(device)\n",
    "\n",
    "        # Train and evaluate\n",
    "        trainer = Trainer(model_new, dataset_namespace_new, params['lr_fine_tune'], params)\n",
    "        train_acc, test_acc, effective_epochs, checkpoints = trainer.train(verbose=0)\n",
    "        print(f\"Training Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "        # Store the results\n",
    "        baselines_results.append({\"lr\":params['lr_fine_tune'], \"sampled_percentage\":sampled_percentage, \"sampled_cut_point\":-1, \"repeat\":repeat, \"train_acc\":train_acc, \"test_acc\":test_acc}) # -1 for the cut point means it's baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(baselines_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save baseline results\n",
    "params_tmp = copy.deepcopy(params)\n",
    "del params_tmp[\"device\"]\n",
    "params_tmp[\"activation_function\"] = str(params_tmp[\"activation_function\"])\n",
    "results = [params_tmp] + baselines_results\n",
    "\n",
    "with open(f'results_jsons/baselines_freeze_{params[\"freeze\"]}_pool_{params[\"use_pooling\"]}_truncate_{params[\"truncate\"]}.json', 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "#percentages = [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "percentages = [0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 0, Lr: 0.001, Repeat: 0\n",
      "Training Accuracy: 0.9226, Test Accuracy: 0.8837\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 0, Lr: 0.001, Repeat: 1\n",
      "Training Accuracy: 0.9216, Test Accuracy: 0.8831\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 0, Lr: 0.001, Repeat: 2\n",
      "Training Accuracy: 0.9129, Test Accuracy: 0.8708\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 0, Lr: 0.001, Repeat: 3\n",
      "Training Accuracy: 0.9030, Test Accuracy: 0.8696\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 0, Lr: 0.001, Repeat: 4\n",
      "Training Accuracy: 0.9120, Test Accuracy: 0.8741\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 1, Lr: 0.001, Repeat: 0\n",
      "Training Accuracy: 0.9139, Test Accuracy: 0.8770\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 1, Lr: 0.001, Repeat: 1\n",
      "Training Accuracy: 0.9231, Test Accuracy: 0.8859\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 1, Lr: 0.001, Repeat: 2\n",
      "Training Accuracy: 0.9343, Test Accuracy: 0.8942\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 1, Lr: 0.001, Repeat: 3\n",
      "Training Accuracy: 0.9323, Test Accuracy: 0.8952\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 1, Lr: 0.001, Repeat: 4\n",
      "Training Accuracy: 0.9126, Test Accuracy: 0.8825\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 2, Lr: 0.001, Repeat: 0\n",
      "Training Accuracy: 0.9507, Test Accuracy: 0.9062\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 2, Lr: 0.001, Repeat: 1\n",
      "Training Accuracy: 0.9139, Test Accuracy: 0.8809\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 2, Lr: 0.001, Repeat: 2\n",
      "Training Accuracy: 0.9510, Test Accuracy: 0.9059\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 2, Lr: 0.001, Repeat: 3\n",
      "Training Accuracy: 0.9440, Test Accuracy: 0.9056\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 2, Lr: 0.001, Repeat: 4\n",
      "Training Accuracy: 0.9367, Test Accuracy: 0.8956\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 3, Lr: 0.001, Repeat: 0\n",
      "Training Accuracy: 0.9369, Test Accuracy: 0.8910\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 3, Lr: 0.001, Repeat: 1\n",
      "Training Accuracy: 0.9400, Test Accuracy: 0.8963\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 3, Lr: 0.001, Repeat: 2\n",
      "Training Accuracy: 0.9266, Test Accuracy: 0.8819\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 3, Lr: 0.001, Repeat: 3\n",
      "Training Accuracy: 0.9271, Test Accuracy: 0.8900\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 3, Lr: 0.001, Repeat: 4\n",
      "Training Accuracy: 0.9436, Test Accuracy: 0.8975\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 4, Lr: 0.001, Repeat: 0\n",
      "Training Accuracy: 0.9354, Test Accuracy: 0.8832\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 4, Lr: 0.001, Repeat: 1\n",
      "Training Accuracy: 0.9394, Test Accuracy: 0.8868\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 4, Lr: 0.001, Repeat: 2\n",
      "Training Accuracy: 0.9421, Test Accuracy: 0.8844\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 4, Lr: 0.001, Repeat: 3\n",
      "Training Accuracy: 0.9380, Test Accuracy: 0.8880\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 4, Lr: 0.001, Repeat: 4\n",
      "Training Accuracy: 0.9343, Test Accuracy: 0.8818\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 5, Lr: 0.001, Repeat: 0\n",
      "Training Accuracy: 0.9228, Test Accuracy: 0.8605\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 5, Lr: 0.001, Repeat: 1\n",
      "Training Accuracy: 0.9251, Test Accuracy: 0.8645\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 5, Lr: 0.001, Repeat: 2\n",
      "Training Accuracy: 0.9079, Test Accuracy: 0.8503\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 5, Lr: 0.001, Repeat: 3\n",
      "Training Accuracy: 0.9252, Test Accuracy: 0.8680\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 5, Lr: 0.001, Repeat: 4\n",
      "Training Accuracy: 0.9161, Test Accuracy: 0.8587\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 6, Lr: 0.001, Repeat: 0\n",
      "Training Accuracy: 0.8196, Test Accuracy: 0.7849\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 6, Lr: 0.001, Repeat: 1\n",
      "Training Accuracy: 0.8148, Test Accuracy: 0.7855\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 6, Lr: 0.001, Repeat: 2\n",
      "Training Accuracy: 0.8152, Test Accuracy: 0.7823\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 6, Lr: 0.001, Repeat: 3\n",
      "Training Accuracy: 0.8072, Test Accuracy: 0.7839\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 6, Lr: 0.001, Repeat: 4\n",
      "Training Accuracy: 0.8144, Test Accuracy: 0.7847\n"
     ]
    }
   ],
   "source": [
    "dataloader_wrapped.update_phase('finetune')\n",
    "\n",
    "for sampled_percentage in percentages:\n",
    "\n",
    "    if sampled_percentage <= 0.01:\n",
    "        repeats = 25\n",
    "    elif sampled_percentage < 0.5:\n",
    "        repeats = 20\n",
    "    else:\n",
    "        repeats = 5\n",
    "        \n",
    "    for sampled_cut_point in cuts:\n",
    "\n",
    "        for repeat in range(repeats):\n",
    "            # Add the combination to the tested set\n",
    "            # tested_combinations.add((sampled_percentage, sampled_cut_point))\n",
    "\n",
    "            # Print or log the sampled values for transparency\n",
    "            print(f\"\\nSampled Percentage: {sampled_percentage}, Sampled Cut Point: {sampled_cut_point}, Lr: {params['lr_fine_tune']}, Repeat: {repeat}\")\n",
    "\n",
    "            # Reduce the dataset\n",
    "            train_loader_reduced = reduce_dataset(dataloader_wrapped.train_loader, sampled_percentage, seed=repeat)\n",
    "            dataset_namespace_new = SimpleNamespace(train_loader=train_loader_reduced, test_loader=dataloader_wrapped.test_loader, val_loader=dataloader_wrapped.val_loader)\n",
    "            torch.manual_seed(repeat) # because in the cut function we reinitialize some layers too (at least the dense layers)\n",
    "            \n",
    "            # Copy and then cut the model - we already deepcopy it in the function: pretrained_model\n",
    "            model_new = cut_custom_cnn_model(pretrained_model, cut_point=sampled_cut_point, params=params, output_dim=dataloader_wrapped.output_dim)\n",
    "            model_new.to(device)\n",
    "            \n",
    "            # Train and evaluate\n",
    "            trainer = Trainer(model_new, dataset_namespace_new, params['lr_fine_tune'], params)\n",
    "            train_acc, test_acc, effective_epochs, checkpoints = trainer.train(verbose=0)\n",
    "            print(f\"Training Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "            # Store the results\n",
    "            results.append({\"lr\":params['lr_fine_tune'], \"sampled_percentage\":sampled_percentage, \"sampled_cut_point\":sampled_cut_point, \"repeat\":repeat, \"train_acc\":train_acc, \"test_acc\":test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 0, 'repeat': 0, 'train_acc': 0.9225555555555556, 'test_acc': 0.88365}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 0, 'repeat': 1, 'train_acc': 0.9215555555555556, 'test_acc': 0.8831}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 0, 'repeat': 2, 'train_acc': 0.9128888888888889, 'test_acc': 0.8708}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 0, 'repeat': 3, 'train_acc': 0.903, 'test_acc': 0.8696}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 0, 'repeat': 4, 'train_acc': 0.912, 'test_acc': 0.87415}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 1, 'repeat': 0, 'train_acc': 0.9138888888888889, 'test_acc': 0.87695}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 1, 'repeat': 1, 'train_acc': 0.9231111111111111, 'test_acc': 0.8859}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 1, 'repeat': 2, 'train_acc': 0.9343333333333333, 'test_acc': 0.89425}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 1, 'repeat': 3, 'train_acc': 0.9323333333333333, 'test_acc': 0.8952}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 1, 'repeat': 4, 'train_acc': 0.9125555555555556, 'test_acc': 0.88255}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 2, 'repeat': 0, 'train_acc': 0.9506666666666667, 'test_acc': 0.90625}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 2, 'repeat': 1, 'train_acc': 0.9138888888888889, 'test_acc': 0.8809}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 2, 'repeat': 2, 'train_acc': 0.951, 'test_acc': 0.9059}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 2, 'repeat': 3, 'train_acc': 0.944, 'test_acc': 0.90565}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 2, 'repeat': 4, 'train_acc': 0.9366666666666666, 'test_acc': 0.8956}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 3, 'repeat': 0, 'train_acc': 0.9368888888888889, 'test_acc': 0.89095}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 3, 'repeat': 1, 'train_acc': 0.94, 'test_acc': 0.8963}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 3, 'repeat': 2, 'train_acc': 0.9265555555555556, 'test_acc': 0.8819}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 3, 'repeat': 3, 'train_acc': 0.9271111111111111, 'test_acc': 0.89}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 3, 'repeat': 4, 'train_acc': 0.9435555555555556, 'test_acc': 0.89755}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 4, 'repeat': 0, 'train_acc': 0.9354444444444444, 'test_acc': 0.8832}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 4, 'repeat': 1, 'train_acc': 0.9394444444444444, 'test_acc': 0.88675}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 4, 'repeat': 2, 'train_acc': 0.9421111111111111, 'test_acc': 0.88445}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 4, 'repeat': 3, 'train_acc': 0.938, 'test_acc': 0.88795}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 4, 'repeat': 4, 'train_acc': 0.9343333333333333, 'test_acc': 0.88175}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 5, 'repeat': 0, 'train_acc': 0.9227777777777778, 'test_acc': 0.86045}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 5, 'repeat': 1, 'train_acc': 0.9251111111111111, 'test_acc': 0.8645}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 5, 'repeat': 2, 'train_acc': 0.9078888888888889, 'test_acc': 0.8503}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 5, 'repeat': 3, 'train_acc': 0.9252222222222222, 'test_acc': 0.86805}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 5, 'repeat': 4, 'train_acc': 0.9161111111111111, 'test_acc': 0.8587}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 6, 'repeat': 0, 'train_acc': 0.8195555555555556, 'test_acc': 0.7849}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 6, 'repeat': 1, 'train_acc': 0.8147777777777778, 'test_acc': 0.78555}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 6, 'repeat': 2, 'train_acc': 0.8152222222222222, 'test_acc': 0.78235}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 6, 'repeat': 3, 'train_acc': 0.8072222222222222, 'test_acc': 0.78385}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 6, 'repeat': 4, 'train_acc': 0.8144444444444444, 'test_acc': 0.78475}]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 0, 'repeat': 0, 'train_acc': 0.9225555555555556, 'test_acc': 0.88365}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 0, 'repeat': 1, 'train_acc': 0.9215555555555556, 'test_acc': 0.8831}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 0, 'repeat': 2, 'train_acc': 0.9128888888888889, 'test_acc': 0.8708}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 0, 'repeat': 3, 'train_acc': 0.903, 'test_acc': 0.8696}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 0, 'repeat': 4, 'train_acc': 0.912, 'test_acc': 0.87415}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 1, 'repeat': 0, 'train_acc': 0.9138888888888889, 'test_acc': 0.87695}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 1, 'repeat': 1, 'train_acc': 0.9231111111111111, 'test_acc': 0.8859}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 1, 'repeat': 2, 'train_acc': 0.9343333333333333, 'test_acc': 0.89425}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 1, 'repeat': 3, 'train_acc': 0.9323333333333333, 'test_acc': 0.8952}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 1, 'repeat': 4, 'train_acc': 0.9125555555555556, 'test_acc': 0.88255}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 2, 'repeat': 0, 'train_acc': 0.9506666666666667, 'test_acc': 0.90625}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 2, 'repeat': 1, 'train_acc': 0.9138888888888889, 'test_acc': 0.8809}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 2, 'repeat': 2, 'train_acc': 0.951, 'test_acc': 0.9059}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 2, 'repeat': 3, 'train_acc': 0.944, 'test_acc': 0.90565}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 2, 'repeat': 4, 'train_acc': 0.9366666666666666, 'test_acc': 0.8956}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 3, 'repeat': 0, 'train_acc': 0.9368888888888889, 'test_acc': 0.89095}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 3, 'repeat': 1, 'train_acc': 0.94, 'test_acc': 0.8963}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 3, 'repeat': 2, 'train_acc': 0.9265555555555556, 'test_acc': 0.8819}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 3, 'repeat': 3, 'train_acc': 0.9271111111111111, 'test_acc': 0.89}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 3, 'repeat': 4, 'train_acc': 0.9435555555555556, 'test_acc': 0.89755}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 4, 'repeat': 0, 'train_acc': 0.9354444444444444, 'test_acc': 0.8832}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 4, 'repeat': 1, 'train_acc': 0.9394444444444444, 'test_acc': 0.88675}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 4, 'repeat': 2, 'train_acc': 0.9421111111111111, 'test_acc': 0.88445}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 4, 'repeat': 3, 'train_acc': 0.938, 'test_acc': 0.88795}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 4, 'repeat': 4, 'train_acc': 0.9343333333333333, 'test_acc': 0.88175}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 5, 'repeat': 0, 'train_acc': 0.9227777777777778, 'test_acc': 0.86045}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 5, 'repeat': 1, 'train_acc': 0.9251111111111111, 'test_acc': 0.8645}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 5, 'repeat': 2, 'train_acc': 0.9078888888888889, 'test_acc': 0.8503}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 5, 'repeat': 3, 'train_acc': 0.9252222222222222, 'test_acc': 0.86805}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 5, 'repeat': 4, 'train_acc': 0.9161111111111111, 'test_acc': 0.8587}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 6, 'repeat': 0, 'train_acc': 0.8195555555555556, 'test_acc': 0.7849}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 6, 'repeat': 1, 'train_acc': 0.8147777777777778, 'test_acc': 0.78555}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 6, 'repeat': 2, 'train_acc': 0.8152222222222222, 'test_acc': 0.78235}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 6, 'repeat': 3, 'train_acc': 0.8072222222222222, 'test_acc': 0.78385}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 6, 'repeat': 4, 'train_acc': 0.8144444444444444, 'test_acc': 0.78475}]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save fine-tuning results\n",
    "params_tmp = copy.deepcopy(params)\n",
    "del params_tmp[\"device\"]\n",
    "params_tmp[\"activation_function\"] = str(params_tmp[\"activation_function\"])\n",
    "results = [params_tmp] + results\n",
    "\n",
    "with open(f'results_jsons/results_freeze_{params[\"freeze\"]}_reinit_{params[\"reinit\"]}_pool_{params[\"use_pooling\"]}_lr_{params[\"lr_fine_tune\"]}_nice_curve.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "results = results[1:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example for:\n",
    "Transfer Learning Empirical Experiment from Vehicles (pretraining) to Animals (finetuning) classes of CIFAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which gpu\n",
    "import os\n",
    "gpu_id = 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/arnisaf/mp-tl-study')\n",
    "from functions.utils import *\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)  # if using multi-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuts=0 means: end-to-end model if we are reinitializing\n",
    "cuts = [0,1,2,3,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Changes Here for the experiments\n",
    "params = {\n",
    "      # MODEL ARCHITECTURE PARAMS\n",
    "      'depth': 6,\n",
    "      'num_channels': 64,\n",
    "      'width':64,\n",
    "      # 'hidden_dim_lin': 128,\n",
    "      'activation_function': nn.ReLU,\n",
    "      'kernel_size': 3,\n",
    "      # TRAINING PARAMS\n",
    "      'device': device,\n",
    "      'lr_pretrain': 0.001,   \n",
    "      'lr_fine_tune': 0.001,  # CHANGE: if no layer-wise lr\n",
    "      # 'lr_fine_tune_reinit': 0.001,         # CHANGE: if no layer-wise lr\n",
    "      # 'lr_fine_tune_no_reinit': 0.0001,     # CHANGE: if layer-wise lr\n",
    "      'num_train': 10,\n",
    "      'early_stop_patience': 5,\n",
    "      'save_best': False,\n",
    "      'save_checkpoints': False,\n",
    "      'is_cnn': True,\n",
    "      'is_debug': False,\n",
    "      'classification_report_flag': False,\n",
    "      'batch_size':64,\n",
    "      # DATASET PARAMS\n",
    "      'pre_train_classes': [0, 1, 8, 9],\n",
    "      'fine_tune_classes': [2, 3, 4, 5, 6, 7],\n",
    "        'val_split': 0.1,\n",
    "      'num_workers': 0,\n",
    "      'generate_dataset_seed': 42,\n",
    "      # EXPERIMENT SETTING PARAMS\n",
    "      'use_pooling': True,   # CHANGE\n",
    "      'pooling_every_n_layers': 2, # add pooling after every n layers specified here. For only one pooling after all the CNN layers, this equals params['depth']\n",
    "      'pooling_stride': 2,\n",
    "      'freeze': True,         # CHANGE: freeze the conv layers before the cut\n",
    "      'reinit': True,         # CHANGE: reinit the conv lyers only after the cut\n",
    "      'reinit_both_dense': True,   # CHANGE: True for reinitialize both dense layers, False for reinit only the last dense layer\n",
    "      'truncate': True\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "root_dir = './data'  # Specify your data directory here\n",
    "transform=transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ])\n",
    "dataloader_wrapped = TransferLearningWrapper(params, datasets.CIFAR10, datasets.CIFAR10, root_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_wrapped.pretrain_train_loader.dataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomCNN(\n",
       "  (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act0): ReLU()\n",
       "  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act1): ReLU()\n",
       "  (pool1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act2): ReLU()\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act3): ReLU()\n",
       "  (pool3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act4): ReLU()\n",
       "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act5): ReLU()\n",
       "  (pool5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (fc): Linear(in_features=1024, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model = CustomCNN(params, dataloader_wrapped.output_dim, tuple(dataloader_wrapped.finetune_test_loader.dataset[0][0].shape))\n",
    "pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Accuracy: 44.91%\n",
      "Validation Accuracy: 43.40%\n",
      "Epoch: 1 \tTraining Accuracy: 53.14%\n",
      "Validation Accuracy: 50.33%\n",
      "Epoch: 2 \tTraining Accuracy: 63.49%\n",
      "Validation Accuracy: 60.30%\n",
      "Epoch: 3 \tTraining Accuracy: 68.63%\n",
      "Validation Accuracy: 65.00%\n",
      "Epoch: 4 \tTraining Accuracy: 70.95%\n",
      "Validation Accuracy: 67.07%\n",
      "Epoch: 5 \tTraining Accuracy: 76.44%\n",
      "Validation Accuracy: 70.37%\n",
      "Epoch: 6 \tTraining Accuracy: 78.74%\n",
      "Validation Accuracy: 71.53%\n",
      "Epoch: 7 \tTraining Accuracy: 81.48%\n",
      "Validation Accuracy: 72.73%\n",
      "Epoch: 8 \tTraining Accuracy: 83.88%\n",
      "Validation Accuracy: 73.43%\n",
      "Epoch: 9 \tTraining Accuracy: 87.08%\n",
      "Validation Accuracy: 74.83%\n",
      "Final Training Accuracy: 0.8708\n",
      "Final Test Accuracy: 0.8586\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate - Skip if loading saved model!\n",
    "trainer = Trainer(pretrained_model, dataloader_wrapped, params[\"lr_pretrain\"], params)\n",
    "train_acc, test_acc, effective_epochs, checkpoints = trainer.train()\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Final Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomCNN(\n",
       "  (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act0): ReLU()\n",
       "  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act1): ReLU()\n",
       "  (pool1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act2): ReLU()\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act3): ReLU()\n",
       "  (pool3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act4): ReLU()\n",
       "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act5): ReLU()\n",
       "  (pool5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (fc): Linear(in_features=1024, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.load_state_dict(torch.load('pretrained_models/pretrained_model_Vehicles_constant_channels/modelcifar10_vehicles_cpu.pth'))\n",
    "pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss: 0.1832, Accuracy: 18689.0/20000 (93%)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.91      0.95      0.93      5000\n",
      "     Class 1       0.97      0.92      0.95      5000\n",
      "     Class 2       0.94      0.93      0.93      5000\n",
      "     Class 3       0.92      0.94      0.93      5000\n",
      "\n",
      "    accuracy                           0.93     20000\n",
      "   macro avg       0.94      0.93      0.93     20000\n",
      "weighted avg       0.94      0.93      0.93     20000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.93445"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(pretrained_model, device, dataloader_wrapped.test_loader, debug=True, classification_report_flag=True, is_cnn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines (End-to-end models trained on subsets of fine-tuning dataset)\n",
    "We also reuse the baselines a lot! so skip if we already have the jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines_results = []\n",
    "percentages = [0.001, 0.01, 0.1, 0.5, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_wrapped.update_phase('finetune')\n",
    "\n",
    "for sampled_percentage in percentages:      \n",
    "    if sampled_percentage <= 0.01:\n",
    "        repeats = 25\n",
    "    elif sampled_percentage < 0.5:\n",
    "        repeats = 20\n",
    "    else:\n",
    "        repeats = 5\n",
    "    \n",
    "    for repeat in range(repeats):\n",
    "        # Print or log the sampled values for transparency\n",
    "        print(f\"\\nSampled Percentage: {sampled_percentage}, Lr: {params['lr_fine_tune']}, Repeat: {repeat}\")\n",
    "\n",
    "        # Reduce the dataset\n",
    "        train_loader_reduced = reduce_dataset(dataloader_wrapped.train_loader, sampled_percentage, seed = repeat)\n",
    "        torch.manual_seed(repeat)\n",
    "        #train_loader_reduced = reduce_dataset(dataloader_wrapped.train_loader, sampled_percentage, seed = repeat)\n",
    "        dataset_namespace_new = SimpleNamespace(train_loader=train_loader_reduced, test_loader=dataloader_wrapped.test_loader, val_loader=dataloader_wrapped.val_loader)\n",
    "\n",
    "        # Copy and then cut the model - we already deepcopy it in the function: pretrained_model\n",
    "        params_tmp = copy.deepcopy(params)\n",
    "        params_tmp[\"reinit\"] = True\n",
    "        model_new = cut_custom_cnn_model(pretrained_model, cut_point=0, params=params_tmp, output_dim=dataloader_wrapped.output_dim)\n",
    "        model_new.to(device)\n",
    "\n",
    "        # Train and evaluate\n",
    "        trainer = Trainer(model_new, dataset_namespace_new, params['lr_fine_tune'], params)\n",
    "        train_acc, test_acc, effective_epochs, checkpoints = trainer.train(verbose=0)\n",
    "        print(f\"Training Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "        # Store the results\n",
    "        baselines_results.append({\"lr\":params['lr_fine_tune'], \"sampled_percentage\":sampled_percentage, \"sampled_cut_point\":-1, \"repeat\":repeat, \"train_acc\":train_acc, \"test_acc\":test_acc}) # -1 for the cut point means it's baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save baseline results\n",
    "params_tmp = copy.deepcopy(params)\n",
    "del params_tmp[\"device\"]\n",
    "params_tmp[\"activation_function\"] = str(params_tmp[\"activation_function\"])\n",
    "results = [params_tmp] + baselines_results\n",
    "\n",
    "with open(f'results_jsons/baselines_freeze_{params[\"freeze\"]}_pool_{params[\"use_pooling\"]}_truncate_{params[\"truncate\"]}.json', 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "#percentages = [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "percentages = [0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 0, Lr: 0.001, Repeat: 0\n",
      "Early stopping invoked.\n",
      "Training Accuracy: 0.4305, Test Accuracy: 0.4095\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 0, Lr: 0.001, Repeat: 1\n",
      "Early stopping invoked.\n",
      "Training Accuracy: 0.4340, Test Accuracy: 0.4055\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 0, Lr: 0.001, Repeat: 2\n",
      "Training Accuracy: 0.4499, Test Accuracy: 0.4063\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 0, Lr: 0.001, Repeat: 3\n",
      "Training Accuracy: 0.4734, Test Accuracy: 0.4243\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 0, Lr: 0.001, Repeat: 4\n",
      "Training Accuracy: 0.4645, Test Accuracy: 0.4185\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 1, Lr: 0.001, Repeat: 0\n",
      "Training Accuracy: 0.8513, Test Accuracy: 0.6755\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 1, Lr: 0.001, Repeat: 1\n",
      "Training Accuracy: 0.8259, Test Accuracy: 0.6518\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 1, Lr: 0.001, Repeat: 2\n",
      "Training Accuracy: 0.8669, Test Accuracy: 0.6871\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 1, Lr: 0.001, Repeat: 3\n",
      "Early stopping invoked.\n",
      "Training Accuracy: 0.7931, Test Accuracy: 0.6525\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 1, Lr: 0.001, Repeat: 4\n",
      "Training Accuracy: 0.8751, Test Accuracy: 0.6865\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 2, Lr: 0.001, Repeat: 0\n",
      "Training Accuracy: 0.8419, Test Accuracy: 0.7091\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 2, Lr: 0.001, Repeat: 1\n",
      "Early stopping invoked.\n",
      "Training Accuracy: 0.7789, Test Accuracy: 0.6811\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 2, Lr: 0.001, Repeat: 2\n",
      "Training Accuracy: 0.8188, Test Accuracy: 0.6964\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 2, Lr: 0.001, Repeat: 3\n",
      "Early stopping invoked.\n",
      "Training Accuracy: 0.7331, Test Accuracy: 0.6603\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 2, Lr: 0.001, Repeat: 4\n",
      "Early stopping invoked.\n",
      "Training Accuracy: 0.7688, Test Accuracy: 0.6741\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 3, Lr: 0.001, Repeat: 0\n",
      "Training Accuracy: 0.8401, Test Accuracy: 0.6994\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 3, Lr: 0.001, Repeat: 1\n",
      "Training Accuracy: 0.8638, Test Accuracy: 0.7150\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 3, Lr: 0.001, Repeat: 2\n",
      "Training Accuracy: 0.8528, Test Accuracy: 0.7078\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 3, Lr: 0.001, Repeat: 3\n",
      "Training Accuracy: 0.8351, Test Accuracy: 0.6960\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 3, Lr: 0.001, Repeat: 4\n",
      "Training Accuracy: 0.8568, Test Accuracy: 0.7086\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 4, Lr: 0.001, Repeat: 0\n",
      "Training Accuracy: 0.7264, Test Accuracy: 0.6590\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 4, Lr: 0.001, Repeat: 1\n",
      "Training Accuracy: 0.7323, Test Accuracy: 0.6628\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 4, Lr: 0.001, Repeat: 2\n",
      "Training Accuracy: 0.7083, Test Accuracy: 0.6475\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 4, Lr: 0.001, Repeat: 3\n",
      "Training Accuracy: 0.7153, Test Accuracy: 0.6508\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 4, Lr: 0.001, Repeat: 4\n",
      "Training Accuracy: 0.7228, Test Accuracy: 0.6503\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 5, Lr: 0.001, Repeat: 0\n",
      "Training Accuracy: 0.7528, Test Accuracy: 0.6573\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 5, Lr: 0.001, Repeat: 1\n",
      "Early stopping invoked.\n",
      "Training Accuracy: 0.7117, Test Accuracy: 0.6410\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 5, Lr: 0.001, Repeat: 2\n",
      "Training Accuracy: 0.7193, Test Accuracy: 0.6356\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 5, Lr: 0.001, Repeat: 3\n",
      "Early stopping invoked.\n",
      "Training Accuracy: 0.6953, Test Accuracy: 0.6324\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 5, Lr: 0.001, Repeat: 4\n",
      "Training Accuracy: 0.7283, Test Accuracy: 0.6378\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 6, Lr: 0.001, Repeat: 0\n",
      "Training Accuracy: 0.6305, Test Accuracy: 0.5952\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 6, Lr: 0.001, Repeat: 1\n",
      "Training Accuracy: 0.6299, Test Accuracy: 0.5949\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 6, Lr: 0.001, Repeat: 2\n",
      "Training Accuracy: 0.6241, Test Accuracy: 0.5945\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 6, Lr: 0.001, Repeat: 3\n",
      "Training Accuracy: 0.6389, Test Accuracy: 0.5994\n",
      "\n",
      "Sampled Percentage: 0.5, Sampled Cut Point: 6, Lr: 0.001, Repeat: 4\n",
      "Training Accuracy: 0.6347, Test Accuracy: 0.5985\n"
     ]
    }
   ],
   "source": [
    "dataloader_wrapped.update_phase('finetune')\n",
    "\n",
    "for sampled_percentage in percentages:\n",
    "\n",
    "    if sampled_percentage <= 0.01:\n",
    "        repeats = 25\n",
    "    elif sampled_percentage < 0.5:\n",
    "        repeats = 20\n",
    "    else:\n",
    "        repeats = 5\n",
    "        \n",
    "    for sampled_cut_point in cuts:\n",
    "\n",
    "        for repeat in range(repeats):\n",
    "            # Add the combination to the tested set\n",
    "            # tested_combinations.add((sampled_percentage, sampled_cut_point))\n",
    "\n",
    "            # Print or log the sampled values for transparency\n",
    "            print(f\"\\nSampled Percentage: {sampled_percentage}, Sampled Cut Point: {sampled_cut_point}, Lr: {params['lr_fine_tune']}, Repeat: {repeat}\")\n",
    "\n",
    "            # Reduce the dataset\n",
    "            train_loader_reduced = reduce_dataset(dataloader_wrapped.train_loader, sampled_percentage, seed=repeat)\n",
    "            dataset_namespace_new = SimpleNamespace(train_loader=train_loader_reduced, test_loader=dataloader_wrapped.test_loader, val_loader=dataloader_wrapped.val_loader)\n",
    "            torch.manual_seed(repeat) # because in the cut function we reinitialize some layers too (at least the dense layers)\n",
    "            \n",
    "            # Copy and then cut the model - we already deepcopy it in the function: pretrained_model\n",
    "            model_new = cut_custom_cnn_model(pretrained_model, cut_point=sampled_cut_point, params=params, output_dim=dataloader_wrapped.output_dim)\n",
    "            model_new.to(device)\n",
    "            \n",
    "            # Train and evaluate\n",
    "            trainer = Trainer(model_new, dataset_namespace_new, params['lr_fine_tune'], params)\n",
    "            train_acc, test_acc, effective_epochs, checkpoints = trainer.train(verbose=0)\n",
    "            print(f\"Training Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "            # Store the results\n",
    "            results.append({\"lr\":params['lr_fine_tune'], \"sampled_percentage\":sampled_percentage, \"sampled_cut_point\":sampled_cut_point, \"repeat\":repeat, \"train_acc\":train_acc, \"test_acc\":test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 0, 'repeat': 0, 'train_acc': 0.4305185185185185, 'test_acc': 0.40946666666666665}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 0, 'repeat': 1, 'train_acc': 0.434, 'test_acc': 0.4055}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 0, 'repeat': 2, 'train_acc': 0.44985185185185184, 'test_acc': 0.4063}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 0, 'repeat': 3, 'train_acc': 0.4734074074074074, 'test_acc': 0.42433333333333334}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 0, 'repeat': 4, 'train_acc': 0.4645185185185185, 'test_acc': 0.4185}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 1, 'repeat': 0, 'train_acc': 0.8513333333333334, 'test_acc': 0.6754666666666667}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 1, 'repeat': 1, 'train_acc': 0.825925925925926, 'test_acc': 0.6517666666666667}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 1, 'repeat': 2, 'train_acc': 0.8668888888888889, 'test_acc': 0.6871333333333334}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 1, 'repeat': 3, 'train_acc': 0.7931111111111111, 'test_acc': 0.6524666666666666}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 1, 'repeat': 4, 'train_acc': 0.8751111111111111, 'test_acc': 0.6864666666666667}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 2, 'repeat': 0, 'train_acc': 0.8418518518518519, 'test_acc': 0.7090666666666666}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 2, 'repeat': 1, 'train_acc': 0.7788888888888889, 'test_acc': 0.6810666666666667}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 2, 'repeat': 2, 'train_acc': 0.8188148148148148, 'test_acc': 0.6964333333333333}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 2, 'repeat': 3, 'train_acc': 0.7331111111111112, 'test_acc': 0.6603333333333333}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 2, 'repeat': 4, 'train_acc': 0.7688148148148148, 'test_acc': 0.6741333333333334}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 3, 'repeat': 0, 'train_acc': 0.8401481481481482, 'test_acc': 0.6994}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 3, 'repeat': 1, 'train_acc': 0.8637777777777778, 'test_acc': 0.7149666666666666}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 3, 'repeat': 2, 'train_acc': 0.8528148148148148, 'test_acc': 0.7078333333333333}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 3, 'repeat': 3, 'train_acc': 0.8351111111111111, 'test_acc': 0.696}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 3, 'repeat': 4, 'train_acc': 0.8568148148148148, 'test_acc': 0.7086}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 4, 'repeat': 0, 'train_acc': 0.7263703703703703, 'test_acc': 0.6590333333333334}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 4, 'repeat': 1, 'train_acc': 0.7322962962962963, 'test_acc': 0.6628}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 4, 'repeat': 2, 'train_acc': 0.7082962962962963, 'test_acc': 0.6474666666666666}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 4, 'repeat': 3, 'train_acc': 0.7153333333333334, 'test_acc': 0.6508333333333334}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 4, 'repeat': 4, 'train_acc': 0.7228148148148148, 'test_acc': 0.6502666666666667}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 5, 'repeat': 0, 'train_acc': 0.7528148148148148, 'test_acc': 0.6573333333333333}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 5, 'repeat': 1, 'train_acc': 0.7117037037037037, 'test_acc': 0.6409666666666667}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 5, 'repeat': 2, 'train_acc': 0.7192592592592593, 'test_acc': 0.6356}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 5, 'repeat': 3, 'train_acc': 0.6953333333333334, 'test_acc': 0.6324}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 5, 'repeat': 4, 'train_acc': 0.7282962962962963, 'test_acc': 0.6378}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 6, 'repeat': 0, 'train_acc': 0.6305185185185185, 'test_acc': 0.5952333333333333}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 6, 'repeat': 1, 'train_acc': 0.6299259259259259, 'test_acc': 0.5949333333333333}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 6, 'repeat': 2, 'train_acc': 0.6240740740740741, 'test_acc': 0.5945}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 6, 'repeat': 3, 'train_acc': 0.6388888888888888, 'test_acc': 0.5994333333333334}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 6, 'repeat': 4, 'train_acc': 0.6347407407407407, 'test_acc': 0.5984666666666667}]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 0, 'repeat': 0, 'train_acc': 0.4305185185185185, 'test_acc': 0.40946666666666665}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 0, 'repeat': 1, 'train_acc': 0.434, 'test_acc': 0.4055}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 0, 'repeat': 2, 'train_acc': 0.44985185185185184, 'test_acc': 0.4063}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 0, 'repeat': 3, 'train_acc': 0.4734074074074074, 'test_acc': 0.42433333333333334}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 0, 'repeat': 4, 'train_acc': 0.4645185185185185, 'test_acc': 0.4185}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 1, 'repeat': 0, 'train_acc': 0.8513333333333334, 'test_acc': 0.6754666666666667}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 1, 'repeat': 1, 'train_acc': 0.825925925925926, 'test_acc': 0.6517666666666667}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 1, 'repeat': 2, 'train_acc': 0.8668888888888889, 'test_acc': 0.6871333333333334}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 1, 'repeat': 3, 'train_acc': 0.7931111111111111, 'test_acc': 0.6524666666666666}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 1, 'repeat': 4, 'train_acc': 0.8751111111111111, 'test_acc': 0.6864666666666667}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 2, 'repeat': 0, 'train_acc': 0.8418518518518519, 'test_acc': 0.7090666666666666}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 2, 'repeat': 1, 'train_acc': 0.7788888888888889, 'test_acc': 0.6810666666666667}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 2, 'repeat': 2, 'train_acc': 0.8188148148148148, 'test_acc': 0.6964333333333333}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 2, 'repeat': 3, 'train_acc': 0.7331111111111112, 'test_acc': 0.6603333333333333}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 2, 'repeat': 4, 'train_acc': 0.7688148148148148, 'test_acc': 0.6741333333333334}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 3, 'repeat': 0, 'train_acc': 0.8401481481481482, 'test_acc': 0.6994}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 3, 'repeat': 1, 'train_acc': 0.8637777777777778, 'test_acc': 0.7149666666666666}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 3, 'repeat': 2, 'train_acc': 0.8528148148148148, 'test_acc': 0.7078333333333333}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 3, 'repeat': 3, 'train_acc': 0.8351111111111111, 'test_acc': 0.696}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 3, 'repeat': 4, 'train_acc': 0.8568148148148148, 'test_acc': 0.7086}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 4, 'repeat': 0, 'train_acc': 0.7263703703703703, 'test_acc': 0.6590333333333334}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 4, 'repeat': 1, 'train_acc': 0.7322962962962963, 'test_acc': 0.6628}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 4, 'repeat': 2, 'train_acc': 0.7082962962962963, 'test_acc': 0.6474666666666666}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 4, 'repeat': 3, 'train_acc': 0.7153333333333334, 'test_acc': 0.6508333333333334}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 4, 'repeat': 4, 'train_acc': 0.7228148148148148, 'test_acc': 0.6502666666666667}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 5, 'repeat': 0, 'train_acc': 0.7528148148148148, 'test_acc': 0.6573333333333333}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 5, 'repeat': 1, 'train_acc': 0.7117037037037037, 'test_acc': 0.6409666666666667}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 5, 'repeat': 2, 'train_acc': 0.7192592592592593, 'test_acc': 0.6356}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 5, 'repeat': 3, 'train_acc': 0.6953333333333334, 'test_acc': 0.6324}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 5, 'repeat': 4, 'train_acc': 0.7282962962962963, 'test_acc': 0.6378}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 6, 'repeat': 0, 'train_acc': 0.6305185185185185, 'test_acc': 0.5952333333333333}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 6, 'repeat': 1, 'train_acc': 0.6299259259259259, 'test_acc': 0.5949333333333333}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 6, 'repeat': 2, 'train_acc': 0.6240740740740741, 'test_acc': 0.5945}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 6, 'repeat': 3, 'train_acc': 0.6388888888888888, 'test_acc': 0.5994333333333334}, {'lr': 0.001, 'sampled_percentage': 0.5, 'sampled_cut_point': 6, 'repeat': 4, 'train_acc': 0.6347407407407407, 'test_acc': 0.5984666666666667}]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save fine-tuning results\n",
    "params_tmp = copy.deepcopy(params)\n",
    "del params_tmp[\"device\"]\n",
    "params_tmp[\"activation_function\"] = str(params_tmp[\"activation_function\"])\n",
    "results = [params_tmp] + results\n",
    "\n",
    "with open(f'results_jsons/results_freeze_{params[\"freeze\"]}_reinit_{params[\"reinit\"]}_pool_{params[\"use_pooling\"]}_lr_{params[\"lr_fine_tune\"]}_nice_curve.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "results = results[1:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example for:\n",
    "Transfer Learning Empirical Experiment from first 5 classes of MNIST into Shoes ([5, 7, 9]) in FashionMNIST\n",
    "- Almost all the details about optimizers, training, lr, ... are in the params dict!\n",
    "- And all experiment design details too! reinit, freeze, .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gpu_id is which GPU to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which gpu\n",
    "import os\n",
    "gpu_id = 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cuts depends on model architecture:\n",
    "- If the model has 5 conv layers, we have 6 cuts as below: cut=0 means do not freeze any conv layers, cut=5 means freeze all conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuts=0 means: end-to-end model if we are reinitializing\n",
    "cuts = [0,1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model architecture\n",
    "* we don't have support for multiple dense layers yet\n",
    "* all the conv layers have to have the same number of channels\n",
    "* even when save_best=False, the trainer function still returns the best epoch -just does not save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes Here for the experiments\n",
    "params = {\n",
    "      # MODEL ARCHITECTURE PARAMS\n",
    "      'depth': 5,         # number of conv layers\n",
    "      'num_channels': 30, # num channels for CNN\n",
    "      # 'hidden_dim_lin': 128,   # for 2 dense layers at the end\n",
    "      'activation_function': nn.ReLU,\n",
    "      'kernel_size': 3,\n",
    "      # TRAINING PARAMS\n",
    "      'device': device,\n",
    "      'lr_pretrain': 0.001,   \n",
    "      'lr_fine_tune': 0.001,  # CHANGE: if no layer-wise lr\n",
    "      # 'lr_fine_tune_reinit': 0.001,         # CHANGE: if no layer-wise lr\n",
    "      # 'lr_fine_tune_no_reinit': 0.0001,     # CHANGE: if layer-wise lr\n",
    "      'num_train': 40,\n",
    "      'early_stop_patience': 6,\n",
    "      'save_best': False,\n",
    "      'save_checkpoints': False,\n",
    "      'is_cnn': True,\n",
    "      'is_debug': False,\n",
    "      'classification_report_flag': False,\n",
    "      'batch_size':4096,\n",
    "      # DATASET PARAMS\n",
    "      'pre_train_classes': list(range(5)), # CHANGE: First 5 classes of MNIST\n",
    "      'fine_tune_classes': [5, 7, 9],       # CHANGE: Shoes in FashionMNIST\n",
    "      'val_split': 0.1,\n",
    "      'num_workers': 0,\n",
    "      'generate_dataset_seed': 42,\n",
    "      # EXPERIMENT SETTING PARAMS\n",
    "      'use_pooling': False,   # CHANGE\n",
    "      'freeze': True,         # CHANGE: freeze the conv layers before the cut\n",
    "      'reinit': True,         # CHANGE: reinit the conv lyers only after the cut\n",
    "      'truncate': False,   # CHANGE: True for reinitialize both dense layers, False for reinit only the last dense layer\n",
    "      'truncate': False\n",
    "      }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset (from MNIST to FashionMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './data'  # Specify your data directory here\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "dataloader_wrapped = TransferLearningWrapper(params, datasets.MNIST, datasets.FashionMNIST, root_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model: see how the final classifier layer has 5 nodes, because the pretraining dataset has 5 classes.\n",
    "- That means the number of nodes in the classifier layer will change according to the fine-tune dataset classes when we cut the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomCNN(\n",
       "  (conv0): Conv2d(1, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act0): ReLU()\n",
       "  (conv1): Conv2d(30, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act1): ReLU()\n",
       "  (conv2): Conv2d(30, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act2): ReLU()\n",
       "  (conv3): Conv2d(30, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act3): ReLU()\n",
       "  (conv4): Conv2d(30, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (act4): ReLU()\n",
       "  (fc): Linear(in_features=23520, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create DNN model\n",
    "pretrained_model = CustomCNN(params, dataloader_wrapped.output_dim)\n",
    "pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either pretrain a new model, or load a suitable one! (we usually load one for the serious experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tTraining Accuracy: 86.66%\n",
      "Validation Accuracy: 86.37%\n",
      "Epoch: 1 \tTraining Accuracy: 93.60%\n",
      "Validation Accuracy: 93.72%\n",
      "Epoch: 2 \tTraining Accuracy: 95.62%\n",
      "Validation Accuracy: 95.00%\n",
      "Epoch: 3 \tTraining Accuracy: 96.19%\n",
      "Validation Accuracy: 95.49%\n",
      "Epoch: 4 \tTraining Accuracy: 96.82%\n",
      "Validation Accuracy: 96.11%\n",
      "Epoch: 5 \tTraining Accuracy: 97.01%\n",
      "Validation Accuracy: 96.24%\n",
      "Epoch: 6 \tTraining Accuracy: 97.66%\n",
      "Validation Accuracy: 97.16%\n",
      "Epoch: 7 \tTraining Accuracy: 98.08%\n",
      "Validation Accuracy: 97.58%\n",
      "Epoch: 8 \tTraining Accuracy: 98.51%\n",
      "Validation Accuracy: 98.01%\n",
      "Epoch: 9 \tTraining Accuracy: 98.86%\n",
      "Validation Accuracy: 98.56%\n",
      "Epoch: 10 \tTraining Accuracy: 98.93%\n",
      "Validation Accuracy: 98.50%\n",
      "Epoch: 11 \tTraining Accuracy: 99.26%\n",
      "Validation Accuracy: 98.89%\n",
      "Epoch: 12 \tTraining Accuracy: 99.30%\n",
      "Validation Accuracy: 98.86%\n",
      "Epoch: 13 \tTraining Accuracy: 99.42%\n",
      "Validation Accuracy: 98.89%\n",
      "Epoch: 14 \tTraining Accuracy: 99.46%\n",
      "Validation Accuracy: 98.95%\n",
      "Epoch: 15 \tTraining Accuracy: 99.48%\n",
      "Validation Accuracy: 99.08%\n",
      "Epoch: 16 \tTraining Accuracy: 99.54%\n",
      "Validation Accuracy: 99.08%\n",
      "Epoch: 17 \tTraining Accuracy: 99.56%\n",
      "Validation Accuracy: 99.08%\n",
      "Epoch: 18 \tTraining Accuracy: 99.60%\n",
      "Validation Accuracy: 99.08%\n",
      "Epoch: 19 \tTraining Accuracy: 99.55%\n",
      "Validation Accuracy: 98.82%\n",
      "Epoch: 20 \tTraining Accuracy: 99.66%\n",
      "Validation Accuracy: 99.05%\n",
      "Epoch: 21 \tTraining Accuracy: 99.64%\n",
      "Validation Accuracy: 99.18%\n",
      "Epoch: 22 \tTraining Accuracy: 99.72%\n",
      "Validation Accuracy: 99.08%\n",
      "Epoch: 23 \tTraining Accuracy: 99.59%\n",
      "Validation Accuracy: 98.89%\n",
      "Epoch: 24 \tTraining Accuracy: 99.74%\n",
      "Validation Accuracy: 99.25%\n",
      "Epoch: 25 \tTraining Accuracy: 99.76%\n",
      "Validation Accuracy: 99.15%\n",
      "Epoch: 26 \tTraining Accuracy: 99.76%\n",
      "Validation Accuracy: 99.22%\n",
      "Epoch: 27 \tTraining Accuracy: 99.70%\n",
      "Validation Accuracy: 98.92%\n",
      "Epoch: 28 \tTraining Accuracy: 99.83%\n",
      "Validation Accuracy: 99.25%\n",
      "Epoch: 29 \tTraining Accuracy: 99.71%\n",
      "Validation Accuracy: 99.08%\n",
      "Epoch: 30 \tTraining Accuracy: 99.76%\n",
      "Validation Accuracy: 99.12%\n",
      "Early stopping invoked.\n",
      "Final Training Accuracy: 0.9974\n",
      "Final Test Accuracy: 0.9969\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate\n",
    "trainer = Trainer(pretrained_model, dataloader_wrapped, params[\"lr_pretrain\"], params)\n",
    "train_acc, test_acc, effective_epochs, checkpoints = trainer.train()\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Final Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving and loading a model: Adjust the folder path!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.load_state_dict(torch.load('pretrained_models/pretrained_0.001/pretrained_model_89percent.pth'))\n",
    "pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval to make sure loading worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss: 0.0247, Accuracy: 3036.0/3059 (99%)\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      1.00      1.00       544\n",
      "     Class 1       0.99      0.99      0.99       670\n",
      "     Class 2       0.99      0.99      0.99       631\n",
      "     Class 3       0.99      0.99      0.99       613\n",
      "     Class 4       0.99      1.00      1.00       601\n",
      "\n",
      "    accuracy                           0.99      3059\n",
      "   macro avg       0.99      0.99      0.99      3059\n",
      "weighted avg       0.99      0.99      0.99      3059\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9924812030075187"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(pretrained_model, device, dataloader_wrapped.val_loader, debug=True, classification_report_flag=True, is_cnn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model for later use\n",
    "foldername = \"FashionMNIST/pretrained_models/pretrained_0.001\"\n",
    "os.mkdir(foldername)\n",
    "torch.save(pretrained_model.state_dict(), os.path.join(foldername, 'pretrained_model.pth'))\n",
    "\n",
    "params_tmp = copy.deepcopy(params)\n",
    "del params_tmp[\"device\"]\n",
    "params_tmp[\"activation_function\"] = str(params_tmp[\"activation_function\"])\n",
    "#save params as well\n",
    "with open(os.path.join(foldername, 'params.json'), 'w') as fp:\n",
    "    json.dump(params_tmp, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines (End-to-end models trained on subsets of fine-tuning dataset)\n",
    "We also reuse the baselines a lot! so skip if we already have the jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines_results = []\n",
    "# percentages = [0.001, 0.002, 0.005, 0.01, 0.05, 0.1, 0.3, 0.5, 0.8, 1]\n",
    "percentages = [0.001, 0.002, 0.005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_wrapped.update_phase('finetune')\n",
    "\n",
    "for sampled_percentage in percentages:      \n",
    "    if sampled_percentage <= 0.01:\n",
    "        repeats = 25\n",
    "    elif sampled_percentage < 0.5:\n",
    "        repeats = 20\n",
    "    else:\n",
    "        repeats = 5\n",
    "    \n",
    "    for repeat in range(repeats):\n",
    "        # Print or log the sampled values for transparency\n",
    "        print(f\"\\nSampled Percentage: {sampled_percentage}, Lr: {params['lr_fine_tune']}, Repeat: {repeat}\")\n",
    "\n",
    "        # Reduce the dataset\n",
    "        train_loader_reduced = reduce_dataset(dataloader_wrapped.train_loader, sampled_percentage, seed = repeat)\n",
    "        torch.manual_seed(repeat)\n",
    "        #train_loader_reduced = reduce_dataset(dataloader_wrapped.train_loader, sampled_percentage, seed = repeat)\n",
    "        dataset_namespace_new = SimpleNamespace(train_loader=train_loader_reduced, test_loader=dataloader_wrapped.test_loader, val_loader=dataloader_wrapped.val_loader)\n",
    "\n",
    "        # Copy and then cut the model - we already deepcopy it in the function: pretrained_model\n",
    "        params_tmp = copy.deepcopy(params)\n",
    "        params_tmp[\"reinit\"] = True\n",
    "        model_new = cut_custom_cnn_model(pretrained_model, cut_point=0, params=params_tmp, output_dim=dataloader_wrapped.output_dim)\n",
    "        model_new.to(device)\n",
    "\n",
    "        # Train and evaluate\n",
    "        trainer = Trainer(model_new, dataset_namespace_new, params['lr_fine_tune'], params)\n",
    "        train_acc, test_acc, effective_epochs, checkpoints = trainer.train(verbose=0)\n",
    "        print(f\"Training Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "        # Store the results\n",
    "        baselines_results.append({\"lr\":params['lr_fine_tune'], \"sampled_percentage\":sampled_percentage, \"sampled_cut_point\":-1, \"repeat\":repeat, \"train_acc\":train_acc, \"test_acc\":test_acc}) # -1 for the cut point means it's baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results in json: adjust the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save baseline results\n",
    "params_tmp = copy.deepcopy(params)\n",
    "del params_tmp[\"device\"]\n",
    "params_tmp[\"activation_function\"] = str(params_tmp[\"activation_function\"])\n",
    "baselines_results = [params_tmp] + baselines_results\n",
    "\n",
    "with open(f'results_jsons/baselines_freeze_{params[\"freeze\"]}_pool_{params[\"use_pooling\"]}_lr_{params[\"lr_fine_tune\"]}_dummy_run.json', 'w') as f:\n",
    "    json.dump(baselines_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the results: can load them in visualizations.ipynb or statistical_tests.ipynb for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load results\n",
    "with open('results_jsons/baselines_freeze_True_pool_False_lr_0.001_dummy_run.json', 'r') as f:\n",
    "    results_baseline = json.load(f)\n",
    "results_baseline = results_baseline[1:]\n",
    "\n",
    "# make sure these are only the baselines\n",
    "results_baseline = [r for r in results_baseline if r[\"sampled_cut_point\"] == -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "# percentages = [0.001, 0.002, 0.005, 0.01, 0.05, 0.1, 0.3, 0.5, 0.8, 1]\n",
    "percentages = [0.001, 0.002, 0.005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_wrapped.update_phase('finetune')\n",
    "\n",
    "for sampled_percentage in percentages:\n",
    "\n",
    "    if sampled_percentage <= 0.01:\n",
    "        repeats = 25\n",
    "    elif sampled_percentage < 0.5:\n",
    "        repeats = 20\n",
    "    else:\n",
    "        repeats = 5\n",
    "        \n",
    "    for sampled_cut_point in cuts:\n",
    "\n",
    "        for repeat in range(repeats):\n",
    "            # Add the combination to the tested set\n",
    "            # tested_combinations.add((sampled_percentage, sampled_cut_point))\n",
    "\n",
    "            # Print or log the sampled values for transparency\n",
    "            print(f\"\\nSampled Percentage: {sampled_percentage}, Sampled Cut Point: {sampled_cut_point}, Lr: {params['lr_fine_tune']}, Repeat: {repeat}\")\n",
    "\n",
    "            # Reduce the dataset\n",
    "            train_loader_reduced = reduce_dataset(dataloader_wrapped.train_loader, sampled_percentage, seed=repeat)\n",
    "            dataset_namespace_new = SimpleNamespace(train_loader=train_loader_reduced, test_loader=dataloader_wrapped.test_loader, val_loader=dataloader_wrapped.val_loader)\n",
    "            torch.manual_seed(repeat) # because in the cut function we reinitialize some layers too (at least the dense layers)\n",
    "            \n",
    "            # Copy and then cut the model - we already deepcopy it in the function: pretrained_model\n",
    "            model_new = cut_custom_cnn_model(pretrained_model, cut_point=sampled_cut_point, params=params, output_dim=dataloader_wrapped.output_dim)\n",
    "            model_new.to(device)\n",
    "            \n",
    "            # Train and evaluate\n",
    "            trainer = Trainer(model_new, dataset_namespace_new, params['lr_fine_tune'], params)\n",
    "            train_acc, test_acc, effective_epochs, checkpoints = trainer.train(verbose=0)\n",
    "            print(f\"Training Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "            # Store the results\n",
    "            results.append({\"lr\":params['lr_fine_tune'], \"sampled_percentage\":sampled_percentage, \"sampled_cut_point\":sampled_cut_point, \"repeat\":repeat, \"train_acc\":train_acc, \"test_acc\":test_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save fine-tuning results\n",
    "params_tmp = copy.deepcopy(params)\n",
    "del params_tmp[\"device\"]\n",
    "params_tmp[\"activation_function\"] = str(params_tmp[\"activation_function\"])\n",
    "results = [params_tmp] + results\n",
    "\n",
    "with open(f'results_jsons/results_freeze_{params[\"freeze\"]}_reinit_{params[\"reinit\"]}_pool_{params[\"use_pooling\"]}_truncate_{params[\"truncate\"]}_lr_{params[\"lr_fine_tune\"]}_{percentages[0]}_to_{percentages[-1]}.json', 'w') as f:\n",
    "    json.dump(results, f)\n",
    "results = results[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load results\n",
    "with open('results_jsons/results_freeze_True_pool_False_lr_0.001_dummy_run.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "results = results[1:]\n",
    "\n",
    "# make sure these are only the baselines\n",
    "results = [r for r in results if r[\"sampled_cut_point\"] == -1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

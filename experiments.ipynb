{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=50, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (3): Tanh()\n",
      "  (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (5): Tanh()\n",
      "  (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "  (7): Tanh()\n",
      "  (8): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (9): LogSoftmax(dim=1)\n",
      ") SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "Train set: Average loss: 0.9528, Accuracy: 42467.0/60000 (71%)\n",
      "\n",
      "Epoch: 0 \tTraining Accuracy: 70.78%\n",
      "\n",
      "Train set: Average loss: 0.6307, Accuracy: 48435.0/60000 (81%)\n",
      "\n",
      "Epoch: 1 \tTraining Accuracy: 80.73%\n",
      "\n",
      "Train set: Average loss: 0.5091, Accuracy: 50608.0/60000 (84%)\n",
      "\n",
      "Epoch: 2 \tTraining Accuracy: 84.35%\n",
      "\n",
      "Train set: Average loss: 0.4417, Accuracy: 51911.0/60000 (87%)\n",
      "\n",
      "Epoch: 3 \tTraining Accuracy: 86.52%\n",
      "\n",
      "Train set: Average loss: 0.3965, Accuracy: 52818.0/60000 (88%)\n",
      "\n",
      "Epoch: 4 \tTraining Accuracy: 88.03%\n",
      "\n",
      "Train set: Average loss: 0.3643, Accuracy: 53389.0/60000 (89%)\n",
      "\n",
      "Epoch: 5 \tTraining Accuracy: 88.98%\n",
      "\n",
      "Train set: Average loss: 0.3395, Accuracy: 53831.0/60000 (90%)\n",
      "\n",
      "Epoch: 6 \tTraining Accuracy: 89.72%\n",
      "\n",
      "Train set: Average loss: 0.3185, Accuracy: 54205.0/60000 (90%)\n",
      "\n",
      "Epoch: 7 \tTraining Accuracy: 90.34%\n",
      "\n",
      "Train set: Average loss: 0.3013, Accuracy: 54580.0/60000 (91%)\n",
      "\n",
      "Epoch: 8 \tTraining Accuracy: 90.97%\n",
      "\n",
      "Train set: Average loss: 0.2852, Accuracy: 54857.0/60000 (91%)\n",
      "\n",
      "Epoch: 9 \tTraining Accuracy: 91.43%\n",
      "\n",
      "Train set: Average loss: 0.2852, Accuracy: 54857.0/60000 (91%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'mps'\n",
    "\n",
    "dataset = MNISTtrainer(batch_size)\n",
    "params = dict(device=device,\n",
    "                width=50, lr=0.01, num_train=10,\n",
    "                sb=1, depth=5, sw=1.5)\n",
    "\n",
    "model = generate_fc_dnn(dataset.input_dim, dataset.output_dim,\n",
    "                params['depth'], params['width']).to(device)\n",
    "\n",
    "pretrain_acc, pre_trained_model = compute_training_acc_epochs(model, dataset, params, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNISTtrainer(args.batch_size)\n",
    "params = dict(device='cpu' if args.no_cuda else 'cuda',\n",
    "                width=args.width, lr=args.lr, num_train=args.num_train,\n",
    "                sb=0.05,\n",
    "    )\n",
    "if args.debug: print(params)\n",
    "\n",
    "# run experiment in grid coordinate.\n",
    "sws = np.linspace(1., 4., args.num_sw)\n",
    "\n",
    "# test a lot less depths\n",
    "depths = np.linspace(10, 100, args.num_depth, dtype=int)\n",
    "\n",
    "pretrain_accs = list()\n",
    "all_cut_accuracies = list()\n",
    "\n",
    "for i, depth in enumerate(depths):\n",
    "    for j, sw in enumerate(sws):\n",
    "        params['depth'], params['sw'] = depth, sw\n",
    "        \n",
    "        # build and initialize the DNN model and optimizer\n",
    "        model = generate_fc_dnn(dataset.input_dim, dataset.output_dim,\n",
    "                        params['depth'], params['width']).to(device)\n",
    "        \n",
    "        pretrain_acc, pre_trained_model = compute_training_acc(model, dataset, params, debug=False)\n",
    "        cut_accuracies = calculate_cut_accuracies(model, sw, sb, dataset, params)\n",
    "        all_cut_accuracies.append(cut_accuracies)\n",
    "        \n",
    "        acc = compute_training_acc(dataset, params, debug=args.debug_train_acc)\n",
    "        pretrain_accs.append(pretrain_acc)\n",
    "        \n",
    "        if args.debug: print('({},{})->[d={},sw={}]: \\t Train Acc: {:.6f}'.format(i,j,depth,sw,acc))\n",
    "\n",
    "# acc = np.array(accs).reshape((len(depths), len(sws)))\n",
    "# np.savez('train_acc', xgrid=sws,ygrid=depths,train_acc=acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

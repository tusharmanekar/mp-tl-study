{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tnr5Tf1Qa1H-"
   },
   "source": [
    "***IMPORTS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-11T09:56:26.954565Z",
     "iopub.status.busy": "2023-12-11T09:56:26.953530Z",
     "iopub.status.idle": "2023-12-11T09:56:31.345428Z",
     "shell.execute_reply": "2023-12-11T09:56:31.344297Z"
    },
    "id": "L0eJ0XYWQHS6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import json, copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-11T09:56:31.349214Z",
     "iopub.status.busy": "2023-12-11T09:56:31.348198Z",
     "iopub.status.idle": "2023-12-11T09:56:31.407519Z",
     "shell.execute_reply": "2023-12-11T09:56:31.406634Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch_directml\n",
    "\n",
    "device = torch_directml.device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***MODEL UTILS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2023-12-11T09:56:31.411518Z",
     "iopub.status.busy": "2023-12-11T09:56:31.411518Z",
     "iopub.status.idle": "2023-12-11T09:56:31.453455Z",
     "shell.execute_reply": "2023-12-11T09:56:31.452448Z"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------ MODEL UTILS ----------------------------------------------\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, depth, num_channels, hidden_dim_lin, activation_function, kernel_size, use_pooling=True):\n",
    "        super(CustomCNN, self).__init__()\n",
    "\n",
    "        # Initial number of input channels, assuming grayscale images\n",
    "        in_channels = 1\n",
    "\n",
    "        # Dynamically add convolutional and activation layers based on the specified depth\n",
    "        for i in range(depth):\n",
    "            # Create a convolutional layer and add it to the model\n",
    "            setattr(self, f\"conv{i}\", nn.Conv2d(in_channels, num_channels, kernel_size=kernel_size, padding=math.floor(kernel_size/2)))\n",
    "\n",
    "            # Create an activation layer (e.g., ReLU) and add it to the model\n",
    "            setattr(self, f\"act{i}\", activation_function())\n",
    "\n",
    "            # Update the input dimensions after convolution\n",
    "            input_dim = (input_dim - kernel_size + 2 * math.floor(kernel_size/2)) + 1\n",
    "\n",
    "            # Optionally add pooling layers to reduce spatial dimensions\n",
    "            if use_pooling and (i+1) % depth == 0:\n",
    "                setattr(self, f\"pool{i}\", nn.MaxPool2d(2))\n",
    "                input_dim = input_dim // 2\n",
    "\n",
    "            # Update the input channels for the next convolutional layer\n",
    "            in_channels = num_channels\n",
    "\n",
    "        # Compute the size of the flattened features for the fully connected layer\n",
    "        flattened_size = in_channels * input_dim * input_dim\n",
    "        # Add two fully connected layers for classification\n",
    "        self.fc_1 = nn.Linear(flattened_size, hidden_dim_lin)\n",
    "        self.relu = activation_function()\n",
    "        self.fc_2 = nn.Linear(hidden_dim_lin, output_dim)\n",
    "\n",
    "        # Add log softmax layer for multi-class classification output\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Iterate over each module in the CustomCNN class\n",
    "        for layer_name, layer in self.named_children():\n",
    "            # Process the input tensor through convolutional and activation layers\n",
    "            if \"conv\" in layer_name or \"act\" in layer_name:\n",
    "                x = layer(x)\n",
    "            # Process the input tensor through pooling layers if they exist\n",
    "            elif \"pool\" in layer_name:\n",
    "                x = layer(x)\n",
    "            # If reached fully connected layers, break the loop\n",
    "            elif isinstance(layer, nn.Linear):\n",
    "                break\n",
    "\n",
    "        # Flatten the tensor to fit the input shape of the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Pass the tensor through the fully connected layers\n",
    "        x = self.fc_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc_2(x)\n",
    "\n",
    "        # Return log softmax activated output\n",
    "        return self.logsoftmax(x)\n",
    "\n",
    "def generate_cnn(input_dim, output_dim, depth, num_channels, hidden_dim_lin, kernel_size, activation_function=nn.ReLU, use_pooling=True):\n",
    "    model = CustomCNN(input_dim, output_dim, depth, num_channels, hidden_dim_lin, activation_function, kernel_size, use_pooling)\n",
    "    return model\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    A class for training and evaluating a model with early stopping and best model saving functionalities.\n",
    "\n",
    "    Attributes:\n",
    "    - model: PyTorch model to be trained and evaluated.\n",
    "    - dataloader: Contains data loaders (train, validation, test) for training and evaluation.\n",
    "    - params: Dictionary containing various hyperparameters and settings.\n",
    "    - device: the device to which tensors should be moved before computation.\n",
    "    - optimizer: The optimizer for training.\n",
    "    - best_model_state: State dictionary of the best model.\n",
    "    - max_val_acc: The highest validation accuracy encountered during training.\n",
    "    - no_improve_epochs: Number of epochs without improvement in validation accuracy.\n",
    "    - is_cnn: Flag indicating if the model is a CNN.\n",
    "    - is_debug: Flag indicating if debug information should be printed.\n",
    "    - classification_report_flag: Flag indicating if a classification report should be generated.\n",
    "\n",
    "    Methods:\n",
    "    - train_epoch(): Runs a single epoch of training.\n",
    "    - evaluate(loader): Evaluates the model on a given data loader.\n",
    "    - save_best_model(): Saves the current state of the model as the best model.\n",
    "    - save_checkpoint(epoch, train_acc, val_acc): Saves the current state of the model and other information as a checkpoint.\n",
    "    - early_stopping_check(val_acc): Checks the stopping criterion and performs actions based on it.\n",
    "    - train(): Runs the training process for a number of epochs, with early stopping functionality.\n",
    "\n",
    "    Usage:\n",
    "    params = {\n",
    "      'device': 'cuda',\n",
    "      'lr': 0.001,\n",
    "      'num_train': 10,\n",
    "      'early_stop_patience': 3,\n",
    "      'save_best': True,\n",
    "      'save_checkpoints': False,\n",
    "      'is_cnn': True,\n",
    "      'is_debug': True,\n",
    "      'classification_report_flag': True\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(model, dataloader, params)\n",
    "    train_acc, test_acc, effective_epochs, checkpoints = trainer.train()\n",
    "    \"\"\"\n",
    "    def __init__(self, model, dataloader, lr, params, param_groups=None):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.params = params\n",
    "        self.device = torch.device(params['device'])\n",
    "        if param_groups:\n",
    "            self.optimizer = optim.Adam(param_groups)\n",
    "        else:\n",
    "            self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        # optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        # Initialize best_model_state with the current model state\n",
    "        self.best_model_state = copy.deepcopy(self.model.state_dict())\n",
    "        self.max_val_acc = 0.\n",
    "        self.no_improve_epochs = 0\n",
    "        self.is_cnn = params.get('is_cnn', False)\n",
    "        self.is_debug = params.get('is_debug', False)\n",
    "        self.classification_report_flag = params.get('classification_report_flag', False)\n",
    "        self.logger = params.get('logger', print)\n",
    "\n",
    "    def train_epoch(self):\n",
    "      self.model.train()\n",
    "      for batch_idx, (data, target) in enumerate(self.dataloader.train_loader):\n",
    "          # Print the size of the current batch\n",
    "          if self.is_cnn:\n",
    "            data = data.view(data.size(0), 1, 28, 28)\n",
    "          else:\n",
    "            data = data.reshape([data.shape[0], -1])\n",
    "          data, target = data.to(self.device), target.to(self.device)\n",
    "          self.optimizer.zero_grad()\n",
    "          output = self.model(data)\n",
    "          loss = F.nll_loss(output, target)\n",
    "          loss.backward()\n",
    "          self.optimizer.step()\n",
    "\n",
    "          if self.is_debug and batch_idx % 20 == 0:\n",
    "              self.logger(f\"Batch: {batch_idx}, Loss: {loss.item()}\")\n",
    "\n",
    "    def evaluate(self, loader):\n",
    "        return eval(self.model, self.device, loader, self.is_debug, self.classification_report_flag, self.is_cnn)\n",
    "\n",
    "    def save_best_model(self):\n",
    "        torch.save(self.model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    def save_checkpoint(self, epoch, train_acc, val_acc):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'train_acc': train_acc,\n",
    "            'val_acc': val_acc\n",
    "        }\n",
    "        torch.save(checkpoint, f'checkpoint_epoch_{epoch}.pth')\n",
    "        return checkpoint\n",
    "\n",
    "    def early_stopping_check(self, val_acc):\n",
    "        if val_acc > self.max_val_acc:\n",
    "            self.max_val_acc = val_acc\n",
    "            self.no_improve_epochs = 0\n",
    "            # Deep copy the model's state\n",
    "            self.best_model_state = copy.deepcopy(self.model.state_dict())\n",
    "            if self.params.get('save_best', False):\n",
    "                self.save_best_model()\n",
    "        else:\n",
    "            self.no_improve_epochs += 1\n",
    "            if self.no_improve_epochs >= self.params['early_stop_patience']:\n",
    "                self.logger(\"Early stopping invoked.\")\n",
    "                # Only load if best_model_state has been set\n",
    "                if self.best_model_state is not None:\n",
    "                    self.model.load_state_dict(self.best_model_state)\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def train(self, verbose=1):\n",
    "        effective_epochs = 0\n",
    "        checkpoints = []\n",
    "\n",
    "        for epoch in range(self.params['num_train']):\n",
    "            effective_epochs += 1\n",
    "            self.train_epoch()\n",
    "\n",
    "            train_acc = self.evaluate(self.dataloader.train_loader)\n",
    "            val_acc = self.evaluate(self.dataloader.val_loader)\n",
    "            if verbose >= 1:\n",
    "                self.logger(f'Epoch: {epoch} \\tTraining Accuracy: {train_acc*100:.2f}%')\n",
    "                self.logger(f'Validation Accuracy: {val_acc*100:.2f}%')\n",
    "\n",
    "            if self.params.get('early_stop_patience', None):\n",
    "                if self.early_stopping_check(val_acc):\n",
    "                    self.model.load_state_dict(self.best_model_state)\n",
    "                    break\n",
    "\n",
    "            if self.params.get('save_checkpoints', False):\n",
    "                checkpoint = self.save_checkpoint(epoch, train_acc, val_acc)\n",
    "                checkpoints.append(checkpoint)\n",
    "\n",
    "        # Final evaluations\n",
    "        train_acc = self.evaluate(self.dataloader.train_loader)\n",
    "        test_acc = self.evaluate(self.dataloader.test_loader)\n",
    "\n",
    "        return train_acc, test_acc, effective_epochs, checkpoints\n",
    "\n",
    "def eval(model, device, dataset_loader, debug=False, classification_report_flag=False, is_cnn=True, logger=print):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the given dataset loader.\n",
    "\n",
    "    Parameters:\n",
    "    - model: the PyTorch model to evaluate.\n",
    "    - device: the device to which tensors should be moved before computation.\n",
    "    - dataset_loader: DataLoader for evaluation.\n",
    "    - debug: whether to print debug info like loss and accuracy.\n",
    "    - classification_report_flag: whether to print a classification report.\n",
    "    - is_cnn: a flag indicating if the model is a CNN. If it's not, the input data will be reshaped.\n",
    "    - logger: logging function for printing messages.\n",
    "\n",
    "    Returns:\n",
    "    - Accuracy of the model on the provided dataset loader.\n",
    "\n",
    "    Usage:\n",
    "    - accuracy = eval(model, device, dataset_loader, debug=False, is_cnn=False, classification_report_flag=False)\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    test_loss, correct = 0., 0.\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataset_loader:\n",
    "            if is_cnn:\n",
    "              data = data.view(data.size(0), 1, 28, 28)\n",
    "            else:\n",
    "              data = data.reshape([data.shape[0], -1])\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(target.cpu().numpy())\n",
    "\n",
    "    num_data = len(dataset_loader.dataset)\n",
    "    test_loss /= num_data\n",
    "    acc = correct / num_data\n",
    "\n",
    "    if debug:\n",
    "        logger('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, num_data, 100. * acc))\n",
    "\n",
    "    if classification_report_flag:\n",
    "        unique_labels = np.unique(all_labels).tolist()\n",
    "        logger(classification_report(all_labels, all_preds, labels=unique_labels, target_names=[f'Class {i}' for i in unique_labels]))\n",
    "\n",
    "    return acc\n",
    "\n",
    "def cut_custom_cnn_model(model, cut_point, params):\n",
    "    new_model = copy.deepcopy(model)\n",
    "    reinitialized_layers = []  # List to store reinitialized layer names\n",
    "\n",
    "    # Get the names of all layers\n",
    "    layer_names = list(new_model._modules.keys())\n",
    "\n",
    "    # Find indices of Conv layers\n",
    "    conv_indices = [i for i, name in enumerate(layer_names) if isinstance(getattr(new_model, name), nn.Conv2d)]\n",
    "\n",
    "    # Freeze layers before cut_point if required\n",
    "    if params.get(\"freeze\", False):\n",
    "        for idx in conv_indices[:cut_point]:\n",
    "            for param in getattr(new_model, layer_names[idx]).parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    # Reinitialize dense layers (fc_1 and fc_2) if specified\n",
    "    if params[\"reinit_both_dense\"]:\n",
    "        new_model.fc_1.reset_parameters()\n",
    "        new_model.fc_2.reset_parameters()\n",
    "        reinitialized_layers.extend([\"fc_1\", \"fc_2\"])\n",
    "\n",
    "    # Reinitialize conv layers after the cut_point\n",
    "    if cut_point > 0 and params[\"reinit\"]:  # Only reinitialize conv layers if cut point is greater than 0\n",
    "        for idx in conv_indices[cut_point:]:\n",
    "            layer_name = layer_names[idx]\n",
    "            getattr(new_model, layer_name).reset_parameters()\n",
    "            reinitialized_layers.append(layer_name)\n",
    "\n",
    "    return new_model, reinitialized_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***DATA UTILS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-11T09:56:31.458926Z",
     "iopub.status.busy": "2023-12-11T09:56:31.457409Z",
     "iopub.status.idle": "2023-12-11T09:56:31.484141Z",
     "shell.execute_reply": "2023-12-11T09:56:31.483008Z"
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------- DATA UTILS -----------------------------------\n",
    "def reduce_dataset(dataloader, percentage, balanced=True, seed=42):\n",
    "\n",
    "    \"\"\"\n",
    "    Reduces the dataset to the given percentage. Can ensure class balance if needed.\n",
    "\n",
    "    Parameters:\n",
    "    - dataloader: PyTorch DataLoader object.\n",
    "    - percentage: Desired percentage of the original dataset.\n",
    "    - balanced: If True, ensures class balance. If False, reduces randomly.\n",
    "    - seed: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - reduced_dataloader: DataLoader with the reduced dataset.\n",
    "    \"\"\"\n",
    "    # Extract the dataset from the dataloader\n",
    "    dataset = dataloader.dataset\n",
    "\n",
    "    # Extract all data and labels from the dataset\n",
    "    X = [dataset[i][0] for i in range(len(dataset))]\n",
    "    y = [dataset[i][1] for i in range(len(dataset))]\n",
    "\n",
    "    # Set the seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if not balanced:\n",
    "        # Determine the number of samples to keep\n",
    "        num_samples = int(len(dataset) * percentage)\n",
    "\n",
    "        # Randomly select indices without replacement\n",
    "        indices = torch.randperm(len(dataset))[:num_samples].tolist()\n",
    "\n",
    "    else:\n",
    "        # Get unique classes and their counts\n",
    "        classes, class_counts = torch.unique(torch.tensor(y), return_counts=True)\n",
    "\n",
    "        # Determine the number of samples per class to keep\n",
    "        num_samples_per_class = int(len(dataset) * percentage / len(classes))\n",
    "        indices = []\n",
    "\n",
    "        for class_label in classes:\n",
    "            class_indices = [i for i, label in enumerate(y) if label == class_label]\n",
    "\n",
    "            # Randomly select indices without replacement for each class\n",
    "            class_selected_indices = torch.randperm(len(class_indices))[:num_samples_per_class].tolist()\n",
    "            indices.extend([class_indices[i] for i in class_selected_indices])\n",
    "\n",
    "    # Use a Subset of the original dataset to create a reduced dataset\n",
    "    reduced_dataset = data.Subset(dataset, indices)\n",
    "\n",
    "    # Create a DataLoader with the reduced dataset.\n",
    "    reduced_dataloader = data.DataLoader(reduced_dataset, batch_size=dataloader.batch_size, shuffle=True)\n",
    "\n",
    "    return reduced_dataloader\n",
    "\n",
    "class RelabeledSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, offset):\n",
    "        self.dataset = dataset\n",
    "        self.offset = offset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.dataset[idx]\n",
    "        # Offset the label to start from 0\n",
    "        label = label - self.offset\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "class TransferLearningFMNIST(object):\n",
    "    def __init__(self, batch_size, input_dim=28*28, val_split=0.1, num_workers=0, seed=42):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = 10\n",
    "        self.val_split = val_split\n",
    "\n",
    "        def filter_dataset(dataset, classes):\n",
    "            indices = [i for i, t in enumerate(dataset.targets) if t in classes]\n",
    "            return torch.utils.data.Subset(dataset, indices)\n",
    "\n",
    "        # Replace MNIST with FashionMNIST\n",
    "        fmnist_train_data = datasets.FashionMNIST(\n",
    "            '../data',\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Lambda(lambda x: (x * 2 - 1) * 0.5),\n",
    "            ]))\n",
    "        \n",
    "        # Classes for pretraining and finetuning\n",
    "        pretrain_classes = [0, 1, 2, 3, 4, 6, 8]\n",
    "        finetune_classes = [5, 7, 9]  # Sandal, Sneaker, Ankle boot\n",
    "\n",
    "        # Filter datasets accordingly\n",
    "        pretrain_train_data = filter_dataset(fmnist_train_data, pretrain_classes)\n",
    "        finetune_train_data = filter_dataset(fmnist_train_data, finetune_classes)\n",
    "\n",
    "        pretrain_len = len(pretrain_train_data)\n",
    "        finetune_len = len(finetune_train_data)\n",
    "        pretrain_val_len = int(val_split * pretrain_len)\n",
    "        finetune_val_len = int(val_split * finetune_len)\n",
    "\n",
    "        pretrain_train_set, pretrain_val_set = torch.utils.data.random_split(\n",
    "            pretrain_train_data, [pretrain_len - pretrain_val_len, pretrain_val_len], generator=torch.Generator().manual_seed(seed))\n",
    "        finetune_train_set, finetune_val_set = torch.utils.data.random_split(\n",
    "            finetune_train_data, [finetune_len - finetune_val_len, finetune_val_len], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "        self.pretrain_train_loader = torch.utils.data.DataLoader(pretrain_train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        self.pretrain_val_loader = torch.utils.data.DataLoader(pretrain_val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "        # Use the RelabeledSubset for fine-tuning datasets\n",
    "        finetune_train_set = RelabeledSubset(finetune_train_set, 5)\n",
    "        finetune_val_set = RelabeledSubset(finetune_val_set, 5)\n",
    "\n",
    "        self.finetune_train_loader = torch.utils.data.DataLoader(finetune_train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        self.finetune_val_loader = torch.utils.data.DataLoader(finetune_val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "        fmnist_test_data = datasets.FashionMNIST(\n",
    "            '../data',\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Lambda(lambda x: (x * 2 - 1) * 0.5),\n",
    "            ]))\n",
    "\n",
    "        pretrain_test_data = filter_dataset(fmnist_test_data, pretrain_classes)\n",
    "        finetune_test_data = filter_dataset(fmnist_test_data, finetune_classes)\n",
    "\n",
    "        self.pretrain_test_loader = torch.utils.data.DataLoader(pretrain_test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "        # Use the RelabeledSubset for fine-tuning test datasets\n",
    "        finetune_test_data = RelabeledSubset(finetune_test_data, 5)\n",
    "        self.finetune_test_loader = torch.utils.data.DataLoader(finetune_test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "        # Complete test loader contains all test examples.\n",
    "        self.complete_test_loader = torch.utils.data.DataLoader(fmnist_test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "class TransferLearningFMNISTWrapper:\n",
    "    \"\"\"\n",
    "    This wrapper class provides a convenient way to switch between pretraining and fine-tuning phases.\n",
    "\n",
    "    It allows for changing the phase and accordingly updating the data loaders (train, val, test)\n",
    "    to either pretraining or fine-tuning sets.\n",
    "    \"\"\"\n",
    "    def __init__(self, transferLearningFMNISTObj, phase):\n",
    "        \"\"\"\n",
    "        Initializes the TransferLearningFMNISTWrapper object.\n",
    "\n",
    "        Parameters:\n",
    "        - transferLearningFMNISTObj: An instance of the TransferLearningFMNIST class.\n",
    "        - phase: String indicating the current phase (\"pretrain\" or \"finetune\").\n",
    "        \"\"\"\n",
    "        self.transferLearningFMNISTObj = transferLearningFMNISTObj\n",
    "        self.phase = phase\n",
    "        self.input_dim = self.transferLearningFMNISTObj.input_dim\n",
    "        self.output_dim = self.transferLearningFMNISTObj.output_dim\n",
    "        self.update_phase(phase)\n",
    "\n",
    "    def update_phase(self, phase):\n",
    "        \"\"\"\n",
    "        Updates the phase and the corresponding data loaders.\n",
    "\n",
    "        Parameters:\n",
    "        - phase: String indicating the desired phase (\"pretrain\" or \"finetune\").\n",
    "\n",
    "        Throws:\n",
    "        - ValueError: If the phase is neither \"pretrain\" nor \"finetune\".\n",
    "        \"\"\"\n",
    "        self.phase = phase\n",
    "        if phase == 'pretrain':\n",
    "            self.train_loader = self.transferLearningFMNISTObj.pretrain_train_loader\n",
    "            self.val_loader = self.transferLearningFMNISTObj.pretrain_val_loader\n",
    "            self.test_loader = self.transferLearningFMNISTObj.pretrain_test_loader\n",
    "        elif phase == 'finetune':\n",
    "            self.train_loader = self.transferLearningFMNISTObj.finetune_train_loader\n",
    "            self.val_loader = self.transferLearningFMNISTObj.finetune_val_loader\n",
    "            self.test_loader = self.transferLearningFMNISTObj.finetune_test_loader\n",
    "        else:\n",
    "            raise ValueError('Phase must be either \"pretrain\" or \"finetune\".')\n",
    "\n",
    "    def get_current_phase(self):\n",
    "      return self.phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***PLOTTING UTILS***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-11T09:56:31.487997Z",
     "iopub.status.busy": "2023-12-11T09:56:31.486654Z",
     "iopub.status.idle": "2023-12-11T09:56:31.499423Z",
     "shell.execute_reply": "2023-12-11T09:56:31.498425Z"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------ PLOTTING UTILS -------------------------------------------\n",
    "'''def effective_rank(singular_values):\n",
    "    sigma_max = np.max(singular_values)\n",
    "    sigma_min = singular_values[-1] if singular_values[-1] > 0 else np.min(singular_values[singular_values > 0])\n",
    "    # print(sigma_max, sigma_min)\n",
    "    print(np.sqrt(sigma_max / sigma_min))\n",
    "    print('----')\n",
    "    return np.sqrt(sigma_max / sigma_min)'''\n",
    "\n",
    "def effective_rank(singular_values):\n",
    "    normalized_singular_values = singular_values / np.sum(singular_values)\n",
    "    entropy = -np.sum(normalized_singular_values * np.log(normalized_singular_values))\n",
    "    eff_rank = np.exp(entropy)\n",
    "    return eff_rank\n",
    "\n",
    "def plot_layer_effective_ranks(model, print_ranks=True):\n",
    "    effective_ranks = []\n",
    "    layer_names = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:  # We are only interested in weight matrices\n",
    "            weight_matrix = param.detach().cpu().numpy()\n",
    "            singular_values = np.linalg.svd(weight_matrix, compute_uv=False)\n",
    "            eff_rank = effective_rank(singular_values)\n",
    "            effective_ranks.append(eff_rank)\n",
    "            layer_names.append(name)\n",
    "\n",
    "    if print_ranks:\n",
    "        for layer_name, eff_rank in zip(layer_names, effective_ranks):\n",
    "            print(f'{layer_name}: {eff_rank:.4f}')\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.bar(layer_names, effective_ranks, color='green')\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('Effective Rank')\n",
    "    plt.title('Effective Rank of Weight Matrices for Each Layer')\n",
    "    plt.grid(True)\n",
    "\n",
    "    y_max = np.max(effective_ranks) + 1  # Get maximum rank and add 1 for better visualization\n",
    "    y_min = np.min(effective_ranks) - 1  # Get minimum rank and subtract 1 for better visualization\n",
    "    plt.yticks(np.arange(0, int(y_max)+2, step=2))  # Set yticks\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:00<00:00, 94986973.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ../data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 4023102.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ../data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 4422102/4422102 [00:00<00:00, 39502482.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ../data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 4285032.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../data\\FashionMNIST\\raw\n",
      "\n",
      "Testing datasets...\n",
      "\n",
      "Pretrain Train Dataset:\n",
      "Total samples: 37800\n",
      "Class distribution: {0: 5440, 1: 5362, 2: 5405, 3: 5385, 4: 5421, 5: 0, 6: 5374, 7: 0, 8: 5413, 9: 0}\n",
      "\n",
      "Pretrain Val Dataset:\n",
      "Total samples: 4200\n",
      "Class distribution: {0: 560, 1: 638, 2: 595, 3: 615, 4: 579, 5: 0, 6: 626, 7: 0, 8: 587, 9: 0}\n",
      "\n",
      "Pretrain Test Dataset:\n",
      "Total samples: 7000\n",
      "Class distribution: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 0, 6: 1000, 7: 0, 8: 1000, 9: 0}\n",
      "\n",
      "Finetune Train Dataset:\n",
      "Total samples: 16200\n",
      "Class distribution: {0: 5404, 1: 0, 2: 5396, 3: 0, 4: 5400, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\n",
      "\n",
      "Finetune Val Dataset:\n",
      "Total samples: 1800\n",
      "Class distribution: {0: 596, 1: 0, 2: 604, 3: 0, 4: 600, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\n",
      "\n",
      "Finetune Test Dataset:\n",
      "Total samples: 3000\n",
      "Class distribution: {0: 1000, 1: 0, 2: 1000, 3: 0, 4: 1000, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\n",
      "\n",
      "Complete Test Dataset:\n",
      "Total samples: 10000\n",
      "Class distribution: {0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000}\n"
     ]
    }
   ],
   "source": [
    "# Utility function to get class distribution in a dataset\n",
    "def get_class_distribution(dataset):\n",
    "    count_dict = {i: 0 for i in range(10)}\n",
    "    for _, label in dataset:\n",
    "        count_dict[label] += 1\n",
    "    return count_dict\n",
    "\n",
    "# Function to test datasets\n",
    "def test_datasets(tlmnist):\n",
    "    print(\"Testing datasets...\")\n",
    "\n",
    "    # Test and print details for each dataset\n",
    "    datasets = {\n",
    "        \"Pretrain Train\": tlmnist.pretrain_train_loader.dataset,\n",
    "        \"Pretrain Val\": tlmnist.pretrain_val_loader.dataset,\n",
    "        \"Pretrain Test\": tlmnist.pretrain_test_loader.dataset,\n",
    "        \"Finetune Train\": tlmnist.finetune_train_loader.dataset,\n",
    "        \"Finetune Val\": tlmnist.finetune_val_loader.dataset,\n",
    "        \"Finetune Test\": tlmnist.finetune_test_loader.dataset,\n",
    "        \"Complete Test\": tlmnist.complete_test_loader.dataset\n",
    "    }\n",
    "\n",
    "    for name, dataset in datasets.items():\n",
    "        print(f\"\\n{name} Dataset:\")\n",
    "        print(f\"Total samples: {len(dataset)}\")\n",
    "        class_distribution = get_class_distribution(dataset)\n",
    "        print(f\"Class distribution: {class_distribution}\")\n",
    "\n",
    "# Initialize the TransferLearningMNIST object\n",
    "tlmnist = TransferLearningFMNIST(batch_size=64)\n",
    "\n",
    "# Test the datasets\n",
    "test_datasets(tlmnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJ5JiRZYQW-J"
   },
   "source": [
    "# EXPERIMENT SETUP 1: _FREEZE, REINIT, POOLING, DENSE:REINIT BOTH_\n",
    "- percentages_set_1 = [0.001, 0.002, 0.005, 0.01, 0.05, 0.1, 0.3, 0.5, 0.8, 1]\n",
    "- dataset: same as before MNIST 5 to 5\n",
    "\n",
    "- architecture:\n",
    "  - Conv 1 (5,5), channels=10\n",
    "  - Relu\n",
    "  - Conv 2 (5,5), channels=10\n",
    "  - Relu\n",
    "  - Conv 3 (5,5), channels=10\n",
    "  - Relu\n",
    "  - _POOLING_\n",
    "  - Dense 1 (x, a) x=output shape of prev layer, a:random hidden layer width (we use 128)\n",
    "  - Relu\n",
    "  - Dense 2 (a, 5)\n",
    "  - softmax\n",
    "\n",
    "- lr pretraining = 0.001\n",
    "- lr finetuning = 0.0001\n",
    "- lr end-to-end = 0.001\n",
    "\n",
    "- Freezing the layers before the cut: _YES_\n",
    "- Reinitializing the Convolutional layers after the cut: _YES_\n",
    "- Reinitializing Dense 1: _YES_\n",
    "- Reinitializing Dense 2: _YES_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-11T09:56:31.503753Z",
     "iopub.status.busy": "2023-12-11T09:56:31.503753Z",
     "iopub.status.idle": "2023-12-11T09:56:31.514540Z",
     "shell.execute_reply": "2023-12-11T09:56:31.513538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='privateuseone', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "device = torch_directml.device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-11T09:56:31.574685Z",
     "iopub.status.busy": "2023-12-11T09:56:31.573689Z",
     "iopub.status.idle": "2023-12-11T09:56:31.591875Z",
     "shell.execute_reply": "2023-12-11T09:56:31.590781Z"
    },
    "id": "FeWfmLUgswad"
   },
   "outputs": [],
   "source": [
    "# percentages = [0.001, 0.002, 0.005, 0.01, 0.05, 0.1, 0.3, 0.5, 0.8, 1]\n",
    "percentages = [0.001, 0.002, 0.005, 0.01, 0.05, 0.1]\n",
    "# percentages = [0.001, 0.002]\n",
    "\n",
    "# cuts=0 means: end-to-end model if we are reinitializing\n",
    "cuts = [0, 1 ,2, 3]\n",
    "seed_set = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]  # currently not being used\n",
    "batch_size = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbrS0kwrcHfE"
   },
   "source": [
    "## Pretraining\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-11T09:56:31.596302Z",
     "iopub.status.busy": "2023-12-11T09:56:31.595302Z",
     "iopub.status.idle": "2023-12-11T09:56:34.530200Z",
     "shell.execute_reply": "2023-12-11T09:56:34.529200Z"
    },
    "id": "6sK-eF2ucUDa",
    "outputId": "f7f2e890-7c35-48ee-edab-dc0e97d5ce52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomCNN(\n",
       "  (conv0): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (act0): ReLU()\n",
       "  (conv1): Conv2d(10, 10, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (act1): ReLU()\n",
       "  (conv2): Conv2d(10, 10, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (act2): ReLU()\n",
       "  (fc_1): Linear(in_features=7840, out_features=128, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc_2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (logsoftmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = TransferLearningFMNIST(batch_size)\n",
    "dataloader_wrapped = TransferLearningFMNISTWrapper(dataloader, phase = 'pretrain')\n",
    "\n",
    "# Changes Here for the experiments\n",
    "params = {\n",
    "      'depth': 3,\n",
    "      'width': 10, # num channels for CNN\n",
    "      'hidden_dim_lin': 128,\n",
    "      'activation_function': nn.ReLU,\n",
    "      'kernel_size': 5,\n",
    "      'device': device,\n",
    "      'lr_pretrain': 0.001,\n",
    "      'lr_fine_tune': 0.001,  # CHANGE: if no layer-wise lr\n",
    "      'lr_fine_tune_reinit': 0.001,         # CHANGE: if no layer-wise lr\n",
    "      'lr_fine_tune_no_reinit': 0.0001,     # CHANGE: if layer-wise lr\n",
    "      'num_train': 40,\n",
    "      'early_stop_patience': 6,\n",
    "      'save_best': True,\n",
    "      'save_checkpoints': False,\n",
    "      'is_cnn': True,\n",
    "      'is_debug': False,\n",
    "      'classification_report_flag': False,\n",
    "      'percentages':percentages,\n",
    "      'batch_size':batch_size,\n",
    "      'seed_set':seed_set,\n",
    "      'use_pooling': False,   # CHANGE\n",
    "      'freeze': True,         # CHANGE: freeze the conv layers before the cut\n",
    "      'reinit': False,         # CHANGE: reinit the conv lyers only after the cut\n",
    "      'reinit_both_dense': True   # CHANGE: True for reinitialize both dense layers, False for reinit only the last dense layer\n",
    "    }\n",
    "\n",
    "# Create DNN model\n",
    "pretrained_model = generate_cnn(input_dim = 28, output_dim = 10, depth = params['depth'], num_channels = params['width'],\n",
    "                    hidden_dim_lin = params['hidden_dim_lin'], kernel_size = params['kernel_size'], activation_function = params[\"activation_function\"], use_pooling=params['use_pooling'])\n",
    "pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-11T09:56:34.536204Z",
     "iopub.status.busy": "2023-12-11T09:56:34.535207Z",
     "iopub.status.idle": "2023-12-11T09:56:34.545541Z",
     "shell.execute_reply": "2023-12-11T09:56:34.544537Z"
    },
    "id": "gU9NwDAjhT3o",
    "outputId": "7335d6d9-a841-4397-b879-6ceef21ab105"
   },
   "outputs": [],
   "source": [
    "# # Train and evaluate\n",
    "# trainer = Trainer(pretrained_model, dataloader_wrapped, params[\"lr_pretrain\"], params)\n",
    "# train_acc, test_acc, effective_epochs, checkpoints = trainer.train()\n",
    "\n",
    "# print(f\"Final Training Accuracy: {train_acc:.4f}\")\n",
    "# print(f\"Final Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-11T09:56:34.551048Z",
     "iopub.status.busy": "2023-12-11T09:56:34.551048Z",
     "iopub.status.idle": "2023-12-11T09:56:34.560654Z",
     "shell.execute_reply": "2023-12-11T09:56:34.559648Z"
    }
   },
   "outputs": [],
   "source": [
    "# eval(pretrained_model, device, dataloader_wrapped.val_loader, debug=True, classification_report_flag=True, is_cnn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ge8Q1YiEqC7e"
   },
   "source": [
    "## Fine-tuning Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-11T09:56:34.566461Z",
     "iopub.status.busy": "2023-12-11T09:56:34.565958Z",
     "iopub.status.idle": "2023-12-11T09:56:34.576557Z",
     "shell.execute_reply": "2023-12-11T09:56:34.575051Z"
    }
   },
   "outputs": [],
   "source": [
    "# # load results: to continue from a checkpoint (actually don't run)\n",
    "# with open('results.json', 'r') as f:\n",
    "#     results = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines (skip if it was already run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-11T09:56:34.582005Z",
     "iopub.status.busy": "2023-12-11T09:56:34.581008Z",
     "iopub.status.idle": "2023-12-11T09:56:34.591225Z",
     "shell.execute_reply": "2023-12-11T09:56:34.590719Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #training of baseline, end to end, models (#trials x #percentages)\n",
    "# results_baseline = []\n",
    "\n",
    "# dataloader_wrapped.update_phase('finetune')\n",
    "\n",
    "# # template_model = generate_cnn(input_dim = 28, output_dim = 10, depth = params['depth'], num_channels = params['width'],\n",
    "# #                      hidden_dim_lin = params['hidden_dim_lin'], kernel_size = params['kernel_size'], activation_function = params[\"activation_function\"], use_pooling=params['use_pooling'])\n",
    "\n",
    "# for sampled_percentage in percentages:      \n",
    "#     if sampled_percentage <= 0.01:\n",
    "#         repeats = 10\n",
    "#     elif sampled_percentage < 0.5:\n",
    "#         repeats = 5\n",
    "#     else:\n",
    "#         repeats = 3\n",
    "    \n",
    "#     for repeat in range(repeats):\n",
    "#         # Print or log the sampled values for transparency\n",
    "#         print(f\"\\nSampled Percentage: {sampled_percentage}, Lr: {params['lr_fine_tune']}, Repeat: {repeat}\")\n",
    "\n",
    "#         # Reduce the dataset\n",
    "#         train_loader_reduced = reduce_dataset(dataloader_wrapped.train_loader, sampled_percentage, seed = repeat)\n",
    "#         torch.manual_seed(repeat)\n",
    "#         #train_loader_reduced = reduce_dataset(dataloader_wrapped.train_loader, sampled_percentage, seed = repeat)\n",
    "#         dataset_namespace_new = SimpleNamespace(train_loader=train_loader_reduced, test_loader=dataloader_wrapped.test_loader, val_loader=dataloader_wrapped.val_loader)\n",
    "\n",
    "#         # Copy and then cut the model - we already deepcopy it in the function: pretrained_model\n",
    "#         #model_temp = copy.deepcopy(template_model)\n",
    "#         model_temp = generate_cnn(input_dim = 28, output_dim = 10, depth = params['depth'], num_channels = params['width'],\n",
    "#         hidden_dim_lin = params['hidden_dim_lin'], kernel_size = params['kernel_size'], activation_function = params[\"activation_function\"], use_pooling=params['use_pooling'])\n",
    "#         model_temp.to(device)\n",
    "\n",
    "#         # Train and evaluate\n",
    "#         trainer = Trainer(model_temp, dataset_namespace_new, params['lr_fine_tune'], params)\n",
    "#         train_acc, test_acc, effective_epochs, checkpoints = trainer.train(verbose=0)\n",
    "#         print(f\"Training Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "#         # Store the results\n",
    "#         results_baseline.append({\"lr\":params['lr_fine_tune'], \"sampled_percentage\":sampled_percentage, \"sampled_cut_point\":-1, \"repeat\":repeat, \"train_acc\":train_acc, \"test_acc\":test_acc}) # -1 for the cut point means it's baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-11T09:56:34.596395Z",
     "iopub.status.busy": "2023-12-11T09:56:34.594889Z",
     "iopub.status.idle": "2023-12-11T09:56:34.607546Z",
     "shell.execute_reply": "2023-12-11T09:56:34.606367Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Save the results to a JSON file\n",
    "# results_filename = 'results_baseline.json'\n",
    "# with open(results_filename, 'w') as file:\n",
    "#     json.dump(results_baseline, file)\n",
    "\n",
    "# print(f\"Results saved to {results_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-11T09:56:34.612381Z",
     "iopub.status.busy": "2023-12-11T09:56:34.612381Z",
     "iopub.status.idle": "2023-12-11T11:44:05.449740Z",
     "shell.execute_reply": "2023-12-11T11:44:05.448153Z"
    },
    "id": "hMZ4o1FGsipP",
    "outputId": "08ac8a68-17f6-48a3-e06e-27f484b9507d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 0, Repeat: 0\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 0, Repeat: 1\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 0, Repeat: 2\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 0, Repeat: 3\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 0, Repeat: 4\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 0, Repeat: 5\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 0, Repeat: 6\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 0, Repeat: 7\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 0, Repeat: 8\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 0, Repeat: 9\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 1, Repeat: 0\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 1, Repeat: 1\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 1, Repeat: 2\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 1, Repeat: 3\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 1, Repeat: 4\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 1, Repeat: 5\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 1, Repeat: 6\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 1, Repeat: 7\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 1, Repeat: 8\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 1, Repeat: 9\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 2, Repeat: 0\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 2, Repeat: 1\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 2, Repeat: 2\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 2, Repeat: 3\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 2, Repeat: 4\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 2, Repeat: 5\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 2, Repeat: 6\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 2, Repeat: 7\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 2, Repeat: 8\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 2, Repeat: 9\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 3, Repeat: 0\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 3, Repeat: 1\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 3, Repeat: 2\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 3, Repeat: 3\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 3, Repeat: 4\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 3, Repeat: 5\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 3, Repeat: 6\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 3, Repeat: 7\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 3, Repeat: 8\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.001, Sampled Cut Point: 3, Repeat: 9\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 0, Repeat: 0\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 0, Repeat: 1\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 0, Repeat: 2\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 0, Repeat: 3\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 0, Repeat: 4\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 0, Repeat: 5\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 0, Repeat: 6\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 0, Repeat: 7\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 0, Repeat: 8\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 0, Repeat: 9\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 1, Repeat: 0\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 1, Repeat: 1\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 1, Repeat: 2\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 1, Repeat: 3\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 1, Repeat: 4\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 1, Repeat: 5\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 1, Repeat: 6\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 1, Repeat: 7\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 1, Repeat: 8\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 1, Repeat: 9\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 2, Repeat: 0\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 2, Repeat: 1\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 2, Repeat: 2\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 2, Repeat: 3\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 2, Repeat: 4\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 2, Repeat: 5\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 2, Repeat: 6\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 2, Repeat: 7\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 2, Repeat: 8\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 2, Repeat: 9\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 3, Repeat: 0\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 3, Repeat: 1\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 3, Repeat: 2\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 3, Repeat: 3\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 3, Repeat: 4\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 3, Repeat: 5\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 3, Repeat: 6\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 3, Repeat: 7\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 3, Repeat: 8\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.002, Sampled Cut Point: 3, Repeat: 9\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 0, Repeat: 0\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 0, Repeat: 1\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 0, Repeat: 2\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 0, Repeat: 3\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 0, Repeat: 4\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 0, Repeat: 5\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 0, Repeat: 6\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 0, Repeat: 7\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 0, Repeat: 8\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 0, Repeat: 9\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 1, Repeat: 0\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 1, Repeat: 1\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 1, Repeat: 2\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 1, Repeat: 3\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 1, Repeat: 4\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 1, Repeat: 5\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 1, Repeat: 6\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 1, Repeat: 7\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 1, Repeat: 8\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 1, Repeat: 9\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 2, Repeat: 0\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 2, Repeat: 1\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 2, Repeat: 2\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 2, Repeat: 3\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 2, Repeat: 4\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 2, Repeat: 5\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 2, Repeat: 6\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 2, Repeat: 7\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 2, Repeat: 8\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 2, Repeat: 9\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 3, Repeat: 0\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 3, Repeat: 1\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 3, Repeat: 2\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 3, Repeat: 3\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 3, Repeat: 4\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 3, Repeat: 5\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 3, Repeat: 6\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 3, Repeat: 7\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 3, Repeat: 8\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.005, Sampled Cut Point: 3, Repeat: 9\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 0, Repeat: 0\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 0, Repeat: 1\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 0, Repeat: 2\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 0, Repeat: 3\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 0, Repeat: 4\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 0, Repeat: 5\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 0, Repeat: 6\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 0, Repeat: 7\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 0, Repeat: 8\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 0, Repeat: 9\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 1, Repeat: 0\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 1, Repeat: 1\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 1, Repeat: 2\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 1, Repeat: 3\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 1, Repeat: 4\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 1, Repeat: 5\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 1, Repeat: 6\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 1, Repeat: 7\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 1, Repeat: 8\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 1, Repeat: 9\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 2, Repeat: 0\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 2, Repeat: 1\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 2, Repeat: 2\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 2, Repeat: 3\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 2, Repeat: 4\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 2, Repeat: 5\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 2, Repeat: 6\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 2, Repeat: 7\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 2, Repeat: 8\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 2, Repeat: 9\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 3, Repeat: 0\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 3, Repeat: 1\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 3, Repeat: 2\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 3, Repeat: 3\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 3, Repeat: 4\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 3, Repeat: 5\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 3, Repeat: 6\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 3, Repeat: 7\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 3, Repeat: 8\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.01, Sampled Cut Point: 3, Repeat: 9\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.05, Sampled Cut Point: 0, Repeat: 0\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.05, Sampled Cut Point: 0, Repeat: 1\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.05, Sampled Cut Point: 0, Repeat: 2\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.05, Sampled Cut Point: 0, Repeat: 3\n",
      "\n",
      "Sampled Percentage: 0.05, Sampled Cut Point: 0, Repeat: 4\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.05, Sampled Cut Point: 1, Repeat: 0\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.05, Sampled Cut Point: 1, Repeat: 1\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.05, Sampled Cut Point: 1, Repeat: 2\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.05, Sampled Cut Point: 1, Repeat: 3\n",
      "\n",
      "Sampled Percentage: 0.05, Sampled Cut Point: 1, Repeat: 4\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.05, Sampled Cut Point: 2, Repeat: 0\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.05, Sampled Cut Point: 2, Repeat: 1\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.05, Sampled Cut Point: 2, Repeat: 2\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.05, Sampled Cut Point: 2, Repeat: 3\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.05, Sampled Cut Point: 2, Repeat: 4\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.05, Sampled Cut Point: 3, Repeat: 0\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.05, Sampled Cut Point: 3, Repeat: 1\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.05, Sampled Cut Point: 3, Repeat: 2\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.05, Sampled Cut Point: 3, Repeat: 3\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.05, Sampled Cut Point: 3, Repeat: 4\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.1, Sampled Cut Point: 0, Repeat: 0\n",
      "\n",
      "Sampled Percentage: 0.1, Sampled Cut Point: 0, Repeat: 1\n",
      "\n",
      "Sampled Percentage: 0.1, Sampled Cut Point: 0, Repeat: 2\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.1, Sampled Cut Point: 0, Repeat: 3\n",
      "\n",
      "Sampled Percentage: 0.1, Sampled Cut Point: 0, Repeat: 4\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.1, Sampled Cut Point: 1, Repeat: 0\n",
      "\n",
      "Sampled Percentage: 0.1, Sampled Cut Point: 1, Repeat: 1\n",
      "\n",
      "Sampled Percentage: 0.1, Sampled Cut Point: 1, Repeat: 2\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.1, Sampled Cut Point: 1, Repeat: 3\n",
      "\n",
      "Sampled Percentage: 0.1, Sampled Cut Point: 1, Repeat: 4\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.1, Sampled Cut Point: 2, Repeat: 0\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.1, Sampled Cut Point: 2, Repeat: 1\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.1, Sampled Cut Point: 2, Repeat: 2\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.1, Sampled Cut Point: 2, Repeat: 3\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.1, Sampled Cut Point: 2, Repeat: 4\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.1, Sampled Cut Point: 3, Repeat: 0\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.1, Sampled Cut Point: 3, Repeat: 1\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.1, Sampled Cut Point: 3, Repeat: 2\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.1, Sampled Cut Point: 3, Repeat: 3\n",
      "Early stopping invoked.\n",
      "\n",
      "Sampled Percentage: 0.1, Sampled Cut Point: 3, Repeat: 4\n",
      "Early stopping invoked.\n"
     ]
    }
   ],
   "source": [
    "# The main training loop\n",
    "dataloader_wrapped.update_phase('finetune')\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "for sampled_percentage in percentages:\n",
    "    if sampled_percentage <= 0.01:\n",
    "        repeats = 10\n",
    "    elif sampled_percentage < 0.5:\n",
    "        repeats = 5\n",
    "    else:\n",
    "        repeats = 1\n",
    "\n",
    "    for sampled_cut_point in cuts:\n",
    "        for repeat in range(repeats):\n",
    "            print(f\"\\nSampled Percentage: {sampled_percentage}, Sampled Cut Point: {sampled_cut_point}, Repeat: {repeat}\")\n",
    "\n",
    "            train_loader_reduced = reduce_dataset(dataloader_wrapped.train_loader, sampled_percentage, seed=repeat)\n",
    "            dataset_namespace_new = SimpleNamespace(train_loader=train_loader_reduced, test_loader=dataloader_wrapped.test_loader, val_loader=dataloader_wrapped.val_loader)\n",
    "            torch.manual_seed(repeat)\n",
    "\n",
    "            model_new, reinitialized_layers = cut_custom_cnn_model(pretrained_model, sampled_cut_point, params)\n",
    "\n",
    "            param_groups = [\n",
    "                {'params': [param for name, param in model_new.named_parameters() \n",
    "                            if any(name.startswith(reinit_layer) for reinit_layer in reinitialized_layers)], \n",
    "                 'lr': params['lr_fine_tune_reinit']},\n",
    "                {'params': [param for name, param in model_new.named_parameters() \n",
    "                            if not any(name.startswith(reinit_layer) for reinit_layer in reinitialized_layers)], \n",
    "                 'lr': params['lr_fine_tune_no_reinit']}\n",
    "            ]\n",
    "\n",
    "            trainer = Trainer(model_new, dataset_namespace_new, params['lr_fine_tune'], params, param_groups)\n",
    "            train_acc, test_acc, effective_epochs, checkpoints = trainer.train(verbose=0)\n",
    "\n",
    "            layer_lr_details = {}\n",
    "            for i, param_group in enumerate(trainer.optimizer.param_groups):\n",
    "                lr = param_group['lr']\n",
    "                for param in param_group['params']:\n",
    "                    param_id = id(param)\n",
    "                    param_name = [name for name, p in model_new.named_parameters() if id(p) == param_id][0]\n",
    "                    layer_lr_details[param_name] = lr\n",
    "\n",
    "            layer_lr_info = {\n",
    "                'reinit_lr': params['lr_fine_tune_reinit'],\n",
    "                'reinitialized_layers': reinitialized_layers,\n",
    "                'non_reinit_lr': params['lr_fine_tune_no_reinit'],\n",
    "                'non_reinitialized_layers': [name for name, _ in model_new.named_parameters() if name not in reinitialized_layers],\n",
    "                'layer_lr_details': layer_lr_details\n",
    "            }\n",
    "\n",
    "            results.append({\n",
    "                \"lr\": params['lr_fine_tune'],\n",
    "                \"sampled_percentage\": sampled_percentage,\n",
    "                \"sampled_cut_point\": sampled_cut_point,\n",
    "                \"repeat\": repeat,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"test_acc\": test_acc,\n",
    "                \"layer_lr_info\": layer_lr_info\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-11T11:44:05.468255Z",
     "iopub.status.busy": "2023-12-11T11:44:05.467350Z",
     "iopub.status.idle": "2023-12-11T11:44:05.511136Z",
     "shell.execute_reply": "2023-12-11T11:44:05.510087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results_finetune.json\n"
     ]
    }
   ],
   "source": [
    "# Save the results to a JSON file\n",
    "results_filename = 'results_finetune.json'\n",
    "with open(results_filename, 'w') as file:\n",
    "    json.dump(results, file)\n",
    "\n",
    "print(f\"Results saved to {results_filename}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

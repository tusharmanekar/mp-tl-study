{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"***IMPORTS***","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom torch.utils.data import DataLoader, random_split, Dataset","metadata":{"execution":{"iopub.status.busy":"2023-10-10T13:25:20.206426Z","iopub.execute_input":"2023-10-10T13:25:20.207187Z","iopub.status.idle":"2023-10-10T13:25:27.562865Z","shell.execute_reply.started":"2023-10-10T13:25:20.207156Z","shell.execute_reply":"2023-10-10T13:25:27.561916Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"***GENERATE 6 DIMENSIONAL DATA***","metadata":{}},{"cell_type":"code","source":"def rotation_matrix_6D(theta, dim1, dim2):\n    \"\"\"\n    Create a 6D rotation matrix to rotate by angle theta in the plane defined by dim1 and dim2.\n    \n    Parameters:\n    - theta: Rotation angle in radians.\n    - dim1, dim2: Dimensions (0-5) in which rotation should occur.\n    \n    Returns:\n    - 6x6 rotation matrix.\n    \"\"\"\n    R2 = np.array([[np.cos(theta), -np.sin(theta)],\n                   [np.sin(theta), np.cos(theta)]])\n    \n    R6 = np.eye(6) # create identity matrix of 6*6\n    R6[dim1, dim1], R6[dim1, dim2], R6[dim2, dim1], R6[dim2, dim2] = R2.flatten()\n    \n    return R6","metadata":{"execution":{"iopub.status.busy":"2023-10-10T13:25:27.564732Z","iopub.execute_input":"2023-10-10T13:25:27.565171Z","iopub.status.idle":"2023-10-10T13:25:27.571253Z","shell.execute_reply.started":"2023-10-10T13:25:27.565138Z","shell.execute_reply":"2023-10-10T13:25:27.570411Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def generate_data_6D(n_samples_per_class=100, n_classes=5):\n    \"\"\"\n    Generate data for n_classes in a 6D space.\n\n    Parameters:\n    - n_samples_per_class: Number of samples per class.\n    - n_classes: Number of classes.\n\n    Returns:\n    - X: Data points.\n    - y: Class targets.\n    \"\"\"\n    \n    # Define means for the classes in 6D space\n    means = [tuple(i*2 for _ in range(6)) for i in range(n_classes)]\n\n    # Define a covariance matrix for 6D\n    # Base covariance defines how spread out the data is in that dimension\n    base_covariance = np.array([[0.2, 0, 0, 0, 0, 0], \n                                [0, 5, 0, 0, 0, 0],\n                                [0, 0, 0.2, 0, 0, 0],\n                                [0, 0, 0, 5, 0, 0],\n                                [0, 0, 0, 0, 0.2, 0],\n                                [0, 0, 0, 0, 0, 5]])\n    \n    theta = np.pi / 4  # or any other desired angle\n    # R = rotation_matrix_6D(theta, 0, 1)  # for rotation in the plane of the first two dimensions\n    \n    R1 = rotation_matrix_6D(theta, 0, 1)\n    R2 = rotation_matrix_6D(theta, 2, 3)\n    R3 = rotation_matrix_6D(theta, 4, 5)\n    covariance = R1 @ R2 @ R3 @ base_covariance @ R3.T @ R2.T @ R1.T\n    \n    X = np.empty((0, 6))\n    y = np.empty((0,))\n\n    for idx, mean in enumerate(means):\n        class_data = np.random.multivariate_normal(mean, covariance, n_samples_per_class)\n        X = np.vstack([X, class_data])\n        y = np.hstack([y, [idx]*n_samples_per_class])\n\n    return X, y","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-10T13:25:27.572736Z","iopub.execute_input":"2023-10-10T13:25:27.573374Z","iopub.status.idle":"2023-10-10T13:25:27.584767Z","shell.execute_reply.started":"2023-10-10T13:25:27.573340Z","shell.execute_reply":"2023-10-10T13:25:27.583942Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Generate and plot the data\nn_classes = 5\nX, y = generate_data_6D(n_samples_per_class = 400, n_classes=n_classes)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T13:25:27.587086Z","iopub.execute_input":"2023-10-10T13:25:27.588021Z","iopub.status.idle":"2023-10-10T13:25:27.624687Z","shell.execute_reply.started":"2023-10-10T13:25:27.587984Z","shell.execute_reply":"2023-10-10T13:25:27.623187Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]","metadata":{"execution":{"iopub.status.busy":"2023-10-10T13:25:27.625911Z","iopub.execute_input":"2023-10-10T13:25:27.626746Z","iopub.status.idle":"2023-10-10T13:25:27.632119Z","shell.execute_reply.started":"2023-10-10T13:25:27.626715Z","shell.execute_reply":"2023-10-10T13:25:27.631157Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Split the data into primary (even classes) and transfer (odd classes) datasets\nX_primary = X[np.isin(y, [0, 2, 4])]\ny_primary = y[np.isin(y, [0, 2, 4])]\n\n\n# Assuming X_primary and y_primary are numpy arrays\nX_primary = torch.tensor(X_primary, dtype=torch.float32)\ny_primary = torch.tensor(y_primary, dtype=torch.int64)\n\nprimary_dataset = CustomDataset(X_primary, y_primary)\n\nX_transfer = X[np.isin(y, [1, 3])]\ny_transfer = y[np.isin(y, [1, 3])]\n\ntransfer_dataset = CustomDataset(X_transfer, y_transfer)\n\n# Assuming X_primary and y_primary are numpy arrays\nX_transfer = torch.tensor(X_transfer, dtype=torch.float32)\ny_transfer = torch.tensor(y_transfer, dtype=torch.int64)\n\n\n# Splitting ratios\ntrain_ratio = 0.7\nval_ratio = 0.15\ntest_ratio = 0.15\n\n# Calculate sizes for primary dataset split\nprimary_train_size = int(train_ratio * len(primary_dataset))\nprimary_val_size = int(val_ratio * len(primary_dataset))\nprimary_test_size = len(primary_dataset) - primary_train_size - primary_val_size\n\n# Split the primary dataset into training, validation, and test sets\nprimary_train_dataset, primary_val_dataset, primary_test_dataset = random_split(primary_dataset, [primary_train_size, primary_val_size, primary_test_size])\n\n# Calculate sizes for transfer dataset split\ntransfer_train_size = int(train_ratio * len(transfer_dataset))\ntransfer_val_size = int(val_ratio * len(transfer_dataset))\ntransfer_test_size = len(transfer_dataset) - transfer_train_size - transfer_val_size\n\n# Split the transfer dataset into training, validation, and test sets\ntransfer_train_dataset, transfer_val_dataset, transfer_test_dataset = random_split(transfer_dataset, [transfer_train_size, transfer_val_size, transfer_test_size])\n\n# Create DataLoader objects\nbatch_size = 32\n\nprimary_train_loader = DataLoader(primary_train_dataset, batch_size=batch_size, shuffle=True)\nprimary_val_loader = DataLoader(primary_val_dataset, batch_size=batch_size, shuffle=False)\nprimary_test_loader = DataLoader(primary_test_dataset, batch_size=batch_size, shuffle=False)\n\ntransfer_train_loader = DataLoader(transfer_train_dataset, batch_size=batch_size, shuffle=True)\ntransfer_val_loader = DataLoader(transfer_val_dataset, batch_size=batch_size, shuffle=False)\ntransfer_test_loader = DataLoader(transfer_test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T13:25:27.633718Z","iopub.execute_input":"2023-10-10T13:25:27.634415Z","iopub.status.idle":"2023-10-10T13:25:27.716932Z","shell.execute_reply.started":"2023-10-10T13:25:27.634380Z","shell.execute_reply":"2023-10-10T13:25:27.716108Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Define the network architecture\nclass DenseNet(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout_prob = 0.15):\n        super(DenseNet, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.fc6 = nn.Linear(hidden_dim, output_dim)\n        \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = F.relu(self.fc3(x))\n        x = self.dropout(x)\n        x = F.relu(self.fc4(x))\n        x = self.dropout(x)\n        x = F.relu(self.fc5(x))\n        x = self.dropout(x)\n        x = self.fc6(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-10-10T13:25:27.718195Z","iopub.execute_input":"2023-10-10T13:25:27.718771Z","iopub.status.idle":"2023-10-10T13:25:27.726899Z","shell.execute_reply.started":"2023-10-10T13:25:27.718742Z","shell.execute_reply":"2023-10-10T13:25:27.726039Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters\ninput_dim = 6\nhidden_dim = 128\noutput_dim = 5\noutput_dim_trnsfr = 5","metadata":{"execution":{"iopub.status.busy":"2023-10-10T13:25:27.728422Z","iopub.execute_input":"2023-10-10T13:25:27.729005Z","iopub.status.idle":"2023-10-10T13:25:27.740376Z","shell.execute_reply.started":"2023-10-10T13:25:27.728975Z","shell.execute_reply":"2023-10-10T13:25:27.739581Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Initialize the network and optimizer\nmodel = DenseNet(input_dim, hidden_dim, output_dim)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2023-10-10T13:25:27.741470Z","iopub.execute_input":"2023-10-10T13:25:27.742414Z","iopub.status.idle":"2023-10-10T13:25:32.786377Z","shell.execute_reply.started":"2023-10-10T13:25:27.742385Z","shell.execute_reply":"2023-10-10T13:25:32.785430Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Training loop\nepochs = 50\npatience = 5  # Number of epochs to wait before stopping\nwait = 0\nbest_loss = float('inf')\n\nfor epoch in range(epochs):\n    # Training phase\n    model.train()\n    train_loss = 0.0\n    for data, target in primary_train_loader:\n        optimizer.zero_grad()\n        output = model(data.to(device))\n        loss = criterion(output.to(device), target.to(device))\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    \n    train_loss /= len(primary_train_loader)\n    \n    # Validation phase\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for data, target in primary_val_loader:\n            output = model(data.to(device))\n            loss = criterion(output.to(device), target.to(device))\n            val_loss += loss.item()\n    \n    val_loss /= len(primary_val_loader)\n    print(f\"Epoch {epoch+1}/{epochs} - Training Loss: {train_loss}, Validation Loss: {val_loss}\")\n    \n    # Check early stopping condition\n    if val_loss < best_loss:\n        best_loss = val_loss\n        wait = 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping!\")\n            break","metadata":{"execution":{"iopub.status.busy":"2023-10-10T13:25:32.792681Z","iopub.execute_input":"2023-10-10T13:25:32.795318Z","iopub.status.idle":"2023-10-10T13:25:38.850012Z","shell.execute_reply.started":"2023-10-10T13:25:32.795276Z","shell.execute_reply":"2023-10-10T13:25:38.849103Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Epoch 1/50 - Training Loss: 1.0212677739284657, Validation Loss: 0.4999668796857198\nEpoch 2/50 - Training Loss: 0.462572470859245, Validation Loss: 0.30311032632986706\nEpoch 3/50 - Training Loss: 0.3233858992656072, Validation Loss: 0.16625078146656355\nEpoch 4/50 - Training Loss: 0.23936856344894128, Validation Loss: 0.10227527096867561\nEpoch 5/50 - Training Loss: 0.1772726975657322, Validation Loss: 0.08754480071365833\nEpoch 6/50 - Training Loss: 0.13768492031980445, Validation Loss: 0.08644431084394455\nEpoch 7/50 - Training Loss: 0.1399601939375754, Validation Loss: 0.07298677352567513\nEpoch 8/50 - Training Loss: 0.08649039068431766, Validation Loss: 0.030614261360218126\nEpoch 9/50 - Training Loss: 0.0679895911814162, Validation Loss: 0.029137626057490706\nEpoch 10/50 - Training Loss: 0.057420522802405886, Validation Loss: 0.015215820089603463\nEpoch 11/50 - Training Loss: 0.03354192271621691, Validation Loss: 0.011264036254336437\nEpoch 12/50 - Training Loss: 0.07394281973096507, Validation Loss: 0.006980613960574071\nEpoch 13/50 - Training Loss: 0.02685603172156132, Validation Loss: 0.0015234362023572128\nEpoch 14/50 - Training Loss: 0.013859989849152043, Validation Loss: 0.029673416703493178\nEpoch 15/50 - Training Loss: 0.033917254837298835, Validation Loss: 0.0023286915966309607\nEpoch 16/50 - Training Loss: 0.0152167539097179, Validation Loss: 0.001133252711345752\nEpoch 17/50 - Training Loss: 0.01438638990253417, Validation Loss: 0.0006970269023440778\nEpoch 18/50 - Training Loss: 0.013932535195231644, Validation Loss: 0.0005920627299929038\nEpoch 19/50 - Training Loss: 0.039680664371526626, Validation Loss: 0.03250757425363796\nEpoch 20/50 - Training Loss: 0.013134050120478842, Validation Loss: 0.0008415843864592413\nEpoch 21/50 - Training Loss: 0.005314621279938836, Validation Loss: 0.0004187813926061305\nEpoch 22/50 - Training Loss: 0.011967970733099652, Validation Loss: 0.0005882526262818525\nEpoch 23/50 - Training Loss: 0.015500899723351554, Validation Loss: 0.0034818890465733907\nEpoch 24/50 - Training Loss: 0.007226309114937774, Validation Loss: 0.00031314896477852017\nEpoch 25/50 - Training Loss: 0.00974884933222913, Validation Loss: 0.00037224031014678377\nEpoch 26/50 - Training Loss: 0.006147121862913655, Validation Loss: 0.00581386886551627\nEpoch 27/50 - Training Loss: 0.0033801572518078267, Validation Loss: 0.0007218814995818926\nEpoch 28/50 - Training Loss: 0.0035833226181609832, Validation Loss: 0.00022104118033894338\nEpoch 29/50 - Training Loss: 0.009785859196132084, Validation Loss: 0.00014939023033851603\nEpoch 30/50 - Training Loss: 0.022031752512298733, Validation Loss: 0.0006438907051536565\nEpoch 31/50 - Training Loss: 0.02749006364805003, Validation Loss: 0.005899255144565056\nEpoch 32/50 - Training Loss: 0.00900025794248062, Validation Loss: 0.0005545357174317663\nEpoch 33/50 - Training Loss: 0.014351825096161553, Validation Loss: 0.0007069104467518628\nEpoch 34/50 - Training Loss: 0.02016133851968873, Validation Loss: 0.0006867471238365397\nEarly stopping!\n","output_type":"stream"}]},{"cell_type":"code","source":"class TransferModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim_transfer, dropout_prob=0.15):\n        super(TransferModel, self).__init__()\n        # Copy the layers from the pretrained model\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n        \n        # Add the new final layer\n        self.fc_final = nn.Linear(hidden_dim, output_dim_transfer)\n        \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = F.relu(self.fc4(x))\n        x = F.relu(self.fc5(x))\n        x = self.fc_final(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-10-10T13:25:38.851029Z","iopub.execute_input":"2023-10-10T13:25:38.851756Z","iopub.status.idle":"2023-10-10T13:25:38.858300Z","shell.execute_reply.started":"2023-10-10T13:25:38.851732Z","shell.execute_reply":"2023-10-10T13:25:38.857305Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Assuming pretrained_model is an instance of DenseNet that has been trained\npretrained_model = DenseNet(input_dim, hidden_dim, output_dim)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T13:25:38.859491Z","iopub.execute_input":"2023-10-10T13:25:38.860345Z","iopub.status.idle":"2023-10-10T13:25:38.880448Z","shell.execute_reply.started":"2023-10-10T13:25:38.860294Z","shell.execute_reply":"2023-10-10T13:25:38.879404Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Create the transfer model\ntransfer_model = TransferModel(input_dim, hidden_dim, output_dim_trnsfr)\ntransfer_model.to(device)\n\n# Copy weights from the pretrained model\nwith torch.no_grad():\n    transfer_model.fc1.weight.copy_(pretrained_model.fc1.weight)\n    transfer_model.fc1.bias.copy_(pretrained_model.fc1.bias)\n    transfer_model.fc2.weight.copy_(pretrained_model.fc2.weight)\n    transfer_model.fc2.bias.copy_(pretrained_model.fc2.bias)\n    transfer_model.fc3.weight.copy_(pretrained_model.fc3.weight)\n    transfer_model.fc3.bias.copy_(pretrained_model.fc3.bias)\n    transfer_model.fc4.weight.copy_(pretrained_model.fc4.weight)\n    transfer_model.fc4.bias.copy_(pretrained_model.fc4.bias)\n    transfer_model.fc5.weight.copy_(pretrained_model.fc5.weight)\n    transfer_model.fc5.bias.copy_(pretrained_model.fc5.bias)\n\n# The final layer (fc_final) will be randomly initialized by default","metadata":{"execution":{"iopub.status.busy":"2023-10-10T13:25:38.881778Z","iopub.execute_input":"2023-10-10T13:25:38.882387Z","iopub.status.idle":"2023-10-10T13:25:38.899980Z","shell.execute_reply.started":"2023-10-10T13:25:38.882329Z","shell.execute_reply":"2023-10-10T13:25:38.899135Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Initialize the network and optimize\n\noptimizer = optim.Adam(transfer_model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2023-10-10T13:25:38.901150Z","iopub.execute_input":"2023-10-10T13:25:38.902072Z","iopub.status.idle":"2023-10-10T13:25:38.910060Z","shell.execute_reply.started":"2023-10-10T13:25:38.902043Z","shell.execute_reply":"2023-10-10T13:25:38.909252Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Training loop\nepochs = 200\npatience = 5  # Number of epochs to wait before stopping\nwait = 0\nbest_loss = float('inf')\n\nfor epoch in range(epochs):\n    # Training phase\n    transfer_model.train()\n    train_loss = 0.0\n    for data, target in transfer_train_loader:\n        optimizer.zero_grad()\n        output = transfer_model(data.float().to(device))\n        loss = criterion(output.float().to(device), target.long().to(device))\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    \n    train_loss /= len(transfer_train_loader)\n    \n    # Validation phase\n    transfer_model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for data, target in transfer_val_loader:\n            output = transfer_model(data.float().to(device))\n            loss = criterion(output.float().to(device), target.long().to(device))\n            val_loss += loss.item()\n    \n    val_loss /= len(transfer_val_loader)\n    print(f\"Epoch {epoch+1}/{epochs} - Training Loss: {train_loss}, Validation Loss: {val_loss}\")\n    \n    # Check early stopping condition\n    if val_loss < best_loss:\n        best_loss = val_loss\n        wait = 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(\"Early stopping!\")\n            break","metadata":{"execution":{"iopub.status.busy":"2023-10-10T13:25:38.911319Z","iopub.execute_input":"2023-10-10T13:25:38.912159Z","iopub.status.idle":"2023-10-10T13:25:46.866965Z","shell.execute_reply.started":"2023-10-10T13:25:38.912131Z","shell.execute_reply":"2023-10-10T13:25:46.865889Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch 1/200 - Training Loss: 1.0878548986381955, Validation Loss: 0.6431633681058884\nEpoch 2/200 - Training Loss: 0.5192563136418661, Validation Loss: 0.3546052426099777\nEpoch 3/200 - Training Loss: 0.21002500131726265, Validation Loss: 0.09400339983403683\nEpoch 4/200 - Training Loss: 0.07645487904341684, Validation Loss: 0.06693738931789994\nEpoch 5/200 - Training Loss: 0.08184411134829538, Validation Loss: 0.022570077562704682\nEpoch 6/200 - Training Loss: 0.024185269573030785, Validation Loss: 0.009710656013339758\nEpoch 7/200 - Training Loss: 0.006573067850291004, Validation Loss: 0.002504124218830839\nEpoch 8/200 - Training Loss: 0.006757996282734287, Validation Loss: 0.0013563911925302818\nEpoch 9/200 - Training Loss: 0.004063306699713899, Validation Loss: 0.0010105907203978859\nEpoch 10/200 - Training Loss: 0.0011167156654765778, Validation Loss: 0.0007625091056979727\nEpoch 11/200 - Training Loss: 0.0006338393072332514, Validation Loss: 0.00029867679586459417\nEpoch 12/200 - Training Loss: 0.00038946006902228366, Validation Loss: 0.0002510858412279049\nEpoch 13/200 - Training Loss: 0.00030401247285125364, Validation Loss: 0.00020241661150066648\nEpoch 14/200 - Training Loss: 0.00024387343506128268, Validation Loss: 0.00015644477934984025\nEpoch 15/200 - Training Loss: 0.00021946131124018898, Validation Loss: 0.00015092430203367257\nEpoch 16/200 - Training Loss: 0.0001866871610521533, Validation Loss: 0.00012830043760914123\nEpoch 17/200 - Training Loss: 0.00016341244413019417, Validation Loss: 0.00010005745662056142\nEpoch 18/200 - Training Loss: 0.00014158092709395633, Validation Loss: 0.00010208065032202285\nEpoch 19/200 - Training Loss: 0.00012250010341732478, Validation Loss: 8.249185430031503e-05\nEpoch 20/200 - Training Loss: 0.00010967493830725693, Validation Loss: 6.883633386678412e-05\nEpoch 21/200 - Training Loss: 9.774651226103338e-05, Validation Loss: 6.420082627300872e-05\nEpoch 22/200 - Training Loss: 8.846736909617903e-05, Validation Loss: 6.304396902123699e-05\nEpoch 23/200 - Training Loss: 7.999114960613143e-05, Validation Loss: 5.426219286164269e-05\nEpoch 24/200 - Training Loss: 7.663809136223183e-05, Validation Loss: 4.716210582955682e-05\nEpoch 25/200 - Training Loss: 6.88215088505078e-05, Validation Loss: 4.3264164560241625e-05\nEpoch 26/200 - Training Loss: 6.20456770421798e-05, Validation Loss: 4.3350283021936775e-05\nEpoch 27/200 - Training Loss: 5.764978691836909e-05, Validation Loss: 3.8221725617404445e-05\nEpoch 28/200 - Training Loss: 5.327464873516874e-05, Validation Loss: 3.2569994573350414e-05\nEpoch 29/200 - Training Loss: 4.96494032202261e-05, Validation Loss: 3.161584936606232e-05\nEpoch 30/200 - Training Loss: 4.6951307025564245e-05, Validation Loss: 2.981916600219847e-05\nEpoch 31/200 - Training Loss: 4.193949580945223e-05, Validation Loss: 2.6530348236519785e-05\nEpoch 32/200 - Training Loss: 4.101024465954753e-05, Validation Loss: 2.4588674818915024e-05\nEpoch 33/200 - Training Loss: 3.553976843148929e-05, Validation Loss: 2.5691218866086274e-05\nEpoch 34/200 - Training Loss: 3.540565563121668e-05, Validation Loss: 2.193277725837106e-05\nEpoch 35/200 - Training Loss: 3.1274593639965235e-05, Validation Loss: 2.1451942757266806e-05\nEpoch 36/200 - Training Loss: 3.4464657523535104e-05, Validation Loss: 1.915301220378751e-05\nEpoch 37/200 - Training Loss: 2.7629437504906997e-05, Validation Loss: 1.71892164644305e-05\nEpoch 38/200 - Training Loss: 2.5564593487261947e-05, Validation Loss: 1.6415713162132306e-05\nEpoch 39/200 - Training Loss: 2.4571504468440253e-05, Validation Loss: 1.547987119465688e-05\nEpoch 40/200 - Training Loss: 2.3026458090195472e-05, Validation Loss: 1.572720412923445e-05\nEpoch 41/200 - Training Loss: 2.2776450536967888e-05, Validation Loss: 1.4385388624305051e-05\nEpoch 42/200 - Training Loss: 2.062049586331543e-05, Validation Loss: 1.297464905292145e-05\nEpoch 43/200 - Training Loss: 1.9714460203582956e-05, Validation Loss: 1.2672924128764862e-05\nEpoch 44/200 - Training Loss: 1.9090494599064794e-05, Validation Loss: 1.162191182402239e-05\nEpoch 45/200 - Training Loss: 1.7774920662658486e-05, Validation Loss: 1.1251896864905575e-05\nEpoch 46/200 - Training Loss: 1.7726708209566925e-05, Validation Loss: 1.082104972738307e-05\nEpoch 47/200 - Training Loss: 1.717285248206382e-05, Validation Loss: 1.0156158282370598e-05\nEpoch 48/200 - Training Loss: 1.5212935484972453e-05, Validation Loss: 1.03681429095559e-05\nEpoch 49/200 - Training Loss: 1.5145133261285082e-05, Validation Loss: 9.370193197355547e-06\nEpoch 50/200 - Training Loss: 1.3832272827585257e-05, Validation Loss: 9.10820108401822e-06\nEpoch 51/200 - Training Loss: 1.3633370612195802e-05, Validation Loss: 8.560016112824087e-06\nEpoch 52/200 - Training Loss: 1.3068794039049357e-05, Validation Loss: 7.949745906898897e-06\nEpoch 53/200 - Training Loss: 1.2091524038421792e-05, Validation Loss: 8.039753879529599e-06\nEpoch 54/200 - Training Loss: 1.1618334522406965e-05, Validation Loss: 7.436933941562529e-06\nEpoch 55/200 - Training Loss: 1.1062229071335929e-05, Validation Loss: 7.1330354955989606e-06\nEpoch 56/200 - Training Loss: 1.062236866763063e-05, Validation Loss: 6.8278941682820005e-06\nEpoch 57/200 - Training Loss: 1.0364207270185943e-05, Validation Loss: 6.475568994801506e-06\nEpoch 58/200 - Training Loss: 9.908956201343244e-06, Validation Loss: 6.369712735931898e-06\nEpoch 59/200 - Training Loss: 9.470535840389655e-06, Validation Loss: 5.9869668120882125e-06\nEpoch 60/200 - Training Loss: 9.698799080600414e-06, Validation Loss: 5.808162796938632e-06\nEpoch 61/200 - Training Loss: 8.720283940672542e-06, Validation Loss: 5.425415139370671e-06\nEpoch 62/200 - Training Loss: 8.471406001970334e-06, Validation Loss: 5.290069111651974e-06\nEpoch 63/200 - Training Loss: 8.22849606960416e-06, Validation Loss: 5.013792616637147e-06\nEpoch 64/200 - Training Loss: 7.856838281996817e-06, Validation Loss: 5.065008537030735e-06\nEpoch 65/200 - Training Loss: 7.900737131194546e-06, Validation Loss: 4.8020806104887015e-06\nEpoch 66/200 - Training Loss: 7.452333190056177e-06, Validation Loss: 4.492278293355412e-06\nEpoch 67/200 - Training Loss: 7.171972678558329e-06, Validation Loss: 4.410013417555092e-06\nEpoch 68/200 - Training Loss: 6.815806977379503e-06, Validation Loss: 4.331474627861098e-06\nEpoch 69/200 - Training Loss: 6.51700538077825e-06, Validation Loss: 3.994664325546182e-06\nEpoch 70/200 - Training Loss: 6.30741598683926e-06, Validation Loss: 3.939407122288685e-06\nEpoch 71/200 - Training Loss: 6.299572349790752e-06, Validation Loss: 3.831998697023664e-06\nEpoch 72/200 - Training Loss: 6.252990841757209e-06, Validation Loss: 3.726142651316877e-06\nEpoch 73/200 - Training Loss: 5.672969045633636e-06, Validation Loss: 3.5613054478744743e-06\nEpoch 74/200 - Training Loss: 5.539506007481678e-06, Validation Loss: 3.3688403249243493e-06\nEpoch 75/200 - Training Loss: 5.350168442595936e-06, Validation Loss: 3.321654290289189e-06\nEpoch 76/200 - Training Loss: 5.141986880137968e-06, Validation Loss: 3.2083475929312044e-06\nEpoch 77/200 - Training Loss: 5.039336667462319e-06, Validation Loss: 3.158678126169434e-06\nEpoch 78/200 - Training Loss: 4.822483729090739e-06, Validation Loss: 2.990115191892073e-06\nEpoch 79/200 - Training Loss: 4.686113307962438e-06, Validation Loss: 2.950690074499107e-06\nEpoch 80/200 - Training Loss: 4.899050133467932e-06, Validation Loss: 2.8951225203854847e-06\nEpoch 81/200 - Training Loss: 4.465307582456211e-06, Validation Loss: 2.7172463745728237e-06\nEpoch 82/200 - Training Loss: 4.301009871217805e-06, Validation Loss: 2.623806480528401e-06\nEpoch 83/200 - Training Loss: 4.1803655395571896e-06, Validation Loss: 2.568859898133269e-06\nEpoch 84/200 - Training Loss: 4.075439153755016e-06, Validation Loss: 2.566686134741758e-06\nEpoch 85/200 - Training Loss: 4.222593774885455e-06, Validation Loss: 2.435373929188245e-06\nEpoch 86/200 - Training Loss: 3.8351869228058095e-06, Validation Loss: 2.3968801201590395e-06\nEpoch 87/200 - Training Loss: 3.845540059425427e-06, Validation Loss: 2.3751493216650488e-06\nEpoch 88/200 - Training Loss: 3.64604355714467e-06, Validation Loss: 2.2727069222128193e-06\nEpoch 89/200 - Training Loss: 3.6023838624209425e-06, Validation Loss: 2.2168290172430716e-06\nEpoch 90/200 - Training Loss: 3.690552015693053e-06, Validation Loss: 2.097623195140841e-06\nEpoch 91/200 - Training Loss: 3.5069919653728398e-06, Validation Loss: 2.117490026876112e-06\nEpoch 92/200 - Training Loss: 3.5123718033800187e-06, Validation Loss: 2.0315002942083993e-06\nEpoch 93/200 - Training Loss: 3.163042484920374e-06, Validation Loss: 1.9874185355206464e-06\nEpoch 94/200 - Training Loss: 3.099305243065626e-06, Validation Loss: 1.9380596256723948e-06\nEpoch 95/200 - Training Loss: 3.1022080146196154e-06, Validation Loss: 1.887458957128274e-06\nEpoch 96/200 - Training Loss: 3.0610277548248632e-06, Validation Loss: 1.8290972860768306e-06\nEpoch 97/200 - Training Loss: 2.9610735212928476e-06, Validation Loss: 1.790293005399235e-06\nEpoch 98/200 - Training Loss: 2.7903359457089514e-06, Validation Loss: 1.7688727496079082e-06\nEpoch 99/200 - Training Loss: 2.72101030576424e-06, Validation Loss: 1.6968521023841276e-06\nEpoch 100/200 - Training Loss: 2.7427472228206775e-06, Validation Loss: 1.642836586768226e-06\nEpoch 101/200 - Training Loss: 2.589809900952231e-06, Validation Loss: 1.5897521663532643e-06\nEpoch 102/200 - Training Loss: 2.520275300652328e-06, Validation Loss: 1.5674007372012966e-06\nEpoch 103/200 - Training Loss: 2.6471359820195376e-06, Validation Loss: 1.546912038463688e-06\nEpoch 104/200 - Training Loss: 2.432323147634937e-06, Validation Loss: 1.4680617397289097e-06\nEpoch 105/200 - Training Loss: 2.4641979455471605e-06, Validation Loss: 1.450987603845988e-06\nEpoch 106/200 - Training Loss: 2.634705023436052e-06, Validation Loss: 1.4590586729923416e-06\nEpoch 107/200 - Training Loss: 2.240690333918691e-06, Validation Loss: 1.3687222164548984e-06\nEpoch 108/200 - Training Loss: 2.192679616936428e-06, Validation Loss: 1.3311594742049238e-06\nEpoch 109/200 - Training Loss: 2.2427643836901248e-06, Validation Loss: 1.2942176113028836e-06\nEpoch 110/200 - Training Loss: 2.080098475687464e-06, Validation Loss: 1.3081869241204913e-06\nEpoch 111/200 - Training Loss: 2.1138329905233554e-06, Validation Loss: 1.2923546250931395e-06\nEpoch 112/200 - Training Loss: 2.0171858542047025e-06, Validation Loss: 1.2383388678927076e-06\nEpoch 113/200 - Training Loss: 1.9412351494175873e-06, Validation Loss: 1.2190917786369937e-06\nEpoch 114/200 - Training Loss: 1.923230978522699e-06, Validation Loss: 1.192704715435866e-06\nEpoch 115/200 - Training Loss: 1.868388885976439e-06, Validation Loss: 1.160108784148406e-06\nEpoch 116/200 - Training Loss: 1.9279928596836903e-06, Validation Loss: 1.14551827934406e-06\nEpoch 117/200 - Training Loss: 1.7893334446720626e-06, Validation Loss: 1.1126119865423334e-06\nEpoch 118/200 - Training Loss: 1.8267940477853826e-06, Validation Loss: 1.0880874583563127e-06\nEpoch 119/200 - Training Loss: 1.7498081000945199e-06, Validation Loss: 1.0672882773121728e-06\nEpoch 120/200 - Training Loss: 1.6879284048501277e-06, Validation Loss: 1.065425575319523e-06\nEpoch 121/200 - Training Loss: 1.6960005879695927e-06, Validation Loss: 1.0213436283379451e-06\nEpoch 122/200 - Training Loss: 1.649230065304942e-06, Validation Loss: 1.0135826151724814e-06\nEpoch 123/200 - Training Loss: 1.5838319124365323e-06, Validation Loss: 9.96198075142729e-07\nEpoch 124/200 - Training Loss: 1.6465402686726015e-06, Validation Loss: 9.552204431884093e-07\nEpoch 125/200 - Training Loss: 1.5548604434595998e-06, Validation Loss: 9.567725847148267e-07\nEpoch 126/200 - Training Loss: 1.5103645169113456e-06, Validation Loss: 9.452864162540209e-07\nEpoch 127/200 - Training Loss: 1.4495196902528405e-06, Validation Loss: 9.170366261912477e-07\nEpoch 128/200 - Training Loss: 1.4391734533979338e-06, Validation Loss: 8.850615778044357e-07\nEpoch 129/200 - Training Loss: 1.5078823254713016e-06, Validation Loss: 8.692292929879386e-07\nEpoch 130/200 - Training Loss: 1.3741898076741816e-06, Validation Loss: 8.636414285945193e-07\nEpoch 131/200 - Training Loss: 1.4106152911431814e-06, Validation Loss: 8.208011159638318e-07\nEpoch 132/200 - Training Loss: 1.3222455851212721e-06, Validation Loss: 8.149027941328768e-07\nEpoch 133/200 - Training Loss: 1.3330075094112697e-06, Validation Loss: 8.133505353669079e-07\nEpoch 134/200 - Training Loss: 1.3388031183423109e-06, Validation Loss: 7.89446861659826e-07\nEpoch 135/200 - Training Loss: 1.2746467221960402e-06, Validation Loss: 7.720623038665053e-07\nEpoch 136/200 - Training Loss: 1.2717501801281135e-06, Validation Loss: 7.562299906282988e-07\nEpoch 137/200 - Training Loss: 1.2806482077268001e-06, Validation Loss: 7.403976560738101e-07\nEpoch 138/200 - Training Loss: 1.2357404963994313e-06, Validation Loss: 7.133895927324829e-07\nEpoch 139/200 - Training Loss: 1.1479906715609427e-06, Validation Loss: 7.093538876290495e-07\nEpoch 140/200 - Training Loss: 1.1231555391412308e-06, Validation Loss: 7.121477700877676e-07\nEpoch 141/200 - Training Loss: 1.1111521822155307e-06, Validation Loss: 7.022137200607403e-07\nEpoch 142/200 - Training Loss: 1.0970796001035726e-06, Validation Loss: 6.702386965429241e-07\nEpoch 143/200 - Training Loss: 1.0683131675095738e-06, Validation Loss: 6.683760513936932e-07\nEpoch 144/200 - Training Loss: 1.0598284533437486e-06, Validation Loss: 6.52543745260914e-07\nEpoch 145/200 - Training Loss: 1.0503083487378717e-06, Validation Loss: 6.450931717694175e-07\nEpoch 146/200 - Training Loss: 1.0122284687478593e-06, Validation Loss: 6.236730367703558e-07\nEpoch 147/200 - Training Loss: 9.869802658120836e-07, Validation Loss: 6.056675587728932e-07\nEpoch 148/200 - Training Loss: 9.838763403990924e-07, Validation Loss: 6.038049136236623e-07\nEpoch 149/200 - Training Loss: 1.010574156307737e-06, Validation Loss: 5.879725790691737e-07\nEpoch 150/200 - Training Loss: 1.0085048058883217e-06, Validation Loss: 5.786594101664377e-07\nEpoch 151/200 - Training Loss: 9.366901001012845e-07, Validation Loss: 5.904559809977172e-07\nEpoch 152/200 - Training Loss: 9.068881939732087e-07, Validation Loss: 5.764862986978869e-07\nEpoch 153/200 - Training Loss: 9.391739439479352e-07, Validation Loss: 5.569286738449364e-07\nEpoch 154/200 - Training Loss: 8.762593542617955e-07, Validation Loss: 5.38923249138179e-07\nEpoch 155/200 - Training Loss: 8.584613255319734e-07, Validation Loss: 5.237117779444134e-07\nEpoch 156/200 - Training Loss: 8.483206875729998e-07, Validation Loss: 5.153299493798613e-07\nEpoch 157/200 - Training Loss: 8.526667575460401e-07, Validation Loss: 5.190551934930454e-07\nEpoch 158/200 - Training Loss: 8.129307277214714e-07, Validation Loss: 5.088107482009718e-07\nEpoch 159/200 - Training Loss: 8.646710373878047e-07, Validation Loss: 4.892531055844529e-07\nEpoch 160/200 - Training Loss: 7.820944800427141e-07, Validation Loss: 4.92978339039496e-07\nEpoch 161/200 - Training Loss: 7.738162093318124e-07, Validation Loss: 4.790086176598152e-07\nEpoch 162/200 - Training Loss: 7.616059044658464e-07, Validation Loss: 4.805607805025147e-07\nEpoch 163/200 - Training Loss: 7.705053503867829e-07, Validation Loss: 4.7031631744687274e-07\nEpoch 164/200 - Training Loss: 7.551906483863983e-07, Validation Loss: 4.541734810459275e-07\nEpoch 165/200 - Training Loss: 7.375993544340215e-07, Validation Loss: 4.4020373124453727e-07\nEpoch 166/200 - Training Loss: 7.876832432726941e-07, Validation Loss: 4.389619672195977e-07\nEpoch 167/200 - Training Loss: 6.947591280553272e-07, Validation Loss: 4.2964881608043015e-07\nEpoch 168/200 - Training Loss: 6.945524108440774e-07, Validation Loss: 4.2219827811607047e-07\nEpoch 169/200 - Training Loss: 6.719939595396024e-07, Validation Loss: 4.1257465888122624e-07\nEpoch 170/200 - Training Loss: 7.011751558631355e-07, Validation Loss: 4.113328824217888e-07\nEpoch 171/200 - Training Loss: 6.469521821964955e-07, Validation Loss: 4.038823444574291e-07\nEpoch 172/200 - Training Loss: 6.374321809153748e-07, Validation Loss: 3.964318100457831e-07\nEpoch 173/200 - Training Loss: 6.289470054888902e-07, Validation Loss: 3.871186553539019e-07\nEpoch 174/200 - Training Loss: 6.426064990621643e-07, Validation Loss: 3.8525602441552564e-07\nEpoch 175/200 - Training Loss: 6.624741801057136e-07, Validation Loss: 3.824620566916792e-07\nEpoch 176/200 - Training Loss: 7.535350046655114e-07, Validation Loss: 3.709758011893882e-07\nEpoch 177/200 - Training Loss: 6.078378089253217e-07, Validation Loss: 3.545225535361851e-07\nEpoch 178/200 - Training Loss: 5.832097167670883e-07, Validation Loss: 3.5203901216362965e-07\nEpoch 179/200 - Training Loss: 5.749314652013659e-07, Validation Loss: 3.4645113355935564e-07\nEpoch 180/200 - Training Loss: 5.699645770660153e-07, Validation Loss: 3.427258610244621e-07\nEpoch 181/200 - Training Loss: 5.498895229531323e-07, Validation Loss: 3.3434397206377753e-07\nEpoch 182/200 - Training Loss: 5.784498935154955e-07, Validation Loss: 3.31239562889607e-07\nEpoch 183/200 - Training Loss: 5.329190108464369e-07, Validation Loss: 3.247203474998628e-07\nEpoch 184/200 - Training Loss: 6.144596410943374e-07, Validation Loss: 3.247203474998628e-07\nEpoch 185/200 - Training Loss: 5.451297787494191e-07, Validation Loss: 3.1168188829866494e-07\nEpoch 186/200 - Training Loss: 5.076702825037829e-07, Validation Loss: 3.0981924670214767e-07\nEpoch 187/200 - Training Loss: 5.027033038975917e-07, Validation Loss: 3.060939848253952e-07\nEpoch 188/200 - Training Loss: 4.962876106616598e-07, Validation Loss: 3.0423135388701894e-07\nEpoch 189/200 - Training Loss: 5.020825541003483e-07, Validation Loss: 2.977121100755653e-07\nEpoch 190/200 - Training Loss: 4.789031616701955e-07, Validation Loss: 2.893302530893038e-07\nEpoch 191/200 - Training Loss: 4.697970062355555e-07, Validation Loss: 2.893302530893038e-07\nEpoch 192/200 - Training Loss: 4.7124579458410506e-07, Validation Loss: 2.8560496989626927e-07\nEpoch 193/200 - Training Loss: 4.6545099374739607e-07, Validation Loss: 2.828110234887049e-07\nEpoch 194/200 - Training Loss: 4.509638955596529e-07, Validation Loss: 2.7784399847519126e-07\nEpoch 195/200 - Training Loss: 4.6048408932877453e-07, Validation Loss: 2.731873998129686e-07\nEpoch 196/200 - Training Loss: 4.3999514530589244e-07, Validation Loss: 2.6977255096483077e-07\nEpoch 197/200 - Training Loss: 4.3088893855427716e-07, Validation Loss: 2.6418463683341997e-07\nEpoch 198/200 - Training Loss: 4.22610623138395e-07, Validation Loss: 2.62321991684189e-07\nEpoch 199/200 - Training Loss: 4.1929931359084725e-07, Validation Loss: 2.554923161923739e-07\nEpoch 200/200 - Training Loss: 4.743497138785339e-07, Validation Loss: 2.5456099539411525e-07\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test loop\ntransfer_model.eval()  # Set the model to evaluation mode\ntest_loss = 0.0\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():  # No gradients required during testing\n    for data, target in transfer_test_loader:\n        data, target = data.float().to(device), target.long().to(device)\n        output = transfer_model(data)\n        loss = criterion(output, target)\n        \n        test_loss += loss.item()\n        \n        _, predicted = output.max(1)  # Get index of the max value for each sample\n        total += target.size(0)  # Increment the total count\n        correct += predicted.eq(target).sum().item()  # Increment the correct count\n\ntest_loss /= len(transfer_test_loader)\naccuracy = 100. * correct / total\n\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"Test Accuracy: {accuracy:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2023-10-10T13:27:42.391948Z","iopub.execute_input":"2023-10-10T13:27:42.392351Z","iopub.status.idle":"2023-10-10T13:27:42.414425Z","shell.execute_reply.started":"2023-10-10T13:27:42.392321Z","shell.execute_reply":"2023-10-10T13:27:42.413357Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Test Loss: 0.0001\nTest Accuracy: 100.00%\n","output_type":"stream"}]}]}
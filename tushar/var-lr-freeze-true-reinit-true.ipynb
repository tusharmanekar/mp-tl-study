{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"***IMPORTS***","metadata":{"id":"Tnr5Tf1Qa1H-"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nimport math\n\nimport json, copy\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchvision import datasets, transforms\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom scipy.interpolate import griddata\n\nfrom types import SimpleNamespace\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"L0eJ0XYWQHS6","execution":{"iopub.status.busy":"2023-11-28T23:10:57.380529Z","iopub.execute_input":"2023-11-28T23:10:57.381177Z","iopub.status.idle":"2023-11-28T23:11:02.812065Z","shell.execute_reply.started":"2023-11-28T23:10:57.381145Z","shell.execute_reply":"2023-11-28T23:11:02.811265Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"***MODEL UTILS***","metadata":{}},{"cell_type":"code","source":"# ------------------------------------ MODEL UTILS ----------------------------------------------\nclass CustomCNN(nn.Module):\n    def __init__(self, input_dim, output_dim, depth, num_channels, hidden_dim_lin, activation_function, kernel_size, use_pooling=True):\n        super(CustomCNN, self).__init__()\n\n        # Initial number of input channels, assuming grayscale images\n        in_channels = 1\n\n        # Dynamically add convolutional and activation layers based on the specified depth\n        for i in range(depth):\n            # Create a convolutional layer and add it to the model\n            setattr(self, f\"conv{i}\", nn.Conv2d(in_channels, num_channels, kernel_size=kernel_size, padding=math.floor(kernel_size/2)))\n\n            # Create an activation layer (e.g., ReLU) and add it to the model\n            setattr(self, f\"act{i}\", activation_function())\n\n            # Update the input dimensions after convolution\n            input_dim = (input_dim - kernel_size + 2 * math.floor(kernel_size/2)) + 1\n\n            # Optionally add pooling layers to reduce spatial dimensions\n            if use_pooling and (i+1) % depth == 0:\n                setattr(self, f\"pool{i}\", nn.MaxPool2d(2))\n                input_dim = input_dim // 2\n\n            # Update the input channels for the next convolutional layer\n            in_channels = num_channels\n\n        # Compute the size of the flattened features for the fully connected layer\n        flattened_size = in_channels * input_dim * input_dim\n        # Add two fully connected layers for classification\n        self.fc_1 = nn.Linear(flattened_size, hidden_dim_lin)\n        self.relu = activation_function()\n        self.fc_2 = nn.Linear(hidden_dim_lin, output_dim)\n\n        # Add log softmax layer for multi-class classification output\n        self.logsoftmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, x):\n        # Iterate over each module in the CustomCNN class\n        for layer_name, layer in self.named_children():\n            # Process the input tensor through convolutional and activation layers\n            if \"conv\" in layer_name or \"act\" in layer_name:\n                x = layer(x)\n            # Process the input tensor through pooling layers if they exist\n            elif \"pool\" in layer_name:\n                x = layer(x)\n            # If reached fully connected layers, break the loop\n            elif isinstance(layer, nn.Linear):\n                break\n\n        # Flatten the tensor to fit the input shape of the fully connected layers\n        x = x.view(x.size(0), -1)\n        # Pass the tensor through the fully connected layers\n        x = self.fc_1(x)\n        x = self.relu(x)\n        x = self.fc_2(x)\n\n        # Return log softmax activated output\n        return self.logsoftmax(x)\n\ndef generate_cnn(input_dim, output_dim, depth, num_channels, hidden_dim_lin, kernel_size, activation_function=nn.ReLU, use_pooling=True):\n    model = CustomCNN(input_dim, output_dim, depth, num_channels, hidden_dim_lin, activation_function, kernel_size, use_pooling)\n    return model\n\nclass Trainer:\n    \"\"\"\n    A class for training and evaluating a model with early stopping and best model saving functionalities.\n\n    Attributes:\n    - model: PyTorch model to be trained and evaluated.\n    - dataloader: Contains data loaders (train, validation, test) for training and evaluation.\n    - params: Dictionary containing various hyperparameters and settings.\n    - device: the device to which tensors should be moved before computation.\n    - optimizer: The optimizer for training.\n    - best_model_state: State dictionary of the best model.\n    - max_val_acc: The highest validation accuracy encountered during training.\n    - no_improve_epochs: Number of epochs without improvement in validation accuracy.\n    - is_cnn: Flag indicating if the model is a CNN.\n    - is_debug: Flag indicating if debug information should be printed.\n    - classification_report_flag: Flag indicating if a classification report should be generated.\n\n    Methods:\n    - train_epoch(): Runs a single epoch of training.\n    - evaluate(loader): Evaluates the model on a given data loader.\n    - save_best_model(): Saves the current state of the model as the best model.\n    - save_checkpoint(epoch, train_acc, val_acc): Saves the current state of the model and other information as a checkpoint.\n    - early_stopping_check(val_acc): Checks the stopping criterion and performs actions based on it.\n    - train(): Runs the training process for a number of epochs, with early stopping functionality.\n\n    Usage:\n    params = {\n      'device': 'cuda',\n      'lr': 0.001,\n      'num_train': 10,\n      'early_stop_patience': 3,\n      'save_best': True,\n      'save_checkpoints': False,\n      'is_cnn': True,\n      'is_debug': True,\n      'classification_report_flag': True\n    }\n\n    trainer = Trainer(model, dataloader, params)\n    train_acc, test_acc, effective_epochs, checkpoints = trainer.train()\n    \"\"\"\n    def __init__(self, model, dataloader, lr, params, param_groups=None):\n        self.model = model\n        self.dataloader = dataloader\n        self.params = params\n        self.device = torch.device(params['device'])\n        if param_groups:\n            self.optimizer = optim.Adam(param_groups)\n        else:\n            self.optimizer = optim.Adam(model.parameters(), lr=lr)\n        # optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)\n        # Initialize best_model_state with the current model state\n        self.best_model_state = copy.deepcopy(self.model.state_dict())\n        self.max_val_acc = 0.\n        self.no_improve_epochs = 0\n        self.is_cnn = params.get('is_cnn', False)\n        self.is_debug = params.get('is_debug', False)\n        self.classification_report_flag = params.get('classification_report_flag', False)\n        self.logger = params.get('logger', print)\n\n    def train_epoch(self):\n      self.model.train()\n      for batch_idx, (data, target) in enumerate(self.dataloader.train_loader):\n          # Print the size of the current batch\n          if self.is_cnn:\n            data = data.view(data.size(0), 1, 28, 28)\n          else:\n            data = data.reshape([data.shape[0], -1])\n          data, target = data.to(self.device), target.to(self.device)\n          self.optimizer.zero_grad()\n          output = self.model(data)\n          loss = F.nll_loss(output, target)\n          loss.backward()\n          self.optimizer.step()\n\n          if self.is_debug and batch_idx % 20 == 0:\n              self.logger(f\"Batch: {batch_idx}, Loss: {loss.item()}\")\n\n    def evaluate(self, loader):\n        return eval(self.model, self.device, loader, self.is_debug, self.classification_report_flag, self.is_cnn)\n\n    def save_best_model(self):\n        torch.save(self.model.state_dict(), 'best_model.pth')\n\n    def save_checkpoint(self, epoch, train_acc, val_acc):\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'train_acc': train_acc,\n            'val_acc': val_acc\n        }\n        torch.save(checkpoint, f'checkpoint_epoch_{epoch}.pth')\n        return checkpoint\n\n    def early_stopping_check(self, val_acc):\n        if val_acc > self.max_val_acc:\n            self.max_val_acc = val_acc\n            self.no_improve_epochs = 0\n            # Deep copy the model's state\n            self.best_model_state = copy.deepcopy(self.model.state_dict())\n            if self.params.get('save_best', False):\n                self.save_best_model()\n        else:\n            self.no_improve_epochs += 1\n            if self.no_improve_epochs >= self.params['early_stop_patience']:\n                self.logger(\"Early stopping invoked.\")\n                # Only load if best_model_state has been set\n                if self.best_model_state is not None:\n                    self.model.load_state_dict(self.best_model_state)\n                return True\n        return False\n\n    def train(self, verbose=1):\n        effective_epochs = 0\n        checkpoints = []\n\n        for epoch in range(self.params['num_train']):\n            effective_epochs += 1\n            self.train_epoch()\n\n            train_acc = self.evaluate(self.dataloader.train_loader)\n            val_acc = self.evaluate(self.dataloader.val_loader)\n            if verbose >= 1:\n                self.logger(f'Epoch: {epoch} \\tTraining Accuracy: {train_acc*100:.2f}%')\n                self.logger(f'Validation Accuracy: {val_acc*100:.2f}%')\n\n            if self.params.get('early_stop_patience', None):\n                if self.early_stopping_check(val_acc):\n                    self.model.load_state_dict(self.best_model_state)\n                    break\n\n            if self.params.get('save_checkpoints', False):\n                checkpoint = self.save_checkpoint(epoch, train_acc, val_acc)\n                checkpoints.append(checkpoint)\n\n        # Final evaluations\n        train_acc = self.evaluate(self.dataloader.train_loader)\n        test_acc = self.evaluate(self.dataloader.test_loader)\n\n        return train_acc, test_acc, effective_epochs, checkpoints\n\ndef eval(model, device, dataset_loader, debug=False, classification_report_flag=False, is_cnn=True, logger=print):\n    \"\"\"\n    Evaluates the model on the given dataset loader.\n\n    Parameters:\n    - model: the PyTorch model to evaluate.\n    - device: the device to which tensors should be moved before computation.\n    - dataset_loader: DataLoader for evaluation.\n    - debug: whether to print debug info like loss and accuracy.\n    - classification_report_flag: whether to print a classification report.\n    - is_cnn: a flag indicating if the model is a CNN. If it's not, the input data will be reshaped.\n    - logger: logging function for printing messages.\n\n    Returns:\n    - Accuracy of the model on the provided dataset loader.\n\n    Usage:\n    - accuracy = eval(model, device, dataset_loader, debug=False, is_cnn=False, classification_report_flag=False)\n    \"\"\"\n\n    model.eval()\n    test_loss, correct = 0., 0.\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for data, target in dataset_loader:\n            if is_cnn:\n              data = data.view(data.size(0), 1, 28, 28)\n            else:\n              data = data.reshape([data.shape[0], -1])\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n            all_preds.extend(pred.cpu().numpy())\n            all_labels.extend(target.cpu().numpy())\n\n    num_data = len(dataset_loader.dataset)\n    test_loss /= num_data\n    acc = correct / num_data\n\n    if debug:\n        logger('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, num_data, 100. * acc))\n\n    if classification_report_flag:\n        unique_labels = np.unique(all_labels).tolist()\n        logger(classification_report(all_labels, all_preds, labels=unique_labels, target_names=[f'Class {i}' for i in unique_labels]))\n\n    return acc\n\ndef cut_custom_cnn_model(model, cut_point, params):\n    new_model = copy.deepcopy(model)\n    reinitialized_layers = []  # List to store reinitialized layer names\n\n    # Reinitialize dense layers (fc_1 and fc_2) if specified\n    if params[\"reinit_both_dense\"]:\n        new_model.fc_1.reset_parameters()\n        new_model.fc_2.reset_parameters()\n        reinitialized_layers.extend([\"fc_1\", \"fc_2\"])\n\n    # Get the names of all convolutional layers\n    conv_layer_names = [name for name, layer in new_model.named_children() if isinstance(layer, nn.Conv2d)]\n\n    # Determine which conv layers to reinitialize based on the cut point\n    if cut_point > 0 and params[\"reinit\"]:  # Only reinitialize conv layers if cut point is greater than 0\n        for name in conv_layer_names[cut_point - 1:]:  # Start from cut_point - 1\n            getattr(new_model, name).reset_parameters()\n            reinitialized_layers.append(name)\n\n    return new_model, reinitialized_layers","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2023-11-28T23:11:02.814222Z","iopub.execute_input":"2023-11-28T23:11:02.814988Z","iopub.status.idle":"2023-11-28T23:11:02.856461Z","shell.execute_reply.started":"2023-11-28T23:11:02.814947Z","shell.execute_reply":"2023-11-28T23:11:02.855548Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"***DATA UTILS***","metadata":{}},{"cell_type":"code","source":"# --------------------------------- DATA UTILS -----------------------------------\ndef reduce_dataset(dataloader, percentage, balanced=True, seed=42):\n\n    \"\"\"\n    Reduces the dataset to the given percentage. Can ensure class balance if needed.\n\n    Parameters:\n    - dataloader: PyTorch DataLoader object.\n    - percentage: Desired percentage of the original dataset.\n    - balanced: If True, ensures class balance. If False, reduces randomly.\n    - seed: Seed for reproducibility.\n\n    Returns:\n    - reduced_dataloader: DataLoader with the reduced dataset.\n    \"\"\"\n    # Extract the dataset from the dataloader\n    dataset = dataloader.dataset\n\n    # Extract all data and labels from the dataset\n    X = [dataset[i][0] for i in range(len(dataset))]\n    y = [dataset[i][1] for i in range(len(dataset))]\n\n    # Set the seed for reproducibility\n    torch.manual_seed(seed)\n\n    if not balanced:\n        # Determine the number of samples to keep\n        num_samples = int(len(dataset) * percentage)\n\n        # Randomly select indices without replacement\n        indices = torch.randperm(len(dataset))[:num_samples].tolist()\n\n    else:\n        # Get unique classes and their counts\n        classes, class_counts = torch.unique(torch.tensor(y), return_counts=True)\n\n        # Determine the number of samples per class to keep\n        num_samples_per_class = int(len(dataset) * percentage / len(classes))\n        indices = []\n\n        for class_label in classes:\n            class_indices = [i for i, label in enumerate(y) if label == class_label]\n\n            # Randomly select indices without replacement for each class\n            class_selected_indices = torch.randperm(len(class_indices))[:num_samples_per_class].tolist()\n            indices.extend([class_indices[i] for i in class_selected_indices])\n\n    # Use a Subset of the original dataset to create a reduced dataset\n    reduced_dataset = data.Subset(dataset, indices)\n\n    # Create a DataLoader with the reduced dataset.\n    reduced_dataloader = data.DataLoader(reduced_dataset, batch_size=dataloader.batch_size, shuffle=True)\n\n    return reduced_dataloader\n\nclass RelabeledSubset(torch.utils.data.Dataset):\n    def __init__(self, dataset, offset):\n        self.dataset = dataset\n        self.offset = offset\n\n    def __getitem__(self, idx):\n        data, label = self.dataset[idx]\n        # Offset the label to start from 0\n        label = label - self.offset\n        return data, label\n\n    def __len__(self):\n        return len(self.dataset)\n\nclass TransferLearningMNIST(object):\n    def __init__(self, batch_size, input_dim=28*28, val_split=0.1, num_workers=0, seed=42):\n        self.input_dim = input_dim\n        self.output_dim = 10\n        self.val_split = val_split\n\n        def filter_dataset(dataset, classes):\n            indices = [i for i, t in enumerate(dataset.targets) if t in classes]\n            return torch.utils.data.Subset(dataset, indices)\n\n        mnist_train_data = datasets.MNIST(\n            '../data',\n            train=True,\n            download=True,\n            transform=transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Lambda(lambda x: (x * 2 - 1) * 0.5),\n            ]))\n\n        pretrain_train_data = filter_dataset(mnist_train_data, list(range(5)))\n        finetune_train_data = filter_dataset(mnist_train_data, list(range(5, 10)))\n\n        pretrain_len = len(pretrain_train_data)\n        finetune_len = len(finetune_train_data)\n        pretrain_val_len = int(val_split * pretrain_len)\n        finetune_val_len = int(val_split * finetune_len)\n        pretrain_train_set, pretrain_val_set = torch.utils.data.random_split(\n            pretrain_train_data, [pretrain_len - pretrain_val_len, pretrain_val_len], generator=torch.Generator().manual_seed(seed))\n        finetune_train_set, finetune_val_set = torch.utils.data.random_split(\n            finetune_train_data, [finetune_len - finetune_val_len, finetune_val_len], generator=torch.Generator().manual_seed(seed))\n\n        self.pretrain_train_loader = torch.utils.data.DataLoader(pretrain_train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n        self.pretrain_val_loader = torch.utils.data.DataLoader(pretrain_val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n        # Use the RelabeledSubset for fine-tuning datasets\n        finetune_train_set = RelabeledSubset(finetune_train_set, 5)\n        finetune_val_set = RelabeledSubset(finetune_val_set, 5)\n\n        self.finetune_train_loader = torch.utils.data.DataLoader(finetune_train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n        self.finetune_val_loader = torch.utils.data.DataLoader(finetune_val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n        mnist_test_data = datasets.MNIST(\n            '../data',\n            train=False,\n            download=True,\n            transform=transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Lambda(lambda x: (x * 2 - 1) * 0.5),\n            ]))\n\n        pretrain_test_data = filter_dataset(mnist_test_data, list(range(5)))\n        finetune_test_data = filter_dataset(mnist_test_data, list(range(5, 10)))\n\n        self.pretrain_test_loader = torch.utils.data.DataLoader(pretrain_test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n        # Use the RelabeledSubset for fine-tuning test datasets\n        finetune_test_data = RelabeledSubset(finetune_test_data, 5)\n        self.finetune_test_loader = torch.utils.data.DataLoader(finetune_test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n        # Complete test loader contains all test examples.\n        self.complete_test_loader = torch.utils.data.DataLoader(mnist_test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\nclass TransferLearningMNISTWrapper:\n    \"\"\"\n    This wrapper class provides a convenient way to switch between pretraining and fine-tuning phases.\n\n    It allows for changing the phase and accordingly updating the data loaders (train, val, test)\n    to either pretraining or fine-tuning sets.\n    \"\"\"\n    def __init__(self, transferLearningMNISTObj, phase):\n        \"\"\"\n        Initializes the TransferLearningMNISTWrapper object.\n\n        Parameters:\n        - transferLearningMNISTObj: An instance of the TransferLearningMNIST class.\n        - phase: String indicating the current phase (\"pretrain\" or \"finetune\").\n        \"\"\"\n        self.transferLearningMNISTObj = transferLearningMNISTObj\n        self.phase = phase\n        self.input_dim = self.transferLearningMNISTObj.input_dim\n        self.output_dim = self.transferLearningMNISTObj.output_dim\n        self.update_phase(phase)\n\n    def update_phase(self, phase):\n        \"\"\"\n        Updates the phase and the corresponding data loaders.\n\n        Parameters:\n        - phase: String indicating the desired phase (\"pretrain\" or \"finetune\").\n\n        Throws:\n        - ValueError: If the phase is neither \"pretrain\" nor \"finetune\".\n        \"\"\"\n        self.phase = phase\n        if phase == 'pretrain':\n            self.train_loader = self.transferLearningMNISTObj.pretrain_train_loader\n            self.val_loader = self.transferLearningMNISTObj.pretrain_val_loader\n            self.test_loader = self.transferLearningMNISTObj.pretrain_test_loader\n        elif phase == 'finetune':\n            self.train_loader = self.transferLearningMNISTObj.finetune_train_loader\n            self.val_loader = self.transferLearningMNISTObj.finetune_val_loader\n            self.test_loader = self.transferLearningMNISTObj.finetune_test_loader\n        else:\n            raise ValueError('Phase must be either \"pretrain\" or \"finetune\".')\n\n    def get_current_phase(self):\n      return self.phase","metadata":{"execution":{"iopub.status.busy":"2023-11-28T23:11:02.857617Z","iopub.execute_input":"2023-11-28T23:11:02.857885Z","iopub.status.idle":"2023-11-28T23:11:02.886566Z","shell.execute_reply.started":"2023-11-28T23:11:02.857861Z","shell.execute_reply":"2023-11-28T23:11:02.885707Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"***PLOTTING UTILS***","metadata":{}},{"cell_type":"code","source":"# ------------------------------------------ PLOTTING UTILS -------------------------------------------\n'''def effective_rank(singular_values):\n    sigma_max = np.max(singular_values)\n    sigma_min = singular_values[-1] if singular_values[-1] > 0 else np.min(singular_values[singular_values > 0])\n    # print(sigma_max, sigma_min)\n    print(np.sqrt(sigma_max / sigma_min))\n    print('----')\n    return np.sqrt(sigma_max / sigma_min)'''\n\ndef effective_rank(singular_values):\n    normalized_singular_values = singular_values / np.sum(singular_values)\n    entropy = -np.sum(normalized_singular_values * np.log(normalized_singular_values))\n    eff_rank = np.exp(entropy)\n    return eff_rank\n\ndef plot_layer_effective_ranks(model, print_ranks=True):\n    effective_ranks = []\n    layer_names = []\n\n    for name, param in model.named_parameters():\n        if 'weight' in name:  # We are only interested in weight matrices\n            weight_matrix = param.detach().cpu().numpy()\n            singular_values = np.linalg.svd(weight_matrix, compute_uv=False)\n            eff_rank = effective_rank(singular_values)\n            effective_ranks.append(eff_rank)\n            layer_names.append(name)\n\n    if print_ranks:\n        for layer_name, eff_rank in zip(layer_names, effective_ranks):\n            print(f'{layer_name}: {eff_rank:.4f}')\n\n    # Plotting\n    plt.figure(figsize=(15, 5))\n    plt.bar(layer_names, effective_ranks, color='green')\n    plt.xlabel('Layer')\n    plt.ylabel('Effective Rank')\n    plt.title('Effective Rank of Weight Matrices for Each Layer')\n    plt.grid(True)\n\n    y_max = np.max(effective_ranks) + 1  # Get maximum rank and add 1 for better visualization\n    y_min = np.min(effective_ranks) - 1  # Get minimum rank and subtract 1 for better visualization\n    plt.yticks(np.arange(0, int(y_max)+2, step=2))  # Set yticks\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-28T23:11:02.888457Z","iopub.execute_input":"2023-11-28T23:11:02.888751Z","iopub.status.idle":"2023-11-28T23:11:02.903912Z","shell.execute_reply.started":"2023-11-28T23:11:02.888727Z","shell.execute_reply":"2023-11-28T23:11:02.903074Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# EXPERIMENT SETUP 1: _FREEZE, REINIT, POOLING, DENSE:REINIT BOTH_\n- percentages_set_1 = [0.001, 0.002, 0.005, 0.01, 0.05, 0.1, 0.3, 0.5, 0.8, 1]\n- dataset: same as before MNIST 5 to 5\n\n- architecture:\n  - Conv 1 (5,5), channels=10\n  - Relu\n  - Conv 2 (5,5), channels=10\n  - Relu\n  - Conv 3 (5,5), channels=10\n  - Relu\n  - _POOLING_\n  - Dense 1 (x, a) x=output shape of prev layer, a:random hidden layer width (we use 128)\n  - Relu\n  - Dense 2 (a, 5)\n  - softmax\n\n- lr pretraining = 0.001\n- lr finetuning = 0.0001\n- lr end-to-end = 0.001\n\n- Freezing the layers before the cut: _YES_\n- Reinitializing the Convolutional layers after the cut: _YES_\n- Reinitializing Dense 1: _YES_\n- Reinitializing Dense 2: _YES_\n","metadata":{"id":"NJ5JiRZYQW-J"}},{"cell_type":"code","source":"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-11-28T23:11:02.905244Z","iopub.execute_input":"2023-11-28T23:11:02.905541Z","iopub.status.idle":"2023-11-28T23:11:02.921603Z","shell.execute_reply.started":"2023-11-28T23:11:02.905515Z","shell.execute_reply":"2023-11-28T23:11:02.920769Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"percentages = [0.001, 0.002, 0.005, 0.01, 0.05, 0.1, 0.3, 0.5, 0.8, 1]\n# percentages = [0.001, 0.002]\n\n# cuts=0 means: end-to-end model if we are reinitializing\ncuts = [0,1,2,3]\nseed_set = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]  # currently not being used\nbatch_size = 4096","metadata":{"id":"FeWfmLUgswad","execution":{"iopub.status.busy":"2023-11-28T23:11:02.922796Z","iopub.execute_input":"2023-11-28T23:11:02.923246Z","iopub.status.idle":"2023-11-28T23:11:02.931281Z","shell.execute_reply.started":"2023-11-28T23:11:02.923185Z","shell.execute_reply":"2023-11-28T23:11:02.930197Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Pretraining\n\n","metadata":{"id":"UbrS0kwrcHfE"}},{"cell_type":"code","source":"dataloader = TransferLearningMNIST(batch_size)\ndataloader_wrapped = TransferLearningMNISTWrapper(dataloader, phase = 'pretrain')\n\n# Changes Here for the experiments\nparams = {\n      'depth': 3,\n      'width': 10, # num channels for CNN\n      'hidden_dim_lin': 128,\n      'activation_function': nn.ReLU,\n      'kernel_size': 5,\n      'device': device,\n      'lr_pretrain': 0.001,\n      'lr_fine_tune': 0.001,  # CHANGE: if no layer-wise lr\n      'lr_fine_tune_reinit': 0.001,         # CHANGE: if no layer-wise lr\n      'lr_fine_tune_no_reinit': 0.0001,     # CHANGE: if layer-wise lr\n      'num_train': 40,\n      'early_stop_patience': 6,\n      'save_best': True,\n      'save_checkpoints': False,\n      'is_cnn': True,\n      'is_debug': False,\n      'classification_report_flag': False,\n      'percentages':percentages,\n      'batch_size':batch_size,\n      'seed_set':seed_set,\n      'use_pooling': False,   # CHANGE\n      'freeze': True,         # CHANGE: freeze the conv layers before the cut\n      'reinit': True,         # CHANGE: reinit the conv lyers only after the cut\n      'reinit_both_dense': True   # CHANGE: True for reinitialize both dense layers, False for reinit only the last dense layer\n    }\n\n# Create DNN model\npretrained_model = generate_cnn(input_dim = 28, output_dim = 10, depth = params['depth'], num_channels = params['width'],\n                    hidden_dim_lin = params['hidden_dim_lin'], kernel_size = params['kernel_size'], activation_function = params[\"activation_function\"], use_pooling=params['use_pooling'])\npretrained_model.to(device)","metadata":{"id":"6sK-eF2ucUDa","outputId":"f7f2e890-7c35-48ee-edab-dc0e97d5ce52","execution":{"iopub.status.busy":"2023-11-28T23:11:02.932419Z","iopub.execute_input":"2023-11-28T23:11:02.932761Z","iopub.status.idle":"2023-11-28T23:11:10.314230Z","shell.execute_reply.started":"2023-11-28T23:11:02.932727Z","shell.execute_reply":"2023-11-28T23:11:10.313258Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9912422/9912422 [00:00<00:00, 103259828.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28881/28881 [00:00<00:00, 53106397.99it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1648877/1648877 [00:00<00:00, 30323148.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4542/4542 [00:00<00:00, 6438164.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"CustomCNN(\n  (conv0): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n  (act0): ReLU()\n  (conv1): Conv2d(10, 10, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n  (act1): ReLU()\n  (conv2): Conv2d(10, 10, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n  (act2): ReLU()\n  (fc_1): Linear(in_features=7840, out_features=128, bias=True)\n  (relu): ReLU()\n  (fc_2): Linear(in_features=128, out_features=10, bias=True)\n  (logsoftmax): LogSoftmax(dim=1)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# # Train and evaluate\n# trainer = Trainer(pretrained_model, dataloader_wrapped, params[\"lr_pretrain\"], params)\n# train_acc, test_acc, effective_epochs, checkpoints = trainer.train()\n\n# print(f\"Final Training Accuracy: {train_acc:.4f}\")\n# print(f\"Final Test Accuracy: {test_acc:.4f}\")","metadata":{"id":"gU9NwDAjhT3o","outputId":"7335d6d9-a841-4397-b879-6ceef21ab105","execution":{"iopub.status.busy":"2023-11-28T23:11:10.315584Z","iopub.execute_input":"2023-11-28T23:11:10.315957Z","iopub.status.idle":"2023-11-28T23:11:10.320727Z","shell.execute_reply.started":"2023-11-28T23:11:10.315928Z","shell.execute_reply":"2023-11-28T23:11:10.319726Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# eval(pretrained_model, device, dataloader_wrapped.val_loader, debug=True, classification_report_flag=True, is_cnn=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T23:11:10.321777Z","iopub.execute_input":"2023-11-28T23:11:10.322119Z","iopub.status.idle":"2023-11-28T23:11:10.333445Z","shell.execute_reply.started":"2023-11-28T23:11:10.322085Z","shell.execute_reply":"2023-11-28T23:11:10.332651Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Fine-tuning Experiments","metadata":{"id":"ge8Q1YiEqC7e"}},{"cell_type":"code","source":"# # load results: to continue from a checkpoint (actually don't run)\n# with open('results.json', 'r') as f:\n#     results = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T23:11:10.335921Z","iopub.execute_input":"2023-11-28T23:11:10.336184Z","iopub.status.idle":"2023-11-28T23:11:10.344823Z","shell.execute_reply.started":"2023-11-28T23:11:10.336160Z","shell.execute_reply":"2023-11-28T23:11:10.344120Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Baselines (skip if it was already run)","metadata":{}},{"cell_type":"code","source":"#training of baseline, end to end, models (#trials x #percentages)\nresults_baseline = []\n\ndataloader_wrapped.update_phase('finetune')\n\n# template_model = generate_cnn(input_dim = 28, output_dim = 10, depth = params['depth'], num_channels = params['width'],\n#                      hidden_dim_lin = params['hidden_dim_lin'], kernel_size = params['kernel_size'], activation_function = params[\"activation_function\"], use_pooling=params['use_pooling'])\n\nfor sampled_percentage in percentages:      \n    if sampled_percentage <= 0.01:\n        repeats = 10\n    elif sampled_percentage < 0.5:\n        repeats = 5\n    else:\n        repeats = 3\n    \n    for repeat in range(repeats):\n        # Print or log the sampled values for transparency\n        print(f\"\\nSampled Percentage: {sampled_percentage}, Lr: {params['lr_fine_tune']}, Repeat: {repeat}\")\n\n        # Reduce the dataset\n        train_loader_reduced = reduce_dataset(dataloader_wrapped.train_loader, sampled_percentage, seed = repeat)\n        torch.manual_seed(repeat)\n        #train_loader_reduced = reduce_dataset(dataloader_wrapped.train_loader, sampled_percentage, seed = repeat)\n        dataset_namespace_new = SimpleNamespace(train_loader=train_loader_reduced, test_loader=dataloader_wrapped.test_loader, val_loader=dataloader_wrapped.val_loader)\n\n        # Copy and then cut the model - we already deepcopy it in the function: pretrained_model\n        #model_temp = copy.deepcopy(template_model)\n        model_temp = generate_cnn(input_dim = 28, output_dim = 10, depth = params['depth'], num_channels = params['width'],\n        hidden_dim_lin = params['hidden_dim_lin'], kernel_size = params['kernel_size'], activation_function = params[\"activation_function\"], use_pooling=params['use_pooling'])\n        model_temp.to(device)\n\n        # Train and evaluate\n        trainer = Trainer(model_temp, dataset_namespace_new, params['lr_fine_tune'], params)\n        train_acc, test_acc, effective_epochs, checkpoints = trainer.train(verbose=0)\n        print(f\"Training Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n\n        # Store the results\n        results_baseline.append({\"lr\":params['lr_fine_tune'], \"sampled_percentage\":sampled_percentage, \"sampled_cut_point\":-1, \"repeat\":repeat, \"train_acc\":train_acc, \"test_acc\":test_acc}) # -1 for the cut point means it's baseline","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-28T23:11:10.346180Z","iopub.execute_input":"2023-11-28T23:11:10.346574Z","iopub.status.idle":"2023-11-29T00:33:53.751514Z","shell.execute_reply.started":"2023-11-28T23:11:10.346540Z","shell.execute_reply":"2023-11-29T00:33:53.750462Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"\nSampled Percentage: 0.001, Lr: 0.001, Repeat: 0\nEarly stopping invoked.\nTraining Accuracy: 1.0000, Test Accuracy: 0.7900\n\nSampled Percentage: 0.001, Lr: 0.001, Repeat: 1\nEarly stopping invoked.\nTraining Accuracy: 0.2000, Test Accuracy: 0.2076\n\nSampled Percentage: 0.001, Lr: 0.001, Repeat: 2\nTraining Accuracy: 1.0000, Test Accuracy: 0.7657\n\nSampled Percentage: 0.001, Lr: 0.001, Repeat: 3\nEarly stopping invoked.\nTraining Accuracy: 1.0000, Test Accuracy: 0.7556\n\nSampled Percentage: 0.001, Lr: 0.001, Repeat: 4\nEarly stopping invoked.\nTraining Accuracy: 1.0000, Test Accuracy: 0.7369\n\nSampled Percentage: 0.001, Lr: 0.001, Repeat: 5\nEarly stopping invoked.\nTraining Accuracy: 1.0000, Test Accuracy: 0.7046\n\nSampled Percentage: 0.001, Lr: 0.001, Repeat: 6\nEarly stopping invoked.\nTraining Accuracy: 1.0000, Test Accuracy: 0.5919\n\nSampled Percentage: 0.001, Lr: 0.001, Repeat: 7\nEarly stopping invoked.\nTraining Accuracy: 0.9600, Test Accuracy: 0.7272\n\nSampled Percentage: 0.001, Lr: 0.001, Repeat: 8\nEarly stopping invoked.\nTraining Accuracy: 1.0000, Test Accuracy: 0.7869\n\nSampled Percentage: 0.001, Lr: 0.001, Repeat: 9\nEarly stopping invoked.\nTraining Accuracy: 1.0000, Test Accuracy: 0.6920\n\nSampled Percentage: 0.002, Lr: 0.001, Repeat: 0\nEarly stopping invoked.\nTraining Accuracy: 1.0000, Test Accuracy: 0.7974\n\nSampled Percentage: 0.002, Lr: 0.001, Repeat: 1\nEarly stopping invoked.\nTraining Accuracy: 1.0000, Test Accuracy: 0.8424\n\nSampled Percentage: 0.002, Lr: 0.001, Repeat: 2\nEarly stopping invoked.\nTraining Accuracy: 0.9800, Test Accuracy: 0.8523\n\nSampled Percentage: 0.002, Lr: 0.001, Repeat: 3\nEarly stopping invoked.\nTraining Accuracy: 0.9800, Test Accuracy: 0.8227\n\nSampled Percentage: 0.002, Lr: 0.001, Repeat: 4\nEarly stopping invoked.\nTraining Accuracy: 0.9800, Test Accuracy: 0.7778\n\nSampled Percentage: 0.002, Lr: 0.001, Repeat: 5\nEarly stopping invoked.\nTraining Accuracy: 1.0000, Test Accuracy: 0.7811\n\nSampled Percentage: 0.002, Lr: 0.001, Repeat: 6\nEarly stopping invoked.\nTraining Accuracy: 1.0000, Test Accuracy: 0.7614\n\nSampled Percentage: 0.002, Lr: 0.001, Repeat: 7\nTraining Accuracy: 1.0000, Test Accuracy: 0.8221\n\nSampled Percentage: 0.002, Lr: 0.001, Repeat: 8\nEarly stopping invoked.\nTraining Accuracy: 1.0000, Test Accuracy: 0.7700\n\nSampled Percentage: 0.002, Lr: 0.001, Repeat: 9\nEarly stopping invoked.\nTraining Accuracy: 1.0000, Test Accuracy: 0.7937\n\nSampled Percentage: 0.005, Lr: 0.001, Repeat: 0\nEarly stopping invoked.\nTraining Accuracy: 0.9231, Test Accuracy: 0.7914\n\nSampled Percentage: 0.005, Lr: 0.001, Repeat: 1\nEarly stopping invoked.\nTraining Accuracy: 0.9462, Test Accuracy: 0.8679\n\nSampled Percentage: 0.005, Lr: 0.001, Repeat: 2\nTraining Accuracy: 0.9692, Test Accuracy: 0.8885\n\nSampled Percentage: 0.005, Lr: 0.001, Repeat: 3\nEarly stopping invoked.\nTraining Accuracy: 0.9692, Test Accuracy: 0.8788\n\nSampled Percentage: 0.005, Lr: 0.001, Repeat: 4\nEarly stopping invoked.\nTraining Accuracy: 0.9308, Test Accuracy: 0.8809\n\nSampled Percentage: 0.005, Lr: 0.001, Repeat: 5\nTraining Accuracy: 0.9923, Test Accuracy: 0.8638\n\nSampled Percentage: 0.005, Lr: 0.001, Repeat: 6\nEarly stopping invoked.\nTraining Accuracy: 0.9923, Test Accuracy: 0.8854\n\nSampled Percentage: 0.005, Lr: 0.001, Repeat: 7\nEarly stopping invoked.\nTraining Accuracy: 0.9923, Test Accuracy: 0.8799\n\nSampled Percentage: 0.005, Lr: 0.001, Repeat: 8\nTraining Accuracy: 1.0000, Test Accuracy: 0.8778\n\nSampled Percentage: 0.005, Lr: 0.001, Repeat: 9\nTraining Accuracy: 1.0000, Test Accuracy: 0.8700\n\nSampled Percentage: 0.01, Lr: 0.001, Repeat: 0\nTraining Accuracy: 0.9731, Test Accuracy: 0.8943\n\nSampled Percentage: 0.01, Lr: 0.001, Repeat: 1\nEarly stopping invoked.\nTraining Accuracy: 0.9615, Test Accuracy: 0.8848\n\nSampled Percentage: 0.01, Lr: 0.001, Repeat: 2\nTraining Accuracy: 0.9500, Test Accuracy: 0.9000\n\nSampled Percentage: 0.01, Lr: 0.001, Repeat: 3\nTraining Accuracy: 0.9808, Test Accuracy: 0.9074\n\nSampled Percentage: 0.01, Lr: 0.001, Repeat: 4\nEarly stopping invoked.\nTraining Accuracy: 0.8923, Test Accuracy: 0.8941\n\nSampled Percentage: 0.01, Lr: 0.001, Repeat: 5\nTraining Accuracy: 0.9692, Test Accuracy: 0.8922\n\nSampled Percentage: 0.01, Lr: 0.001, Repeat: 6\nEarly stopping invoked.\nTraining Accuracy: 0.3231, Test Accuracy: 0.3110\n\nSampled Percentage: 0.01, Lr: 0.001, Repeat: 7\nTraining Accuracy: 0.9885, Test Accuracy: 0.8947\n\nSampled Percentage: 0.01, Lr: 0.001, Repeat: 8\nTraining Accuracy: 0.9769, Test Accuracy: 0.8963\n\nSampled Percentage: 0.01, Lr: 0.001, Repeat: 9\nTraining Accuracy: 0.9654, Test Accuracy: 0.8869\n\nSampled Percentage: 0.05, Lr: 0.001, Repeat: 0\nTraining Accuracy: 0.9235, Test Accuracy: 0.9144\n\nSampled Percentage: 0.05, Lr: 0.001, Repeat: 1\nTraining Accuracy: 0.9447, Test Accuracy: 0.9206\n\nSampled Percentage: 0.05, Lr: 0.001, Repeat: 2\nTraining Accuracy: 0.9197, Test Accuracy: 0.9196\n\nSampled Percentage: 0.05, Lr: 0.001, Repeat: 3\nTraining Accuracy: 0.9341, Test Accuracy: 0.9171\n\nSampled Percentage: 0.05, Lr: 0.001, Repeat: 4\nTraining Accuracy: 0.9341, Test Accuracy: 0.9338\n\nSampled Percentage: 0.1, Lr: 0.001, Repeat: 0\nTraining Accuracy: 0.9244, Test Accuracy: 0.9198\n\nSampled Percentage: 0.1, Lr: 0.001, Repeat: 1\nTraining Accuracy: 0.9380, Test Accuracy: 0.9307\n\nSampled Percentage: 0.1, Lr: 0.001, Repeat: 2\nEarly stopping invoked.\nTraining Accuracy: 0.8367, Test Accuracy: 0.8472\n\nSampled Percentage: 0.1, Lr: 0.001, Repeat: 3\nTraining Accuracy: 0.9353, Test Accuracy: 0.9264\n\nSampled Percentage: 0.1, Lr: 0.001, Repeat: 4\nTraining Accuracy: 0.9350, Test Accuracy: 0.9274\n\nSampled Percentage: 0.3, Lr: 0.001, Repeat: 0\nTraining Accuracy: 0.9577, Test Accuracy: 0.9539\n\nSampled Percentage: 0.3, Lr: 0.001, Repeat: 1\nTraining Accuracy: 0.9653, Test Accuracy: 0.9611\n\nSampled Percentage: 0.3, Lr: 0.001, Repeat: 2\nTraining Accuracy: 0.9590, Test Accuracy: 0.9539\n\nSampled Percentage: 0.3, Lr: 0.001, Repeat: 3\nTraining Accuracy: 0.9658, Test Accuracy: 0.9593\n\nSampled Percentage: 0.3, Lr: 0.001, Repeat: 4\nTraining Accuracy: 0.9627, Test Accuracy: 0.9554\n\nSampled Percentage: 0.5, Lr: 0.001, Repeat: 0\nTraining Accuracy: 0.9812, Test Accuracy: 0.9768\n\nSampled Percentage: 0.5, Lr: 0.001, Repeat: 1\nTraining Accuracy: 0.9828, Test Accuracy: 0.9831\n\nSampled Percentage: 0.5, Lr: 0.001, Repeat: 2\nTraining Accuracy: 0.9791, Test Accuracy: 0.9796\n\nSampled Percentage: 0.8, Lr: 0.001, Repeat: 0\nTraining Accuracy: 0.9863, Test Accuracy: 0.9837\n\nSampled Percentage: 0.8, Lr: 0.001, Repeat: 1\nTraining Accuracy: 0.9892, Test Accuracy: 0.9864\n\nSampled Percentage: 0.8, Lr: 0.001, Repeat: 2\nTraining Accuracy: 0.9889, Test Accuracy: 0.9872\n\nSampled Percentage: 1, Lr: 0.001, Repeat: 0\nTraining Accuracy: 0.9862, Test Accuracy: 0.9829\n\nSampled Percentage: 1, Lr: 0.001, Repeat: 1\nEarly stopping invoked.\nTraining Accuracy: 0.9895, Test Accuracy: 0.9870\n\nSampled Percentage: 1, Lr: 0.001, Repeat: 2\nTraining Accuracy: 0.9904, Test Accuracy: 0.9887\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the results to a JSON file\nresults_filename = 'results_baseline.json'\nwith open(results_filename, 'w') as file:\n    json.dump(results_baseline, file)\n\nprint(f\"Results saved to {results_filename}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-29T00:33:53.752651Z","iopub.execute_input":"2023-11-29T00:33:53.752925Z","iopub.status.idle":"2023-11-29T00:33:53.760925Z","shell.execute_reply.started":"2023-11-29T00:33:53.752901Z","shell.execute_reply":"2023-11-29T00:33:53.759943Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Results saved to results_baseline.json\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Fine-tuning","metadata":{}},{"cell_type":"code","source":"# The main training loop\ndataloader_wrapped.update_phase('finetune')\n\n# Store results\nresults = []\n\nfor sampled_percentage in percentages:\n    if sampled_percentage <= 0.01:\n        repeats = 10\n    elif sampled_percentage < 0.5:\n        repeats = 5\n    else:\n        repeats = 3\n\n    for sampled_cut_point in cuts:\n        for repeat in range(repeats):\n            print(f\"\\nSampled Percentage: {sampled_percentage}, Sampled Cut Point: {sampled_cut_point}, Repeat: {repeat}\")\n\n            train_loader_reduced = reduce_dataset(dataloader_wrapped.train_loader, sampled_percentage, seed=repeat)\n            dataset_namespace_new = SimpleNamespace(train_loader=train_loader_reduced, test_loader=dataloader_wrapped.test_loader, val_loader=dataloader_wrapped.val_loader)\n            torch.manual_seed(repeat)\n\n            model_new, reinitialized_layers = cut_custom_cnn_model(pretrained_model, sampled_cut_point, params)\n\n#             # Debug: Print reinitialized layers\n#             print(\"Reinitialized Layers:\", reinitialized_layers)\n\n            param_groups = [\n                {'params': [param for name, param in model_new.named_parameters() \n                            if any(name.startswith(reinit_layer) for reinit_layer in reinitialized_layers)], \n                 'lr': params['lr_fine_tune_reinit']},\n                {'params': [param for name, param in model_new.named_parameters() \n                            if not any(name.startswith(reinit_layer) for reinit_layer in reinitialized_layers)], \n                 'lr': params['lr_fine_tune_no_reinit']}\n            ]\n\n#             # Debug: Print parameter groups and their learning rates\n#             for i, group in enumerate(param_groups):\n#                 print(f\"Param Group {i}, LR: {group['lr']}\")\n#                 for param in group['params']:\n#                     param_name = [name for name, p in model_new.named_parameters() if p is param][0]\n#                     print(f\"  Layer: {param_name}\")\n\n            trainer = Trainer(model_new, dataset_namespace_new, params['lr_fine_tune'], params, param_groups)\n            train_acc, test_acc, effective_epochs, checkpoints = trainer.train(verbose=0)\n\n            layer_lr_details = {}\n            for i, param_group in enumerate(trainer.optimizer.param_groups):\n                lr = param_group['lr']\n                for param in param_group['params']:\n                    param_id = id(param)\n                    param_name = [name for name, p in model_new.named_parameters() if id(p) == param_id][0]\n                    layer_lr_details[param_name] = lr\n\n            layer_lr_info = {\n                'reinit_lr': params['lr_fine_tune_reinit'],\n                'reinitialized_layers': reinitialized_layers,\n                'non_reinit_lr': params['lr_fine_tune_no_reinit'],\n                'non_reinitialized_layers': [name for name, _ in model_new.named_parameters() if name not in reinitialized_layers],\n                'layer_lr_details': layer_lr_details\n            }\n\n            results.append({\n                \"lr\": params['lr_fine_tune'],\n                \"sampled_percentage\": sampled_percentage,\n                \"sampled_cut_point\": sampled_cut_point,\n                \"repeat\": repeat,\n                \"train_acc\": train_acc,\n                \"test_acc\": test_acc,\n                \"layer_lr_info\": layer_lr_info\n            })","metadata":{"id":"hMZ4o1FGsipP","outputId":"08ac8a68-17f6-48a3-e06e-27f484b9507d","scrolled":true,"execution":{"iopub.status.busy":"2023-11-29T00:33:53.762393Z","iopub.execute_input":"2023-11-29T00:33:53.762730Z","iopub.status.idle":"2023-11-29T05:46:33.316877Z","shell.execute_reply.started":"2023-11-29T00:33:53.762701Z","shell.execute_reply":"2023-11-29T05:46:33.315898Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"\nSampled Percentage: 0.001, Sampled Cut Point: 0, Repeat: 0\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 0, Repeat: 1\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 0, Repeat: 2\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 0, Repeat: 3\n\nSampled Percentage: 0.001, Sampled Cut Point: 0, Repeat: 4\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 0, Repeat: 5\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 0, Repeat: 6\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 0, Repeat: 7\n\nSampled Percentage: 0.001, Sampled Cut Point: 0, Repeat: 8\n\nSampled Percentage: 0.001, Sampled Cut Point: 0, Repeat: 9\n\nSampled Percentage: 0.001, Sampled Cut Point: 1, Repeat: 0\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 1, Repeat: 1\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 1, Repeat: 2\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 1, Repeat: 3\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 1, Repeat: 4\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 1, Repeat: 5\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 1, Repeat: 6\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 1, Repeat: 7\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 1, Repeat: 8\n\nSampled Percentage: 0.001, Sampled Cut Point: 1, Repeat: 9\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 2, Repeat: 0\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 2, Repeat: 1\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 2, Repeat: 2\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 2, Repeat: 3\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 2, Repeat: 4\n\nSampled Percentage: 0.001, Sampled Cut Point: 2, Repeat: 5\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 2, Repeat: 6\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 2, Repeat: 7\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 2, Repeat: 8\n\nSampled Percentage: 0.001, Sampled Cut Point: 2, Repeat: 9\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 3, Repeat: 0\n\nSampled Percentage: 0.001, Sampled Cut Point: 3, Repeat: 1\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 3, Repeat: 2\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 3, Repeat: 3\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 3, Repeat: 4\n\nSampled Percentage: 0.001, Sampled Cut Point: 3, Repeat: 5\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 3, Repeat: 6\n\nSampled Percentage: 0.001, Sampled Cut Point: 3, Repeat: 7\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 3, Repeat: 8\nEarly stopping invoked.\n\nSampled Percentage: 0.001, Sampled Cut Point: 3, Repeat: 9\n\nSampled Percentage: 0.002, Sampled Cut Point: 0, Repeat: 0\n\nSampled Percentage: 0.002, Sampled Cut Point: 0, Repeat: 1\n\nSampled Percentage: 0.002, Sampled Cut Point: 0, Repeat: 2\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 0, Repeat: 3\n\nSampled Percentage: 0.002, Sampled Cut Point: 0, Repeat: 4\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 0, Repeat: 5\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 0, Repeat: 6\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 0, Repeat: 7\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 0, Repeat: 8\n\nSampled Percentage: 0.002, Sampled Cut Point: 0, Repeat: 9\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 1, Repeat: 0\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 1, Repeat: 1\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 1, Repeat: 2\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 1, Repeat: 3\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 1, Repeat: 4\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 1, Repeat: 5\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 1, Repeat: 6\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 1, Repeat: 7\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 1, Repeat: 8\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 1, Repeat: 9\n\nSampled Percentage: 0.002, Sampled Cut Point: 2, Repeat: 0\n\nSampled Percentage: 0.002, Sampled Cut Point: 2, Repeat: 1\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 2, Repeat: 2\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 2, Repeat: 3\n\nSampled Percentage: 0.002, Sampled Cut Point: 2, Repeat: 4\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 2, Repeat: 5\n\nSampled Percentage: 0.002, Sampled Cut Point: 2, Repeat: 6\n\nSampled Percentage: 0.002, Sampled Cut Point: 2, Repeat: 7\n\nSampled Percentage: 0.002, Sampled Cut Point: 2, Repeat: 8\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 2, Repeat: 9\n\nSampled Percentage: 0.002, Sampled Cut Point: 3, Repeat: 0\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 3, Repeat: 1\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 3, Repeat: 2\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 3, Repeat: 3\n\nSampled Percentage: 0.002, Sampled Cut Point: 3, Repeat: 4\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 3, Repeat: 5\n\nSampled Percentage: 0.002, Sampled Cut Point: 3, Repeat: 6\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 3, Repeat: 7\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 3, Repeat: 8\nEarly stopping invoked.\n\nSampled Percentage: 0.002, Sampled Cut Point: 3, Repeat: 9\nEarly stopping invoked.\n\nSampled Percentage: 0.005, Sampled Cut Point: 0, Repeat: 0\nEarly stopping invoked.\n\nSampled Percentage: 0.005, Sampled Cut Point: 0, Repeat: 1\nEarly stopping invoked.\n\nSampled Percentage: 0.005, Sampled Cut Point: 0, Repeat: 2\n\nSampled Percentage: 0.005, Sampled Cut Point: 0, Repeat: 3\n\nSampled Percentage: 0.005, Sampled Cut Point: 0, Repeat: 4\nEarly stopping invoked.\n\nSampled Percentage: 0.005, Sampled Cut Point: 0, Repeat: 5\n\nSampled Percentage: 0.005, Sampled Cut Point: 0, Repeat: 6\nEarly stopping invoked.\n\nSampled Percentage: 0.005, Sampled Cut Point: 0, Repeat: 7\nEarly stopping invoked.\n\nSampled Percentage: 0.005, Sampled Cut Point: 0, Repeat: 8\n\nSampled Percentage: 0.005, Sampled Cut Point: 0, Repeat: 9\nEarly stopping invoked.\n\nSampled Percentage: 0.005, Sampled Cut Point: 1, Repeat: 0\n\nSampled Percentage: 0.005, Sampled Cut Point: 1, Repeat: 1\n\nSampled Percentage: 0.005, Sampled Cut Point: 1, Repeat: 2\nEarly stopping invoked.\n\nSampled Percentage: 0.005, Sampled Cut Point: 1, Repeat: 3\n\nSampled Percentage: 0.005, Sampled Cut Point: 1, Repeat: 4\nEarly stopping invoked.\n\nSampled Percentage: 0.005, Sampled Cut Point: 1, Repeat: 5\n\nSampled Percentage: 0.005, Sampled Cut Point: 1, Repeat: 6\n\nSampled Percentage: 0.005, Sampled Cut Point: 1, Repeat: 7\nEarly stopping invoked.\n\nSampled Percentage: 0.005, Sampled Cut Point: 1, Repeat: 8\nEarly stopping invoked.\n\nSampled Percentage: 0.005, Sampled Cut Point: 1, Repeat: 9\n\nSampled Percentage: 0.005, Sampled Cut Point: 2, Repeat: 0\n\nSampled Percentage: 0.005, Sampled Cut Point: 2, Repeat: 1\nEarly stopping invoked.\n\nSampled Percentage: 0.005, Sampled Cut Point: 2, Repeat: 2\n\nSampled Percentage: 0.005, Sampled Cut Point: 2, Repeat: 3\nEarly stopping invoked.\n\nSampled Percentage: 0.005, Sampled Cut Point: 2, Repeat: 4\nEarly stopping invoked.\n\nSampled Percentage: 0.005, Sampled Cut Point: 2, Repeat: 5\nEarly stopping invoked.\n\nSampled Percentage: 0.005, Sampled Cut Point: 2, Repeat: 6\nEarly stopping invoked.\n\nSampled Percentage: 0.005, Sampled Cut Point: 2, Repeat: 7\n\nSampled Percentage: 0.005, Sampled Cut Point: 2, Repeat: 8\nEarly stopping invoked.\n\nSampled Percentage: 0.005, Sampled Cut Point: 2, Repeat: 9\nEarly stopping invoked.\n\nSampled Percentage: 0.005, Sampled Cut Point: 3, Repeat: 0\nEarly stopping invoked.\n\nSampled Percentage: 0.005, Sampled Cut Point: 3, Repeat: 1\n\nSampled Percentage: 0.005, Sampled Cut Point: 3, Repeat: 2\n\nSampled Percentage: 0.005, Sampled Cut Point: 3, Repeat: 3\nEarly stopping invoked.\n\nSampled Percentage: 0.005, Sampled Cut Point: 3, Repeat: 4\n\nSampled Percentage: 0.005, Sampled Cut Point: 3, Repeat: 5\nEarly stopping invoked.\n\nSampled Percentage: 0.005, Sampled Cut Point: 3, Repeat: 6\nEarly stopping invoked.\n\nSampled Percentage: 0.005, Sampled Cut Point: 3, Repeat: 7\n\nSampled Percentage: 0.005, Sampled Cut Point: 3, Repeat: 8\nEarly stopping invoked.\n\nSampled Percentage: 0.005, Sampled Cut Point: 3, Repeat: 9\n\nSampled Percentage: 0.01, Sampled Cut Point: 0, Repeat: 0\n\nSampled Percentage: 0.01, Sampled Cut Point: 0, Repeat: 1\nEarly stopping invoked.\n\nSampled Percentage: 0.01, Sampled Cut Point: 0, Repeat: 2\nEarly stopping invoked.\n\nSampled Percentage: 0.01, Sampled Cut Point: 0, Repeat: 3\nEarly stopping invoked.\n\nSampled Percentage: 0.01, Sampled Cut Point: 0, Repeat: 4\nEarly stopping invoked.\n\nSampled Percentage: 0.01, Sampled Cut Point: 0, Repeat: 5\nEarly stopping invoked.\n\nSampled Percentage: 0.01, Sampled Cut Point: 0, Repeat: 6\nEarly stopping invoked.\n\nSampled Percentage: 0.01, Sampled Cut Point: 0, Repeat: 7\nEarly stopping invoked.\n\nSampled Percentage: 0.01, Sampled Cut Point: 0, Repeat: 8\n\nSampled Percentage: 0.01, Sampled Cut Point: 0, Repeat: 9\n\nSampled Percentage: 0.01, Sampled Cut Point: 1, Repeat: 0\n\nSampled Percentage: 0.01, Sampled Cut Point: 1, Repeat: 1\n\nSampled Percentage: 0.01, Sampled Cut Point: 1, Repeat: 2\nEarly stopping invoked.\n\nSampled Percentage: 0.01, Sampled Cut Point: 1, Repeat: 3\n\nSampled Percentage: 0.01, Sampled Cut Point: 1, Repeat: 4\n\nSampled Percentage: 0.01, Sampled Cut Point: 1, Repeat: 5\n\nSampled Percentage: 0.01, Sampled Cut Point: 1, Repeat: 6\nEarly stopping invoked.\n\nSampled Percentage: 0.01, Sampled Cut Point: 1, Repeat: 7\n\nSampled Percentage: 0.01, Sampled Cut Point: 1, Repeat: 8\n\nSampled Percentage: 0.01, Sampled Cut Point: 1, Repeat: 9\nEarly stopping invoked.\n\nSampled Percentage: 0.01, Sampled Cut Point: 2, Repeat: 0\n\nSampled Percentage: 0.01, Sampled Cut Point: 2, Repeat: 1\n\nSampled Percentage: 0.01, Sampled Cut Point: 2, Repeat: 2\n\nSampled Percentage: 0.01, Sampled Cut Point: 2, Repeat: 3\n\nSampled Percentage: 0.01, Sampled Cut Point: 2, Repeat: 4\nEarly stopping invoked.\n\nSampled Percentage: 0.01, Sampled Cut Point: 2, Repeat: 5\nEarly stopping invoked.\n\nSampled Percentage: 0.01, Sampled Cut Point: 2, Repeat: 6\nEarly stopping invoked.\n\nSampled Percentage: 0.01, Sampled Cut Point: 2, Repeat: 7\nEarly stopping invoked.\n\nSampled Percentage: 0.01, Sampled Cut Point: 2, Repeat: 8\n\nSampled Percentage: 0.01, Sampled Cut Point: 2, Repeat: 9\n\nSampled Percentage: 0.01, Sampled Cut Point: 3, Repeat: 0\nEarly stopping invoked.\n\nSampled Percentage: 0.01, Sampled Cut Point: 3, Repeat: 1\nEarly stopping invoked.\n\nSampled Percentage: 0.01, Sampled Cut Point: 3, Repeat: 2\n\nSampled Percentage: 0.01, Sampled Cut Point: 3, Repeat: 3\nEarly stopping invoked.\n\nSampled Percentage: 0.01, Sampled Cut Point: 3, Repeat: 4\n\nSampled Percentage: 0.01, Sampled Cut Point: 3, Repeat: 5\n\nSampled Percentage: 0.01, Sampled Cut Point: 3, Repeat: 6\nEarly stopping invoked.\n\nSampled Percentage: 0.01, Sampled Cut Point: 3, Repeat: 7\n\nSampled Percentage: 0.01, Sampled Cut Point: 3, Repeat: 8\n\nSampled Percentage: 0.01, Sampled Cut Point: 3, Repeat: 9\nEarly stopping invoked.\n\nSampled Percentage: 0.05, Sampled Cut Point: 0, Repeat: 0\nEarly stopping invoked.\n\nSampled Percentage: 0.05, Sampled Cut Point: 0, Repeat: 1\nEarly stopping invoked.\n\nSampled Percentage: 0.05, Sampled Cut Point: 0, Repeat: 2\nEarly stopping invoked.\n\nSampled Percentage: 0.05, Sampled Cut Point: 0, Repeat: 3\nEarly stopping invoked.\n\nSampled Percentage: 0.05, Sampled Cut Point: 0, Repeat: 4\nEarly stopping invoked.\n\nSampled Percentage: 0.05, Sampled Cut Point: 1, Repeat: 0\n\nSampled Percentage: 0.05, Sampled Cut Point: 1, Repeat: 1\n\nSampled Percentage: 0.05, Sampled Cut Point: 1, Repeat: 2\n\nSampled Percentage: 0.05, Sampled Cut Point: 1, Repeat: 3\nEarly stopping invoked.\n\nSampled Percentage: 0.05, Sampled Cut Point: 1, Repeat: 4\n\nSampled Percentage: 0.05, Sampled Cut Point: 2, Repeat: 0\n\nSampled Percentage: 0.05, Sampled Cut Point: 2, Repeat: 1\nEarly stopping invoked.\n\nSampled Percentage: 0.05, Sampled Cut Point: 2, Repeat: 2\n\nSampled Percentage: 0.05, Sampled Cut Point: 2, Repeat: 3\n\nSampled Percentage: 0.05, Sampled Cut Point: 2, Repeat: 4\n\nSampled Percentage: 0.05, Sampled Cut Point: 3, Repeat: 0\nEarly stopping invoked.\n\nSampled Percentage: 0.05, Sampled Cut Point: 3, Repeat: 1\nEarly stopping invoked.\n\nSampled Percentage: 0.05, Sampled Cut Point: 3, Repeat: 2\n\nSampled Percentage: 0.05, Sampled Cut Point: 3, Repeat: 3\n\nSampled Percentage: 0.05, Sampled Cut Point: 3, Repeat: 4\nEarly stopping invoked.\n\nSampled Percentage: 0.1, Sampled Cut Point: 0, Repeat: 0\nEarly stopping invoked.\n\nSampled Percentage: 0.1, Sampled Cut Point: 0, Repeat: 1\nEarly stopping invoked.\n\nSampled Percentage: 0.1, Sampled Cut Point: 0, Repeat: 2\nEarly stopping invoked.\n\nSampled Percentage: 0.1, Sampled Cut Point: 0, Repeat: 3\n\nSampled Percentage: 0.1, Sampled Cut Point: 0, Repeat: 4\nEarly stopping invoked.\n\nSampled Percentage: 0.1, Sampled Cut Point: 1, Repeat: 0\nEarly stopping invoked.\n\nSampled Percentage: 0.1, Sampled Cut Point: 1, Repeat: 1\nEarly stopping invoked.\n\nSampled Percentage: 0.1, Sampled Cut Point: 1, Repeat: 2\n\nSampled Percentage: 0.1, Sampled Cut Point: 1, Repeat: 3\nEarly stopping invoked.\n\nSampled Percentage: 0.1, Sampled Cut Point: 1, Repeat: 4\n\nSampled Percentage: 0.1, Sampled Cut Point: 2, Repeat: 0\n\nSampled Percentage: 0.1, Sampled Cut Point: 2, Repeat: 1\nEarly stopping invoked.\n\nSampled Percentage: 0.1, Sampled Cut Point: 2, Repeat: 2\n\nSampled Percentage: 0.1, Sampled Cut Point: 2, Repeat: 3\n\nSampled Percentage: 0.1, Sampled Cut Point: 2, Repeat: 4\n\nSampled Percentage: 0.1, Sampled Cut Point: 3, Repeat: 0\nEarly stopping invoked.\n\nSampled Percentage: 0.1, Sampled Cut Point: 3, Repeat: 1\nEarly stopping invoked.\n\nSampled Percentage: 0.1, Sampled Cut Point: 3, Repeat: 2\nEarly stopping invoked.\n\nSampled Percentage: 0.1, Sampled Cut Point: 3, Repeat: 3\nEarly stopping invoked.\n\nSampled Percentage: 0.1, Sampled Cut Point: 3, Repeat: 4\n\nSampled Percentage: 0.3, Sampled Cut Point: 0, Repeat: 0\n\nSampled Percentage: 0.3, Sampled Cut Point: 0, Repeat: 1\n\nSampled Percentage: 0.3, Sampled Cut Point: 0, Repeat: 2\n\nSampled Percentage: 0.3, Sampled Cut Point: 0, Repeat: 3\n\nSampled Percentage: 0.3, Sampled Cut Point: 0, Repeat: 4\n\nSampled Percentage: 0.3, Sampled Cut Point: 1, Repeat: 0\n\nSampled Percentage: 0.3, Sampled Cut Point: 1, Repeat: 1\n\nSampled Percentage: 0.3, Sampled Cut Point: 1, Repeat: 2\n\nSampled Percentage: 0.3, Sampled Cut Point: 1, Repeat: 3\n\nSampled Percentage: 0.3, Sampled Cut Point: 1, Repeat: 4\n\nSampled Percentage: 0.3, Sampled Cut Point: 2, Repeat: 0\n\nSampled Percentage: 0.3, Sampled Cut Point: 2, Repeat: 1\n\nSampled Percentage: 0.3, Sampled Cut Point: 2, Repeat: 2\n\nSampled Percentage: 0.3, Sampled Cut Point: 2, Repeat: 3\n\nSampled Percentage: 0.3, Sampled Cut Point: 2, Repeat: 4\n\nSampled Percentage: 0.3, Sampled Cut Point: 3, Repeat: 0\n\nSampled Percentage: 0.3, Sampled Cut Point: 3, Repeat: 1\nEarly stopping invoked.\n\nSampled Percentage: 0.3, Sampled Cut Point: 3, Repeat: 2\n\nSampled Percentage: 0.3, Sampled Cut Point: 3, Repeat: 3\n\nSampled Percentage: 0.3, Sampled Cut Point: 3, Repeat: 4\n\nSampled Percentage: 0.5, Sampled Cut Point: 0, Repeat: 0\n\nSampled Percentage: 0.5, Sampled Cut Point: 0, Repeat: 1\n\nSampled Percentage: 0.5, Sampled Cut Point: 0, Repeat: 2\n\nSampled Percentage: 0.5, Sampled Cut Point: 1, Repeat: 0\n\nSampled Percentage: 0.5, Sampled Cut Point: 1, Repeat: 1\n\nSampled Percentage: 0.5, Sampled Cut Point: 1, Repeat: 2\n\nSampled Percentage: 0.5, Sampled Cut Point: 2, Repeat: 0\n\nSampled Percentage: 0.5, Sampled Cut Point: 2, Repeat: 1\n\nSampled Percentage: 0.5, Sampled Cut Point: 2, Repeat: 2\n\nSampled Percentage: 0.5, Sampled Cut Point: 3, Repeat: 0\n\nSampled Percentage: 0.5, Sampled Cut Point: 3, Repeat: 1\n\nSampled Percentage: 0.5, Sampled Cut Point: 3, Repeat: 2\n\nSampled Percentage: 0.8, Sampled Cut Point: 0, Repeat: 0\n\nSampled Percentage: 0.8, Sampled Cut Point: 0, Repeat: 1\n\nSampled Percentage: 0.8, Sampled Cut Point: 0, Repeat: 2\n\nSampled Percentage: 0.8, Sampled Cut Point: 1, Repeat: 0\n\nSampled Percentage: 0.8, Sampled Cut Point: 1, Repeat: 1\nEarly stopping invoked.\n\nSampled Percentage: 0.8, Sampled Cut Point: 1, Repeat: 2\n\nSampled Percentage: 0.8, Sampled Cut Point: 2, Repeat: 0\n\nSampled Percentage: 0.8, Sampled Cut Point: 2, Repeat: 1\n\nSampled Percentage: 0.8, Sampled Cut Point: 2, Repeat: 2\n\nSampled Percentage: 0.8, Sampled Cut Point: 3, Repeat: 0\n\nSampled Percentage: 0.8, Sampled Cut Point: 3, Repeat: 1\n\nSampled Percentage: 0.8, Sampled Cut Point: 3, Repeat: 2\n\nSampled Percentage: 1, Sampled Cut Point: 0, Repeat: 0\n\nSampled Percentage: 1, Sampled Cut Point: 0, Repeat: 1\n\nSampled Percentage: 1, Sampled Cut Point: 0, Repeat: 2\n\nSampled Percentage: 1, Sampled Cut Point: 1, Repeat: 0\nEarly stopping invoked.\n\nSampled Percentage: 1, Sampled Cut Point: 1, Repeat: 1\n\nSampled Percentage: 1, Sampled Cut Point: 1, Repeat: 2\n\nSampled Percentage: 1, Sampled Cut Point: 2, Repeat: 0\n\nSampled Percentage: 1, Sampled Cut Point: 2, Repeat: 1\n\nSampled Percentage: 1, Sampled Cut Point: 2, Repeat: 2\n\nSampled Percentage: 1, Sampled Cut Point: 3, Repeat: 0\n\nSampled Percentage: 1, Sampled Cut Point: 3, Repeat: 1\n\nSampled Percentage: 1, Sampled Cut Point: 3, Repeat: 2\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the results to a JSON file\nresults_filename = 'results.json'\nwith open(results_filename, 'w') as file:\n    json.dump(results, file)\n\nprint(f\"Results saved to {results_filename}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:46:33.318133Z","iopub.execute_input":"2023-11-29T05:46:33.318434Z","iopub.status.idle":"2023-11-29T05:46:33.347667Z","shell.execute_reply.started":"2023-11-29T05:46:33.318406Z","shell.execute_reply":"2023-11-29T05:46:33.346661Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Results saved to results.json\n","output_type":"stream"}]},{"cell_type":"code","source":"# results","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:46:33.348771Z","iopub.execute_input":"2023-11-29T05:46:33.349057Z","iopub.status.idle":"2023-11-29T05:46:33.353379Z","shell.execute_reply.started":"2023-11-29T05:46:33.349032Z","shell.execute_reply":"2023-11-29T05:46:33.352351Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n\n# # Assuming 'results' is your list of dictionaries\n# df = pd.DataFrame(results)\n\n# # Expand the 'layer_lr_info' dictionary into separate columns\n# layer_lr_info_df = df['layer_lr_info'].apply(pd.Series)\n# df = pd.concat([df.drop('layer_lr_info', axis=1), layer_lr_info_df], axis=1)\n\n# # Formatting layer names as comma-separated strings\n# df['reinitialized_layers'] = df['reinitialized_layers'].apply(lambda x: ', '.join(x))\n# df['non_reinitialized_layers'] = df['non_reinitialized_layers'].apply(lambda x: ', '.join(x))\n\n# # Display the DataFrame in the console\n# print(df)\n\n# # Optionally, save it to a CSV file for easy viewing in spreadsheet programs\n# df.to_csv(\"model_results_with_layer_names.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:46:33.354419Z","iopub.execute_input":"2023-11-29T05:46:33.354754Z","iopub.status.idle":"2023-11-29T05:46:33.362114Z","shell.execute_reply.started":"2023-11-29T05:46:33.354722Z","shell.execute_reply":"2023-11-29T05:46:33.361268Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:46:33.363104Z","iopub.execute_input":"2023-11-29T05:46:33.363395Z","iopub.status.idle":"2023-11-29T05:46:33.374196Z","shell.execute_reply.started":"2023-11-29T05:46:33.363361Z","shell.execute_reply":"2023-11-29T05:46:33.373279Z"},"trusted":true},"execution_count":17,"outputs":[]}]}